#!/usr/bin/env python3
"""D√©mo BBIAEmotionRecognition - Reconnaissance √©motions humaines.

D√©monstration du module de reconnaissance d'√©motions faciales et vocales.
"""

import argparse
import sys
from pathlib import Path

import numpy as np

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from bbia_sim.bbia_emotion_recognition import BBIAEmotionRecognition


def main() -> int:
    """Fonction principale."""
    parser = argparse.ArgumentParser(description="D√©mo BBIAEmotionRecognition")
    parser.add_argument(
        "--mode",
        choices=["facial", "vocal", "multimodal"],
        default="facial",
        help="Mode de reconnaissance",
    )
    parser.add_argument("--headless", action="store_true", help="Mode headless")
    parser.add_argument(
        "--device",
        default="auto",
        choices=["auto", "cpu", "cuda"],
        help="Device pour mod√®les ML",
    )

    args = parser.parse_args()

    print("üòä D√©mo BBIAEmotionRecognition - Reconnaissance √©motions")
    print(f"   ‚Ä¢ Mode : {args.mode}")
    print(f"   ‚Ä¢ Device : {args.device}")

    try:
        # Cr√©er module
        emotion_recognition = BBIAEmotionRecognition(device=args.device)
        print("‚úÖ BBIAEmotionRecognition cr√©√©")

        # 1. Reconnaissance faciale
        if args.mode == "facial":
            print("\nüì∏ Analyse √©motion faciale...")
            # Simuler une image (dans version r√©elle, utiliser webcam)
            image = np.zeros((480, 640, 3), dtype=np.uint8)
            result = emotion_recognition.analyze_facial_emotion(image)
            print(f"   √âmotion d√©tect√©e : {result.get('emotion', 'N/A')}")
            print(f"   Confiance : {result.get('confidence', 0.0):.2f}")

        # 2. Reconnaissance vocale
        elif args.mode == "vocal":
            print("\nüé§ Analyse √©motion vocale...")
            # Simuler un texte (dans version r√©elle, utiliser audio)
            text = "Je suis tr√®s heureux aujourd'hui !"
            result = emotion_recognition.analyze_vocal_emotion(text)
            print(f"   √âmotion d√©tect√©e : {result.get('emotion', 'N/A')}")
            print(f"   Confiance : {result.get('confidence', 0.0):.2f}")

        # 3. Multimodal
        elif args.mode == "multimodal":
            print("\nüé≠ Analyse multimodale (faciale + vocale)...")
            image = np.zeros((480, 640, 3), dtype=np.uint8)
            text = "Je suis surpris !"
            result = emotion_recognition.analyze_multimodal_emotion(image, text)
            print(f"   √âmotion d√©tect√©e : {result.get('emotion', 'N/A')}")
            print(f"   Confiance : {result.get('confidence', 0.0):.2f}")

        print("\n‚úÖ D√©mo termin√©e avec succ√®s")
        return 0

    except ImportError as e:
        print(f"‚ùå D√©pendances manquantes : {e}")
        print("üí° Installez avec: pip install mediapipe torch transformers")
        return 1
    except Exception as e:
        print(f"‚ùå Erreur : {e}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
