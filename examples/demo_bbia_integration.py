#!/usr/bin/env python3
"""
Exemple d'utilisation simple de l'intÃ©gration BBIA â†” Robot
DÃ©monstration rapide des fonctionnalitÃ©s principales
"""

import asyncio
import sys
from pathlib import Path

# Ajouter le chemin du module parent
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.bbia_sim.bbia_integration import create_bbia_integration


async def demo_simple():
    """DÃ©monstration simple de l'intÃ©gration BBIA."""

    print("ğŸ­ DÃ‰MONSTRATION BBIA â†” ROBOT REACHY MINI")
    print("=" * 50)

    # CrÃ©er l'intÃ©gration BBIA
    print("ğŸš€ Initialisation de l'intÃ©gration BBIA...")
    integration = await create_bbia_integration(
        "src/bbia_sim/sim/models/reachy_mini_REAL_OFFICIAL.xml"
    )

    print("âœ… IntÃ©gration BBIA prÃªte !")

    # DÃ©monstration des Ã©motions
    print("\nğŸ­ DÃ©monstration des Ã©motions :")

    emotions_demo = [
        ("neutral", "Ã‰tat de repos"),
        ("happy", "Joie et contentement"),
        ("sad", "Tristesse et mÃ©lancolie"),
        ("angry", "ColÃ¨re et frustration"),
        ("surprised", "Surprise et Ã©tonnement"),
        ("curious", "CuriositÃ© et intÃ©rÃªt"),
        ("excited", "Excitation et enthousiasme"),
        ("fearful", "Peur et apprÃ©hension"),
    ]

    for emotion, description in emotions_demo:
        print(f"\nğŸ­ {emotion.upper()} - {description}")
        await integration.apply_emotion_to_robot(emotion, 0.7)
        await asyncio.sleep(2.0)

    # DÃ©monstration des rÃ©actions visuelles
    print("\nğŸ‘ï¸ DÃ©monstration des rÃ©actions visuelles :")

    # Simulation dÃ©tection de visage
    print("ğŸ‘¤ Simulation dÃ©tection de visage...")
    face_data = {"faces": [{"position": (0.2, 0.1), "confidence": 0.9}]}
    await integration.react_to_vision_detection(face_data)
    await asyncio.sleep(2.0)

    # Simulation dÃ©tection d'objet
    print("ğŸ“¦ Simulation dÃ©tection d'objet...")
    object_data = {
        "objects": [{"name": "bottle", "position": (0.1, 0.0), "confidence": 0.85}]
    }
    await integration.react_to_vision_detection(object_data)
    await asyncio.sleep(2.0)

    # DÃ©monstration synchronisation voix
    print("\nğŸ—£ï¸ DÃ©monstration synchronisation voix :")

    test_phrases = [
        ("Bonjour ! Je suis BBIA.", "happy"),
        ("Je suis trÃ¨s surpris !", "surprised"),
        ("Je suis curieux de vous.", "curious"),
    ]

    for text, emotion in test_phrases:
        print(f"ğŸ—£ï¸ '{text}' (Ã©motion: {emotion})")
        await integration.sync_voice_with_movements(text, emotion)
        await asyncio.sleep(1.5)

    # DÃ©monstration comportements
    print("\nğŸ¬ DÃ©monstration des comportements :")

    behaviors = ["greeting", "antenna_animation", "hide"]

    for behavior in behaviors:
        print(f"ğŸ¬ Comportement : {behavior}")
        await integration.execute_behavior_sequence(behavior)
        await asyncio.sleep(2.0)

    # Retour Ã  l'Ã©tat neutre
    print("\nğŸ Retour Ã  l'Ã©tat neutre...")
    await integration.apply_emotion_to_robot("neutral", 0.5)

    # Afficher le statut final
    status = integration.get_integration_status()
    print("\nğŸ“Š Statut final :")
    print(f"   â€¢ Ã‰motion : {status['current_emotion']}")
    print(f"   â€¢ IntensitÃ© : {status['emotion_intensity']}")
    print(f"   â€¢ Active : {status['is_active']}")

    print("\nğŸ‰ DÃ©monstration terminÃ©e avec succÃ¨s !")

    # ArrÃªter l'intÃ©gration
    await integration.stop_integration()


async def demo_interactive():
    """DÃ©monstration interactive de l'intÃ©gration BBIA."""

    print("ğŸ® DÃ‰MONSTRATION INTERACTIVE BBIA â†” ROBOT")
    print("=" * 50)

    # CrÃ©er l'intÃ©gration BBIA
    integration = await create_bbia_integration(
        "src/bbia_sim/sim/models/reachy_mini_REAL_OFFICIAL.xml"
    )

    print("âœ… IntÃ©gration BBIA prÃªte !")
    print("\nCommandes disponibles :")
    print("  emotion <nom> <intensitÃ©> - Appliquer une Ã©motion")
    print("  vision <type> - Simuler une dÃ©tection visuelle")
    print("  voice <texte> <Ã©motion> - Synchroniser voix + mouvements")
    print("  behavior <nom> - ExÃ©cuter un comportement")
    print("  status - Afficher le statut")
    print("  quit - Quitter")

    while True:
        try:
            command = input("\nğŸ® Commande : ").strip().lower()

            if command == "quit":
                break

            elif command.startswith("emotion "):
                parts = command.split()
                if len(parts) >= 2:
                    emotion = parts[1]
                    intensity = float(parts[2]) if len(parts) > 2 else 0.5
                    await integration.apply_emotion_to_robot(emotion, intensity)
                    print(f"âœ… Ã‰motion '{emotion}' appliquÃ©e (intensitÃ©: {intensity})")
                else:
                    print("âŒ Usage: emotion <nom> [intensitÃ©]")

            elif command.startswith("vision "):
                vision_type = command.split()[1] if len(command.split()) > 1 else "face"
                if vision_type == "face":
                    data = {"faces": [{"position": (0.2, 0.1), "confidence": 0.9}]}
                elif vision_type == "object":
                    data = {
                        "objects": [
                            {
                                "name": "bottle",
                                "position": (0.1, 0.0),
                                "confidence": 0.85,
                            }
                        ]
                    }
                else:
                    print("âŒ Types disponibles: face, object")
                    continue

                await integration.react_to_vision_detection(data)
                print(f"âœ… RÃ©action Ã  la dÃ©tection '{vision_type}' appliquÃ©e")

            elif command.startswith("voice "):
                parts = command.split(" ", 2)
                if len(parts) >= 3:
                    text = parts[1]
                    emotion = parts[2]
                    await integration.sync_voice_with_movements(text, emotion)
                    print("âœ… Synchronisation voix + mouvements appliquÃ©e")
                else:
                    print("âŒ Usage: voice <texte> <Ã©motion>")

            elif command.startswith("behavior "):
                behavior = (
                    command.split()[1] if len(command.split()) > 1 else "greeting"
                )
                await integration.execute_behavior_sequence(behavior)
                print(f"âœ… Comportement '{behavior}' exÃ©cutÃ©")

            elif command == "status":
                status = integration.get_integration_status()
                print(
                    f"ğŸ“Š Statut : {status['current_emotion']} (intensitÃ©: {status['emotion_intensity']})"
                )

            else:
                print("âŒ Commande inconnue")

        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"âŒ Erreur : {e}")

    # ArrÃªter l'intÃ©gration
    await integration.stop_integration()
    print("\nğŸ‘‹ DÃ©monstration interactive terminÃ©e !")


async def main():
    """Fonction principale."""

    if len(sys.argv) > 1 and sys.argv[1] == "interactive":
        await demo_interactive()
    else:
        await demo_simple()


if __name__ == "__main__":
    asyncio.run(main())
