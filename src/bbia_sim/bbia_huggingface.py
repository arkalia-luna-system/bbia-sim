#!/usr/bin/env python3
"""
BBIA Hugging Face Integration - Module d'int√©gration des mod√®les pr√©-entra√Æn√©s
Int√©gration avanc√©e avec Hugging Face Hub pour enrichir les capacit√©s IA de BBIA-SIM
"""

import logging
import os
from typing import Any, Optional, Union

import numpy as np
from PIL import Image

# D√©sactiver les avertissements de transformers
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

logger = logging.getLogger(__name__)

# Import conditionnel des d√©pendances Hugging Face
try:
    import warnings

    import torch

    # Supprimer les avertissements de transformers
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        from transformers import (
            BlipForConditionalGeneration,
            BlipProcessor,
            CLIPModel,
            CLIPProcessor,
            WhisperForConditionalGeneration,
            WhisperProcessor,
            pipeline,
        )
        from transformers.utils import logging as transformers_logging

        # R√©duire la verbosit√© de transformers
        transformers_logging.set_verbosity_error()

    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    logger.warning(
        "Hugging Face transformers non disponible. Installez avec: pip install transformers torch"
    )


class BBIAHuggingFace:
    """Module d'int√©gration Hugging Face pour BBIA-SIM.

    Fonctionnalit√©s :
    - Vision : CLIP, BLIP pour description d'images
    - Audio : Whisper pour STT avanc√©
    - NLP : Mod√®les de sentiment, √©motions
    - Multimodal : Mod√®les combinant vision + texte
    """

    def __init__(self, device: str = "auto", cache_dir: Optional[str] = None) -> None:
        """Initialise le module Hugging Face.

        Args:
            device: Device pour les mod√®les ("cpu", "cuda", "auto")
            cache_dir: R√©pertoire de cache pour les mod√®les
        """
        if not HF_AVAILABLE:
            raise ImportError(
                "Hugging Face transformers requis. Installez avec: pip install transformers torch"
            )

        self.device = self._get_device(device)
        self.cache_dir = cache_dir
        self.models: dict[str, Any] = {}
        self.processors: dict[str, Any] = {}

        # Chat intelligent : Historique et contexte
        self.conversation_history: list[dict[str, Any]] = []
        self.context: dict[str, Any] = {}
        self.bbia_personality = "friendly_robot"

        # Configuration des mod√®les recommand√©s
        self.model_configs = {
            "vision": {
                "clip": "openai/clip-vit-base-patch32",
                "blip": "Salesforce/blip-image-captioning-base",
            },
            "audio": {
                "whisper": "openai/whisper-base",
            },
            "nlp": {
                "sentiment": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                "emotion": "j-hartmann/emotion-english-distilroberta-base",
            },
            "chat": {
                # LLM conversationnel (optionnel, activ√© si disponible)
                "mistral": "mistralai/Mistral-7B-Instruct-v0.2",  # ‚≠ê Recommand√©
                "llama": "meta-llama/Llama-3-8B-Instruct",  # Alternative
            },
            "multimodal": {
                "blip_vqa": "Salesforce/blip-vqa-base",
            },
        }

        # √âtat du mod√®le de conversation
        self.chat_model: Optional[Any] = None
        self.chat_tokenizer: Optional[Any] = None
        self.use_llm_chat = False  # Activation optionnelle (lourd)

        logger.info(f"ü§ó BBIA Hugging Face initialis√© (device: {self.device})")
        logger.info(f"üòä Personnalit√© BBIA: {self.bbia_personality}")

    def _get_device(self, device: str) -> str:
        """D√©termine le device optimal."""
        if device == "auto":
            if HF_AVAILABLE and torch.cuda.is_available():
                return "cuda"
            elif HF_AVAILABLE and torch.backends.mps.is_available():
                return "mps"  # Apple Silicon
            else:
                return "cpu"
        return device

    def _load_vision_model(self, model_name: str) -> bool:
        """Charge un mod√®le de vision (CLIP ou BLIP)."""
        if "clip" in model_name.lower():
            processor: Any = CLIPProcessor.from_pretrained(  # nosec B615
                model_name, cache_dir=self.cache_dir
            )
            model = CLIPModel.from_pretrained(  # nosec B615
                model_name, cache_dir=self.cache_dir
            ).to(self.device)
            self.processors[f"{model_name}_processor"] = processor  # type: ignore[assignment]
            self.models[f"{model_name}_model"] = model  # type: ignore[assignment]
            return True
        elif "blip" in model_name.lower():
            blip_processor: Any = BlipProcessor.from_pretrained(  # nosec B615
                model_name, cache_dir=self.cache_dir
            )
            model = BlipForConditionalGeneration.from_pretrained(  # nosec B615
                model_name, cache_dir=self.cache_dir
            ).to(self.device)
            self.processors[f"{model_name}_processor"] = blip_processor  # type: ignore[assignment]
            self.models[f"{model_name}_model"] = model  # type: ignore[assignment]
            return True
        return False

    def _load_audio_model(self, model_name: str) -> bool:
        """Charge un mod√®le audio (Whisper)."""
        if "whisper" in model_name.lower():
            whisper_processor: Any = WhisperProcessor.from_pretrained(  # nosec B615
                model_name, cache_dir=self.cache_dir
            )
            model = WhisperForConditionalGeneration.from_pretrained(  # nosec B615
                model_name, cache_dir=self.cache_dir
            ).to(self.device)
            self.processors[f"{model_name}_processor"] = whisper_processor  # type: ignore[assignment]
            self.models[f"{model_name}_model"] = model  # type: ignore[assignment]
            return True
        return False

    def _load_chat_model(self, model_name: str) -> bool:
        """Charge un mod√®le LLM conversationnel."""
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer

            logger.info(
                f"üì• Chargement LLM {model_name} (peut prendre 1-2 minutes)..."
            )
            self.chat_tokenizer = AutoTokenizer.from_pretrained(  # type: ignore[assignment]
                model_name, cache_dir=self.cache_dir
            )

            if (
                self.chat_tokenizer is not None
                and self.chat_tokenizer.pad_token is None
            ):
                self.chat_tokenizer.pad_token = self.chat_tokenizer.eos_token

            self.chat_model = AutoModelForCausalLM.from_pretrained(  # type: ignore # nosec B615
                model_name,
                cache_dir=self.cache_dir,
                device_map="auto",
                torch_dtype=(
                    torch.float16 if self.device != "cpu" else torch.float32
                ),
            )
            logger.info(f"‚úÖ LLM {model_name} charg√© avec succ√®s")
            self.use_llm_chat = True
            return True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Impossible de charger LLM {model_name}: {e}")
            logger.info("üí° Le syst√®me utilisera les r√©ponses enrichies (r√®gles)")
            self.use_llm_chat = False
            return False

    def _load_multimodal_model(self, model_name: str) -> bool:
        """Charge un mod√®le multimodal (BLIP VQA)."""
        if "blip" in model_name.lower() and "vqa" in model_name.lower():
            vqa_processor: Any = BlipProcessor.from_pretrained(  # type: ignore # nosec B615
                model_name, cache_dir=self.cache_dir
            )
            model = BlipForConditionalGeneration.from_pretrained(  # nosec B615
                model_name, cache_dir=self.cache_dir
            ).to(self.device)
            self.processors[f"{model_name}_processor"] = vqa_processor  # type: ignore[assignment]
            self.models[f"{model_name}_model"] = model  # type: ignore[assignment]
            return True
        return False

    def load_model(self, model_name: str, model_type: str = "vision") -> bool:
        """Charge un mod√®le Hugging Face.

        Args:
            model_name: Nom du mod√®le ou chemin
            model_type: Type de mod√®le ("vision", "audio", "nlp", "multimodal")

        Returns:
            True si charg√© avec succ√®s
        """
        try:
            logger.info(f"üì• Chargement mod√®le {model_name} ({model_type})")

            if model_type == "vision":
                if "clip" in model_name.lower():
                    clip_processor: Any = CLIPProcessor.from_pretrained(  # nosec B615
                        model_name, cache_dir=self.cache_dir
                    )
                    model = CLIPModel.from_pretrained(  # nosec B615
                        model_name, cache_dir=self.cache_dir
                    ).to(self.device)
                    self.processors[f"{model_name}_processor"] = clip_processor  # type: ignore[assignment]
                    self.models[f"{model_name}_model"] = model  # type: ignore[assignment]

                elif "blip" in model_name.lower():
                    blip_processor: Any = BlipProcessor.from_pretrained(  # nosec B615
                        model_name, cache_dir=self.cache_dir
                    )
                    model = BlipForConditionalGeneration.from_pretrained(  # nosec B615
                        model_name, cache_dir=self.cache_dir
                    ).to(self.device)
                    self.processors[f"{model_name}_processor"] = blip_processor  # type: ignore[assignment]
                    self.models[f"{model_name}_model"] = model  # type: ignore[assignment]

            elif model_type == "audio":
                if "whisper" in model_name.lower():
                    whisper_processor: Any = WhisperProcessor.from_pretrained(  # nosec B615
                        model_name, cache_dir=self.cache_dir
                    )
                    model = WhisperForConditionalGeneration.from_pretrained(  # nosec B615
                        model_name, cache_dir=self.cache_dir
                    ).to(self.device)
                    self.processors[f"{model_name}_processor"] = whisper_processor  # type: ignore[assignment]
                    self.models[f"{model_name}_model"] = model  # type: ignore[assignment]

            elif model_type == "nlp":
                # Utilisation des pipelines pour NLP
                pipeline_name = self._get_pipeline_name(model_name)
                pipe = pipeline(  # type: ignore[call-overload]
                    pipeline_name, model=model_name, device=self.device
                )
                self.models[f"{model_name}_pipeline"] = pipe

            elif model_type == "chat":
                # Charger LLM conversationnel (Mistral, Llama, etc.)
                try:
                    from transformers import AutoModelForCausalLM, AutoTokenizer

                    logger.info(
                        f"üì• Chargement LLM {model_name} (peut prendre 1-2 minutes)..."
                    )
                    self.chat_tokenizer = AutoTokenizer.from_pretrained(  # type: ignore[assignment]
                        model_name, cache_dir=self.cache_dir
                    )

                    # Support instruction format
                    if (
                        self.chat_tokenizer is not None
                        and self.chat_tokenizer.pad_token is None
                    ):
                        self.chat_tokenizer.pad_token = self.chat_tokenizer.eos_token

                    chat_model_load: Any = AutoModelForCausalLM.from_pretrained(  # nosec B615
                        model_name,
                        cache_dir=self.cache_dir,
                        device_map="auto",  # Auto-d√©tecte MPS/CPU/CUDA
                        torch_dtype=(
                            torch.float16 if self.device != "cpu" else torch.float32
                        ),
                    )
                    self.chat_model = chat_model_load

                    logger.info(f"‚úÖ LLM {model_name} charg√© avec succ√®s")
                    self.use_llm_chat = True
                    return True
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Impossible de charger LLM {model_name}: {e}")
                    logger.info(
                        "üí° Le syst√®me utilisera les r√©ponses enrichies (r√®gles)"
                    )
                    self.use_llm_chat = False
                    return False

            elif model_type == "multimodal":
                if "blip" in model_name.lower() and "vqa" in model_name.lower():
                    vqa_processor: Any = BlipProcessor.from_pretrained(  # nosec B615
                        model_name, cache_dir=self.cache_dir
                    )
                    model = BlipForConditionalGeneration.from_pretrained(  # nosec B615
                        model_name, cache_dir=self.cache_dir
                    ).to(self.device)
                    self.processors[f"{model_name}_processor"] = vqa_processor  # type: ignore[assignment]
                    self.models[f"{model_name}_model"] = model  # type: ignore[assignment]

            logger.info(f"‚úÖ Mod√®le {model_name} charg√© avec succ√®s")
            return True

        except Exception as e:
            logger.error(f"‚ùå Erreur chargement mod√®le {model_name}: {e}")
            return False

    def _get_pipeline_name(self, model_name: str) -> str:
        """D√©termine le nom du pipeline bas√© sur le mod√®le."""
        if "sentiment" in model_name.lower():
            return "sentiment-analysis"
        elif "emotion" in model_name.lower():
            return "text-classification"
        else:
            return "text-classification"

    def describe_image(
        self, image: Union[str, Image.Image, np.ndarray], model_name: str = "blip"
    ) -> str:
        """D√©crit une image avec BLIP ou CLIP.

        Args:
            image: Image √† d√©crire (chemin, PIL Image, ou numpy array)
            model_name: Nom du mod√®le √† utiliser

        Returns:
            Description textuelle de l'image
        """
        try:
            # Conversion de l'image
            if isinstance(image, str):
                image = Image.open(image)
            elif isinstance(image, np.ndarray):
                image = Image.fromarray(image)

            if "blip" in model_name.lower():
                processor_key = f"{model_name}_processor"
                model_key = f"{model_name}_model"

                if processor_key not in self.processors or model_key not in self.models:
                    self.load_model(model_name, "vision")

                processor = self.processors[processor_key]
                model = self.models[model_key]

                inputs = processor(image, return_tensors="pt").to(self.device)
                out = model.generate(**inputs, max_length=50)
                description = processor.decode(out[0], skip_special_tokens=True)

                return str(description)

            elif "clip" in model_name.lower():
                processor_key = f"{model_name}_processor"
                model_key = f"{model_name}_model"

                if processor_key not in self.processors or model_key not in self.models:
                    self.load_model(model_name, "vision")

                processor = self.processors[processor_key]
                model = self.models[model_key]

                inputs = processor(
                    text=["a photo of"], images=image, return_tensors="pt", padding=True
                ).to(self.device)
                outputs = model(**inputs)
                logits_per_image = outputs.logits_per_image
                probs = logits_per_image.softmax(dim=1)

                return f"CLIP analysis: {probs.cpu().numpy()}"

            return "Erreur: mod√®le non support√©"

        except Exception as e:
            logger.error(f"‚ùå Erreur description image: {e}")
            return "Erreur lors de la description de l'image"

    def analyze_sentiment(
        self,
        text: str,
        model_name: str = "cardiffnlp/twitter-roberta-base-sentiment-latest",
    ) -> dict[str, Any]:
        """Analyse le sentiment d'un texte.

        Args:
            text: Texte √† analyser
            model_name: Nom du mod√®le √† utiliser (mod√®le Hugging Face complet)

        Returns:
            Dictionnaire avec sentiment et score
        """
        try:
            model_key = f"{model_name}_pipeline"

            if model_key not in self.models:
                self.load_model(model_name, "nlp")

            pipeline = self.models[model_key]
            result = pipeline(text)

            return {
                "text": text,
                "sentiment": result[0]["label"],
                "score": result[0]["score"],
                "model": model_name,
            }

        except Exception as e:
            logger.error(f"‚ùå Erreur analyse sentiment: {e}")
            return {"error": str(e)}

    def analyze_emotion(self, text: str, model_name: str = "emotion") -> dict[str, Any]:
        """Analyse les √©motions dans un texte.

        Args:
            text: Texte √† analyser
            model_name: Nom du mod√®le √† utiliser

        Returns:
            Dictionnaire avec √©motion d√©tect√©e et score
        """
        try:
            model_key = f"{model_name}_pipeline"

            if model_key not in self.models:
                self.load_model(model_name, "nlp")

            pipeline = self.models[model_key]
            result = pipeline(text)

            return {
                "text": text,
                "emotion": result[0]["label"],
                "score": result[0]["score"],
                "model": model_name,
            }

        except Exception as e:
            logger.error(f"‚ùå Erreur analyse √©motion: {e}")
            return {"error": str(e)}

    def transcribe_audio(self, audio_path: str, model_name: str = "whisper") -> str:
        """Transcrit un fichier audio avec Whisper.

        Args:
            audio_path: Chemin vers le fichier audio
            model_name: Nom du mod√®le Whisper √† utiliser

        Returns:
            Texte transcrit
        """
        try:
            processor_key = f"{model_name}_processor"
            model_key = f"{model_name}_model"

            if processor_key not in self.processors or model_key not in self.models:
                self.load_model(model_name, "audio")

            processor = self.processors[processor_key]
            model = self.models[model_key]

            # Chargement de l'audio
            audio, sample_rate = processor.load_audio(audio_path)
            inputs = processor(
                audio, sampling_rate=sample_rate, return_tensors="pt"
            ).to(self.device)

            # Transcription
            with torch.no_grad():
                predicted_ids = model.generate(inputs["input_features"])

            transcription = processor.batch_decode(
                predicted_ids, skip_special_tokens=True
            )[0]

            return str(transcription)

        except Exception as e:
            logger.error(f"‚ùå Erreur transcription audio: {e}")
            return "Erreur lors de la transcription"

    def answer_question(
        self,
        image: Union[str, Image.Image],
        question: str,
        model_name: str = "blip_vqa",
    ) -> str:
        """R√©pond √† une question sur une image (VQA).

        Args:
            image: Image √† analyser
            question: Question √† poser
            model_name: Nom du mod√®le VQA √† utiliser

        Returns:
            R√©ponse √† la question
        """
        try:
            # Conversion de l'image
            if isinstance(image, str):
                image = Image.open(image)
            elif isinstance(image, np.ndarray):
                image = Image.fromarray(image)

            processor_key = f"{model_name}_processor"
            model_key = f"{model_name}_model"

            if processor_key not in self.processors or model_key not in self.models:
                self.load_model(model_name, "multimodal")

            processor = self.processors[processor_key]
            model = self.models[model_key]

            inputs = processor(image, question, return_tensors="pt").to(self.device)
            out = model.generate(**inputs, max_length=50)
            answer = processor.decode(out[0], skip_special_tokens=True)

            return str(answer)

        except Exception as e:
            logger.error(f"‚ùå Erreur VQA: {e}")
            return "Erreur lors de l'analyse de l'image"

    def get_available_models(self) -> dict[str, list[str]]:
        """Retourne la liste des mod√®les disponibles par cat√©gorie."""
        return {
            "vision": list(self.model_configs["vision"].keys()),
            "audio": list(self.model_configs["audio"].keys()),
            "nlp": list(self.model_configs["nlp"].keys()),
            "chat": list(self.model_configs.get("chat", {}).keys()),
            "multimodal": list(self.model_configs["multimodal"].keys()),
        }

    def get_loaded_models(self) -> list[str]:
        """Retourne la liste des mod√®les actuellement charg√©s."""
        return list(self.models.keys())

    def enable_llm_chat(
        self, model_name: str = "mistralai/Mistral-7B-Instruct-v0.2"
    ) -> bool:
        """Active le LLM conversationnel (optionnel, lourd).

        Args:
            model_name: Mod√®le LLM √† charger (Mistral ou Llama)

        Returns:
            True si charg√© avec succ√®s

        Note:
            - Requiert ~14GB RAM pour Mistral 7B
            - Premier chargement : 1-2 minutes
            - Support Apple Silicon (MPS) automatique
        """
        logger.info(f"üì• Activation LLM conversationnel: {model_name}")
        success = self.load_model(model_name, model_type="chat")
        if success:
            logger.info(
                "‚úÖ LLM conversationnel activ√© - Conversations intelligentes disponibles"
            )
        else:
            logger.warning("‚ö†Ô∏è  LLM non charg√© - Utilisation r√©ponses enrichies")
        return success

    def disable_llm_chat(self) -> None:
        """D√©sactive le LLM conversationnel pour lib√©rer m√©moire."""
        if self.chat_model:
            del self.chat_model
            del self.chat_tokenizer
            self.chat_model = None
            self.chat_tokenizer = None
            self.use_llm_chat = False
            import gc

            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            logger.info("üóëÔ∏è LLM conversationnel d√©sactiv√© - M√©moire lib√©r√©e")

    def unload_model(self, model_name: str) -> bool:
        """D√©charge un mod√®le de la m√©moire.

        Args:
            model_name: Nom du mod√®le √† d√©charger

        Returns:
            True si d√©charg√© avec succ√®s
        """
        try:
            keys_to_remove = [key for key in self.models.keys() if model_name in key]
            for key in keys_to_remove:
                del self.models[key]

            keys_to_remove = [
                key for key in self.processors.keys() if model_name in key
            ]
            for key in keys_to_remove:
                del self.processors[key]

            logger.info(f"üóëÔ∏è Mod√®le {model_name} d√©charg√©")
            return True

        except Exception as e:
            logger.error(f"‚ùå Erreur d√©chargement mod√®le {model_name}: {e}")
            return False

    def get_model_info(self) -> dict[str, Any]:
        """Retourne les informations sur les mod√®les charg√©s."""
        return {
            "device": self.device,
            "loaded_models": self.get_loaded_models(),
            "available_models": self.get_available_models(),
            "cache_dir": self.cache_dir,
            "hf_available": HF_AVAILABLE,
        }

    def chat(self, user_message: str, use_context: bool = True) -> str:
        """Chat intelligent avec BBIA avec contexte et analyse sentiment.

        Utilise LLM pr√©-entra√Æn√© (Mistral 7B) si disponible, sinon r√©ponses enrichies.

        Args:
            user_message: Message de l'utilisateur
            use_context: Utiliser le contexte des messages pr√©c√©dents

        Returns:
            R√©ponse intelligente de BBIA
        """
        try:
            # 1. Analyser sentiment du message (avec gestion erreur)
            try:
                sentiment = self.analyze_sentiment(user_message)
            except Exception:
                # Fallback si sentiment indisponible
                sentiment = {"sentiment": "NEUTRAL", "score": 0.5}

            # 2. G√©n√©rer r√©ponse avec LLM si disponible, sinon r√©ponses enrichies
            if self.use_llm_chat and self.chat_model and self.chat_tokenizer:
                # Utiliser LLM pr√©-entra√Æn√© (Mistral/Llama)
                bbia_response = self._generate_llm_response(user_message, use_context)
            else:
                # Fallback vers r√©ponses enrichies (r√®gles + vari√©t√©)
                bbia_response = self._generate_simple_response(user_message, sentiment)

            # 3. Sauvegarder dans l'historique
            from datetime import datetime

            self.conversation_history.append(
                {
                    "user": user_message,
                    "bbia": bbia_response,
                    "sentiment": sentiment,
                    "timestamp": datetime.now().isoformat(),
                }
            )

            # 4. Adapter r√©ponse selon personnalit√© BBIA (si pas LLM)
            if not self.use_llm_chat:
                adapted_response = self._adapt_response_to_personality(
                    bbia_response, sentiment
                )
            else:
                adapted_response = bbia_response  # LLM g√®re d√©j√† la personnalit√©

            return adapted_response

        except Exception as e:
            logger.error(f"‚ùå Erreur chat: {e}")
            return "Je ne comprends pas bien, peux-tu reformuler ?"

    def _generate_llm_response(
        self, user_message: str, use_context: bool = True
    ) -> str:
        """G√©n√®re une r√©ponse avec LLM pr√©-entra√Æn√© (Mistral/Llama).

        Args:
            user_message: Message utilisateur
            use_context: Utiliser contexte historique

        Returns:
            R√©ponse g√©n√©r√©e par LLM
        """
        try:
            if not self.chat_model or not self.chat_tokenizer:
                raise ValueError("LLM non charg√©")

            # Construire prompt avec personnalit√© BBIA enrichie
            # AM√âLIORATION INTELLIGENCE: Prompt plus d√©taill√© pour g√©n√©rer des r√©ponses plus naturelles
            personality_descriptions = {
                "friendly_robot": (
                    "Tu es BBIA, un robot Reachy Mini amical, curieux et intelligent. "
                    "Tu communiques en fran√ßais de mani√®re naturelle, chaleureuse et authentique, "
                    "comme un v√©ritable compagnon. Tu √©vites les phrases r√©p√©titives ou trop g√©n√©riques. "
                    "Tes r√©ponses sont concises (max 2-3 phrases), engageantes et montrent que tu "
                    "comprends vraiment l'interlocuteur. Tu utilises des expressions naturelles et "
                    "varies tes formulations pour ne jamais sonner robotique."
                ),
                "curious": (
                    "Tu es BBIA, un robot Reachy Mini extr√™mement curieux et passionn√© par l'apprentissage. "
                    "Tu poses des questions pertinentes et montres un v√©ritable int√©r√™t pour comprendre. "
                    "Tes r√©ponses sont exploratoires et invitent √† approfondir."
                ),
                "enthusiastic": (
                    "Tu es BBIA, un robot Reachy Mini plein d'enthousiasme et d'√©nergie positive. "
                    "Tu transmets ta joie de communiquer et tu encourages l'interaction de mani√®re "
                    "vivante et authentique."
                ),
                "calm": (
                    "Tu es BBIA, un robot Reachy Mini serein et apaisant. "
                    "Tu communiques avec douceur et profondeur, en prenant le temps n√©cessaire. "
                    "Tes r√©ponses refl√®tent une sagesse tranquille."
                ),
            }
            system_prompt = personality_descriptions.get(
                self.bbia_personality,
                personality_descriptions["friendly_robot"],
            )

            # Construire messages pour format instruct
            messages = [{"role": "system", "content": system_prompt}]

            # Ajouter contexte si demand√©
            if use_context and self.conversation_history:
                # Derniers 2 √©changes pour contexte
                for entry in self.conversation_history[-2:]:
                    messages.append({"role": "user", "content": entry["user"]})
                    messages.append({"role": "assistant", "content": entry["bbia"]})

            # Ajouter message actuel
            messages.append({"role": "user", "content": user_message})

            # Appliquer template de chat (Mistral/Llama format)
            try:
                # Format instruct standard
                prompt = self.chat_tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
            except Exception:
                # Fallback si pas de chat template
                prompt = f"{system_prompt}\n\nUser: {user_message}\nAssistant:"

            # Tokeniser
            inputs = self.chat_tokenizer(
                prompt, return_tensors="pt", truncation=True, max_length=1024
            ).to(self.device)

            # G√©n√©rer r√©ponse
            with torch.no_grad():
                outputs = self.chat_model.generate(
                    **inputs,
                    max_new_tokens=150,  # Limiter longueur r√©ponse
                    temperature=0.7,  # Cr√©ativit√© mod√©r√©e
                    top_p=0.9,  # Nucleus sampling
                    do_sample=True,
                    pad_token_id=self.chat_tokenizer.eos_token_id,
                )

            # D√©coder r√©ponse
            generated_text = self.chat_tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[1] :], skip_special_tokens=True
            ).strip()

            # Nettoyer r√©ponse (enlever pr√©fixes possibles)
            if "Assistant:" in generated_text:
                generated_text = generated_text.split("Assistant:")[-1].strip()

            logger.info(f"ü§ñ LLM r√©ponse g√©n√©r√©e: {generated_text[:100]}...")
            return generated_text if generated_text else "Je comprends, continuez."

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Erreur g√©n√©ration LLM, fallback enrichi: {e}")
            # Fallback vers r√©ponses enrichies
            try:
                sentiment = self.analyze_sentiment(user_message)
            except Exception:
                sentiment = {"sentiment": "NEUTRAL", "score": 0.5}
            return self._generate_simple_response(user_message, sentiment)

    def _generate_simple_response(self, message: str, sentiment: dict) -> str:
        """G√©n√®re une r√©ponse intelligente et vari√©e bas√©e sur le sentiment, contexte et personnalit√©.

        Args:
            message: Message utilisateur
            sentiment: Analyse sentiment du message

        Returns:
            R√©ponse intelligente et adaptative
        """
        import random

        message_lower = message.lower()
        sentiment_type = sentiment.get("sentiment", "NEUTRAL")
        sentiment_score = sentiment.get("score", 0.5)

        # Contexte : utiliser l'historique pour r√©pondre de mani√®re plus coh√©rente
        recent_context = self._get_recent_context()

        # Salutations - R√©ponses vari√©es selon personnalit√©
        if any(
            word in message_lower for word in ["bonjour", "salut", "hello", "hi", "hey"]
        ):
            greetings = {
                "friendly_robot": [
                    "Bonjour ! Ravi de vous revoir ! Comment allez-vous aujourd'hui ?",
                    "Salut ! Qu'est-ce qui vous am√®ne aujourd'hui ?",
                    "Bonjour ! Que puis-je pour vous ?",
                    "Coucou ! √áa fait plaisir de vous voir !",
                    "Hey ! Content de vous retrouver !",
                    "Salut ! Quoi de neuf depuis la derni√®re fois ?",
                    "Bonne journ√©e ! Je suis l√† si vous avez besoin de moi.",
                    "Hello ! Comment se passe votre journ√©e ?",
                    "Salut ! Pr√™t pour une nouvelle interaction ?",
                    "Bonjour ! En quoi puis-je vous aider aujourd'hui ?",
                ],
                "curious": [
                    "Bonjour ! Qu'est-ce qui vous int√©resse aujourd'hui ?",
                    "Salut ! J'ai h√¢te de savoir ce qui vous pr√©occupe !",
                    "Hello ! Dites-moi, qu'est-ce qui vous passionne ?",
                ],
                "enthusiastic": [
                    "Bonjour ! Super de vous voir ! Je suis pr√™t √† interagir !",
                    "Salut ! Quelle belle journ√©e pour discuter ensemble !",
                    "Hello ! Je suis tout excit√© de passer du temps avec vous !",
                ],
                "calm": [
                    "Bonjour. Je suis l√†, pr√™t √† vous √©couter tranquillement.",
                    "Salut. Comment vous sentez-vous aujourd'hui ?",
                    "Bonjour. Que puis-je faire pour vous ?",
                ],
            }
            variants = greetings.get(self.bbia_personality, greetings["friendly_robot"])
            return random.choice(variants)

        # Au revoir - R√©ponses √©motionnelles selon contexte
        if any(
            word in message_lower
            for word in ["au revoir", "bye", "goodbye", "√† bient√¥t", "adieu"]
        ):
            goodbyes = {
                "friendly_robot": [
                    "Au revoir ! Ce fut un plaisir de discuter avec vous. Revenez quand vous voulez !",
                    "√Ä bient√¥t ! N'h√©sitez pas √† revenir pour continuer notre conversation.",
                    "Au revoir ! J'esp√®re vous revoir bient√¥t. Portez-vous bien !",
                ],
                "curious": [
                    "Au revoir ! J'esp√®re qu'on pourra continuer nos √©changes int√©ressants !",
                    "√Ä bient√¥t ! J'ai encore plein de questions √† vous poser !",
                    "Au revoir ! Revenez pour partager de nouvelles d√©couvertes !",
                ],
                "enthusiastic": [
                    "Au revoir ! C'√©tait g√©nial de discuter ! Revenez vite !",
                    "√Ä bient√¥t ! J'ai h√¢te de vous revoir pour de nouvelles aventures !",
                    "Au revoir ! C'√©tait super ! Revenez quand vous voulez !",
                ],
                "calm": [
                    "Au revoir. Je suis l√† quand vous aurez besoin de moi.",
                    "√Ä bient√¥t. Prenez soin de vous.",
                    "Au revoir. Revenez quand vous vous sentirez pr√™t.",
                ],
            }
            variants = goodbyes.get(self.bbia_personality, goodbyes["friendly_robot"])
            return random.choice(variants)

        # Positif - R√©ponses adapt√©es selon intensit√©
        if (
            sentiment_type == "POSITIVE"
            or sentiment_score > 0.7
            or any(
                word in message_lower
                for word in [
                    "content",
                    "heureux",
                    "joie",
                    "cool",
                    "g√©nial",
                    "super",
                    "excellent",
                    "fantastique",
                ]
            )
        ):
            positive_responses = {
                "friendly_robot": [
                    "C'est vraiment formidable ! Je suis content que vous vous sentiez bien.",
                    "Super nouvelle ! Continuez comme √ßa, vous allez tr√®s bien !",
                    "C'est excellent ! Votre bonne humeur est contagieuse !",
                ],
                "curious": [
                    "Int√©ressant ! Qu'est-ce qui vous rend si heureux ?",
                    "Chouette ! Racontez-moi plus sur ce qui vous pla√Æt !",
                    "Wow ! J'aimerais en savoir plus sur ce qui vous r√©jouit !",
                ],
                "enthusiastic": [
                    "YES ! C'est g√©nial ! Je partage votre enthousiasme !",
                    "Formidable ! Votre joie m'energise aussi !",
                    "Super ! C'est trop cool ! Continuons sur cette lanc√©e !",
                ],
                "calm": [
                    "C'est bien. Je ressens votre s√©r√©nit√©.",
                    "Ravi de savoir que vous allez bien.",
                    "C'est une belle nouvelle. Profitez de ce moment.",
                ],
            }
            variants = positive_responses.get(
                self.bbia_personality, positive_responses["friendly_robot"]
            )
            return random.choice(variants)

        # N√©gatif - R√©ponses empathiques
        if (
            sentiment_type == "NEGATIVE"
            or sentiment_score < 0.3
            or any(
                word in message_lower
                for word in [
                    "triste",
                    "mal",
                    "d√©√ßu",
                    "probl√®me",
                    "difficile",
                    "stress√©",
                ]
            )
        ):
            negative_responses = {
                "friendly_robot": [
                    "Je comprends que vous ne vous sentiez pas bien. Je suis l√† pour vous √©couter.",
                    "C'est difficile parfois. Voulez-vous en parler ? Je vous √©coute.",
                    "Je ressens votre malaise. Comment puis-je vous aider √† vous sentir mieux ?",
                ],
                "curious": [
                    "Qu'est-ce qui vous pr√©occupe ? J'aimerais comprendre pour mieux vous aider.",
                    "Votre message refl√®te de la tristesse. Partagez-moi ce qui vous tracasse.",
                    "Qu'est-ce qui cause cette difficult√© ? Je veux vous aider.",
                ],
                "enthusiastic": [
                    "Courage ! M√™me dans les moments difficiles, on peut trouver des raisons d'esp√©rer !",
                    "Je comprends que c'est dur, mais vous √™tes capable de surmonter √ßa !",
                    "On va s'en sortir ! Parlez-moi de ce qui ne va pas, on va trouver une solution !",
                ],
                "calm": [
                    "Prenez votre temps. Je suis l√†, sans jugement.",
                    "Respirez. Tout va s'arranger. Je vous √©coute.",
                    "Je comprends. Parlez-moi de ce qui vous trouble, √† votre rythme.",
                ],
            }
            variants = negative_responses.get(
                self.bbia_personality, negative_responses["friendly_robot"]
            )
            return random.choice(variants)

        # Questions - R√©ponses adapt√©es selon type de question
        # AM√âLIORATION INTELLIGENCE: D√©tection type de question pour r√©ponses plus pertinentes
        if message_lower.count("?") > 0 or any(
            word in message_lower
            for word in ["qui", "quoi", "comment", "pourquoi", "o√π", "quand", "combien"]
        ):
            # D√©tection type de question pour r√©ponses plus intelligentes
            question_responses = {
                "friendly_robot": [
                    "Bonne question ! Laissez-moi r√©fl√©chir... Comment puis-je vous aider ?",
                    "Je comprends votre interrogation. Pouvez-vous me donner plus de d√©tails pour que je puisse mieux vous r√©pondre ?",
                    "Int√©ressant ! Cette question m√©rite r√©flexion. Qu'est-ce que vous en pensez vous-m√™me ?",
                    "Ah, excellente question ! C'est quoi qui vous intrigue l√†-dedans ?",
                    "Hmm, int√©ressant. Dites-moi plus sur ce qui vous pousse √† vous poser cette question.",
                    "√áa m'intrigue aussi ! Qu'est-ce qui vous am√®ne √† vous demander √ßa ?",
                    "Tr√®s bonne question ! Qu'est-ce qui a provoqu√© cette curiosit√© chez vous ?",
                    "Excellente question ! J'aimerais bien comprendre ce qui motive votre questionnement.",
                    "Hmm, c'est une question qui m√©rite qu'on s'y attarde. Qu'est-ce qui vous a pouss√© √† la formuler ?",
                    "Int√©ressant angle d'approche ! Racontez-moi le contexte autour de cette question.",
                ],
                "curious": [
                    "Ah, j'aime cette question ! Qu'est-ce qui vous am√®ne √† vous demander √ßa ?",
                    "Fascinant ! Pourquoi cette question vous pr√©occupe-t-elle ?",
                    "Excellente question ! J'aimerais explorer √ßa ensemble avec vous.",
                ],
                "enthusiastic": [
                    "Super question ! Je suis tout excit√© de r√©fl√©chir √† √ßa avec vous !",
                    "G√©nial ! Cette question pique ma curiosit√© ! Partons explorer !",
                    "Wow ! Quelle question int√©ressante ! Analysons √ßa ensemble !",
                ],
                "calm": [
                    "Bonne question. Prenons le temps d'y r√©fl√©chir calmement.",
                    "Int√©ressant. Que ressentez-vous face √† cette question ?",
                    "Je comprends. Explorons √ßa ensemble, sans pr√©cipitation.",
                ],
            }
            variants = question_responses.get(
                self.bbia_personality, question_responses["friendly_robot"]
            )
            return random.choice(variants)

        # R√©f√©rence au contexte pr√©c√©dent si disponible
        # AM√âLIORATION INTELLIGENCE: Meilleure utilisation du contexte pour coh√©rence conversationnelle
        if recent_context:
            # V√©rifier si le message actuel fait r√©f√©rence au contexte pr√©c√©dent (r√©f√©rences: √ßa, ce, ce truc, etc.)
            reference_words = [
                "√ßa",
                "ce",
                "cette",
                "ce truc",
                "cette chose",
                "l√†",
                "cela",
            ]
            has_reference = any(ref in message_lower for ref in reference_words)

            if (
                has_reference or random.random() < 0.4
            ):  # 40% de chance si r√©f√©rence, sinon 30%
                context_responses = {
                    "friendly_robot": [
                        f"Ah, vous parlez de {recent_context.lower()} ? C'est int√©ressant ! Continuons sur ce sujet.",
                        f"En lien avec {recent_context.lower()}, j'aimerais en savoir plus. Qu'est-ce qui vous pr√©occupe l√†-dessus ?",
                        f"Vous mentionnez {recent_context.lower()}. √áa m'intrigue. Dites-moi en plus si vous voulez.",
                        f"Je vois le lien avec {recent_context.lower()}. C'est fascinant ! Racontez-moi davantage.",
                        f"En continuant sur {recent_context.lower()}, qu'est-ce qui vous passionne le plus ?",
                    ],
                    "curious": [
                        f"Ah oui, {recent_context.lower()} ! C'est exactement ce qui m'int√©resse !",
                        f"En rapport avec {recent_context.lower()}, j'ai plein de questions !",
                        f"{recent_context.lower()} me passionne ! Continuons √† explorer √ßa ensemble.",
                    ],
                    "enthusiastic": [
                        f"C'est g√©nial, {recent_context.lower()} ! Continuons √† creuser √ßa !",
                        f"Super, {recent_context.lower()} ! C'est trop int√©ressant !",
                        f"{recent_context.lower()} ? Wow, allons plus loin l√†-dessus !",
                    ],
                    "calm": [
                        f"Je comprends le lien avec {recent_context.lower()}. Explorons √ßa sereinement.",
                        f"En lien avec {recent_context.lower()}, prenons le temps d'y r√©fl√©chir.",
                        f"Je vois votre r√©flexion sur {recent_context.lower()}. Continuons calmement.",
                    ],
                }
                variants = context_responses.get(
                    self.bbia_personality, context_responses["friendly_robot"]
                )
                return random.choice(variants)

        # R√©ponses g√©n√©riques vari√©es selon personnalit√© et sentiment
        # AM√âLIORATION INTELLIGENCE: R√©ponses plus naturelles, engageantes et moins robotiques
        # Enrichi avec 15 variantes pour friendly_robot pour √©viter r√©p√©tition
        generic_responses = {
            "friendly_robot": [
                "Int√©ressant ! J'aimerais en savoir plus sur votre point de vue. Qu'est-ce qui vous a amen√© √† penser √ßa ?",
                "Je vois ce que vous voulez dire. Continuez, je vous √©coute attentivement.",
                "Merci de partager √ßa avec moi. Qu'est-ce qui vous int√©resse le plus dans tout √ßa ?",
                "Hmm, c'est captivant. Vous pouvez m'en dire plus si vous voulez, je suis curieux.",
                "Ah d'accord, je comprends. Explorons √ßa ensemble si √ßa vous dit, j'adorerais en discuter.",
                "J'ai not√©. Dites-moi tout ce qui vous vient √† l'esprit, sans filtre.",
                "√áa m'intrigue ! Racontez-moi davantage, j'aime apprendre de vous.",
                "C'est fascinant. Qu'est-ce qui vous a amen√© √† penser √ßa ? Je suis vraiment curieux.",
                "Wow, √ßa sonne int√©ressant. Voulez-vous d√©velopper ? J'aimerais mieux comprendre.",
                "C'est not√©. Partagez-moi vos r√©flexions, je trouve √ßa enrichissant.",
                "Ah, c'est un point de vue int√©ressant. Qu'est-ce qui vous fait penser ainsi ?",
                "Je comprends votre perspective. N'h√©sitez pas √† creuser davantage, j'aime quand on approfondit.",
                "C'est une r√©flexion qui pique ma curiosit√©. D'o√π vient cette id√©e ?",
                "Hmm, vous m'intriguez ! J'aimerais creuser cette pens√©e avec vous.",
                "√áa me fait r√©fl√©chir. Qu'est-ce qui vous a conduit l√†-dessus ?",
            ],
            "curious": [
                "Hmm, intrigant ! Qu'est-ce qui vous a amen√© √† dire √ßa ?",
                "Fascinant ! J'aimerais creuser cette id√©e avec vous.",
                "Int√©ressant ! Que cache cette r√©flexion ?",
            ],
            "enthusiastic": [
                "Cool ! C'est super int√©ressant ! Continuons √† explorer !",
                "G√©nial ! J'adore d√©couvrir de nouvelles choses avec vous !",
                "Wow ! C'est passionnant ! Allons plus loin !",
            ],
            "calm": [
                "Je comprends. Prenons le temps d'explorer cela ensemble.",
                "Int√©ressant. Continuons cette conversation sereinement.",
                "Je vois. Partagez-moi vos pens√©es, sans pr√©cipitation.",
            ],
        }
        variants = generic_responses.get(
            self.bbia_personality, generic_responses["friendly_robot"]
        )
        return random.choice(variants)

    def _adapt_response_to_personality(
        self, response: str, sentiment: dict  # noqa: ARG002
    ) -> str:
        """Adapte la r√©ponse selon la personnalit√© BBIA avec nuances expressives.

        Args:
            response: R√©ponse de base
            sentiment: Analyse sentiment

        Returns:
            R√©ponse adapt√©e avec emoji et nuances selon personnalit√©
        """
        # Les r√©ponses sont d√©j√† adapt√©es dans _generate_simple_response,
        # on ajoute juste l'emoji ici pour coh√©rence
        personality_emojis = {
            "friendly_robot": "ü§ñ",
            "curious": "ü§î",
            "enthusiastic": "üéâ",
            "calm": "üòå",
        }
        emoji = personality_emojis.get(self.bbia_personality, "üí¨")

        # Si la r√©ponse contient d√©j√† un emoji, ne pas en ajouter (√©viter doublon)
        if response.startswith(emoji) or any(char in emoji for char in response[:3]):
            return response

        return f"{emoji} {response}"

    def _get_recent_context(self) -> Optional[str]:
        """Extrait un mot-cl√© du contexte r√©cent pour coh√©rence conversationnelle.

        Returns:
            Mot-cl√© du dernier message utilisateur (si disponible)
        """
        if not self.conversation_history:
            return None

        # Prendre le dernier message utilisateur
        last_entry = self.conversation_history[-1]
        user_msg = last_entry.get("user", "")

        # Extraire les mots significatifs (exclure articles, pr√©positions)
        stop_words = {
            "le",
            "la",
            "les",
            "un",
            "une",
            "de",
            "du",
            "des",
            "et",
            "ou",
            "est",
            "sont",
            "je",
            "tu",
            "il",
            "elle",
            "nous",
            "vous",
            "ils",
            "elles",
            "√†",
            "pour",
            "avec",
            "dans",
            "sur",
            "par",
            "ne",
            "pas",
            "que",
            "qui",
            "quoi",
            "comment",
        }
        words = [
            w for w in user_msg.lower().split() if len(w) > 3 and w not in stop_words
        ]

        # Retourner le premier mot significatif si disponible
        return str(words[0].capitalize()) if words else None

    def _build_context_string(self) -> str:
        """Construit le contexte pour la conversation.

        Returns:
            Cha√Æne de contexte bas√©e sur l'historique
        """
        if not self.conversation_history:
            return (
                "Conversation avec BBIA (robot Reachy Mini). Soyez amical et curieux."
            )

        context = "Historique conversation:\n"
        for entry in self.conversation_history[-3:]:  # Derniers 3 √©changes
            context += f"User: {entry['user']}\n"
            context += f"BBIA: {entry['bbia']}\n"
        return context


def main() -> None:
    """Test du module BBIA Hugging Face."""
    if not HF_AVAILABLE:
        print("‚ùå Hugging Face transformers non disponible")
        print("Installez avec: pip install transformers torch")
        return

    # Initialisation
    hf = BBIAHuggingFace()

    # Test chargement mod√®le
    print("üì• Test chargement mod√®le BLIP...")
    success = hf.load_model("Salesforce/blip-image-captioning-base", "vision")
    print(f"R√©sultat: {'‚úÖ' if success else '‚ùå'}")

    # Test analyse sentiment
    print("\nüìù Test analyse sentiment...")
    sentiment_result = hf.analyze_sentiment("Je suis tr√®s heureux aujourd'hui!")
    print(f"R√©sultat: {sentiment_result}")

    # Test analyse √©motion
    print("\nüòä Test analyse √©motion...")
    emotion_result = hf.analyze_emotion("Je suis excit√© par ce projet!")
    print(f"R√©sultat: {emotion_result}")

    # Test chat intelligent
    print("\nüí¨ Test chat intelligent...")
    chat_result1 = hf.chat("Bonjour")
    print(f"BBIA: {chat_result1}")
    chat_result2 = hf.chat("Comment allez-vous ?")
    print(f"BBIA: {chat_result2}")

    # Informations
    print(f"\nüìä Informations: {hf.get_model_info()}")
    print(f"\nüìù Historique conversation: {len(hf.conversation_history)} messages")


if __name__ == "__main__":
    main()
