#!/usr/bin/env python3
"""
BBIA Hugging Face Integration - Module d'int√©gration des mod√®les pr√©-entra√Æn√©s
Int√©gration avanc√©e avec Hugging Face Hub pour enrichir les capacit√©s IA de BBIA-SIM
"""

import logging
import os
from typing import Any, Optional, Union

import numpy as np
from PIL import Image

# D√©sactiver les avertissements de transformers
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

logger = logging.getLogger(__name__)

# Import conditionnel des d√©pendances Hugging Face
try:
    import warnings

    import torch

    # Supprimer les avertissements de transformers
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        from transformers import (
            BlipForConditionalGeneration,
            BlipProcessor,
            CLIPModel,
            CLIPProcessor,
            WhisperForConditionalGeneration,
            WhisperProcessor,
            pipeline,
        )
        from transformers.utils import logging as transformers_logging

        # R√©duire la verbosit√© de transformers
        transformers_logging.set_verbosity_error()

    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    logger.warning(
        "Hugging Face transformers non disponible. Installez avec: pip install transformers torch"
    )


class BBIAHuggingFace:
    """Module d'int√©gration Hugging Face pour BBIA-SIM.

    Fonctionnalit√©s :
    - Vision : CLIP, BLIP pour description d'images
    - Audio : Whisper pour STT avanc√©
    - NLP : Mod√®les de sentiment, √©motions
    - Multimodal : Mod√®les combinant vision + texte
    """

    def __init__(self, device: str = "auto", cache_dir: Optional[str] = None):
        """Initialise le module Hugging Face.

        Args:
            device: Device pour les mod√®les ("cpu", "cuda", "auto")
            cache_dir: R√©pertoire de cache pour les mod√®les
        """
        if not HF_AVAILABLE:
            raise ImportError(
                "Hugging Face transformers requis. Installez avec: pip install transformers torch"
            )

        self.device = self._get_device(device)
        self.cache_dir = cache_dir
        self.models: dict[str, Any] = {}
        self.processors: dict[str, Any] = {}

        # Chat intelligent : Historique et contexte
        self.conversation_history: list[dict[str, Any]] = []
        self.context: dict[str, Any] = {}
        self.bbia_personality = "friendly_robot"

        # Configuration des mod√®les recommand√©s
        self.model_configs = {
            "vision": {
                "clip": "openai/clip-vit-base-patch32",
                "blip": "Salesforce/blip-image-captioning-base",
            },
            "audio": {
                "whisper": "openai/whisper-base",
            },
            "nlp": {
                "sentiment": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                "emotion": "j-hartmann/emotion-english-distilroberta-base",
            },
            "multimodal": {
                "blip_vqa": "Salesforce/blip-vqa-base",
            },
        }

        logger.info(f"ü§ó BBIA Hugging Face initialis√© (device: {self.device})")
        logger.info(f"üòä Personnalit√© BBIA: {self.bbia_personality}")

    def _get_device(self, device: str) -> str:
        """D√©termine le device optimal."""
        if device == "auto":
            if HF_AVAILABLE and torch.cuda.is_available():
                return "cuda"
            elif HF_AVAILABLE and torch.backends.mps.is_available():
                return "mps"  # Apple Silicon
            else:
                return "cpu"
        return device

    def load_model(self, model_name: str, model_type: str = "vision") -> bool:
        """Charge un mod√®le Hugging Face.

        Args:
            model_name: Nom du mod√®le ou chemin
            model_type: Type de mod√®le ("vision", "audio", "nlp", "multimodal")

        Returns:
            True si charg√© avec succ√®s
        """
        try:
            logger.info(f"üì• Chargement mod√®le {model_name} ({model_type})")

            if model_type == "vision":
                if "clip" in model_name.lower():
                    processor = CLIPProcessor.from_pretrained(  # nosec B615
                        model_name, cache_dir=self.cache_dir
                    )
                    model = CLIPModel.from_pretrained(  # nosec B615
                        model_name, cache_dir=self.cache_dir
                    ).to(self.device)
                    self.processors[f"{model_name}_processor"] = processor
                    self.models[f"{model_name}_model"] = model

                elif "blip" in model_name.lower():
                    processor = (
                        BlipProcessor.from_pretrained(  # nosec B615 # type: ignore
                            model_name, cache_dir=self.cache_dir
                        )
                    )
                    model = BlipForConditionalGeneration.from_pretrained(  # nosec B615
                        model_name, cache_dir=self.cache_dir
                    ).to(self.device)
                    self.processors[f"{model_name}_processor"] = processor  # type: ignore
                    self.models[f"{model_name}_model"] = model

            elif model_type == "audio":
                if "whisper" in model_name.lower():
                    processor = (
                        WhisperProcessor.from_pretrained(  # nosec B615 # type: ignore
                            model_name, cache_dir=self.cache_dir
                        )
                    )
                    model = (
                        WhisperForConditionalGeneration.from_pretrained(  # nosec B615
                            model_name, cache_dir=self.cache_dir
                        ).to(self.device)
                    )
                    self.processors[f"{model_name}_processor"] = processor  # type: ignore
                    self.models[f"{model_name}_model"] = model

            elif model_type == "nlp":
                # Utilisation des pipelines pour NLP
                pipeline_name = self._get_pipeline_name(model_name)
                pipe = pipeline(  # type: ignore[call-overload]
                    pipeline_name, model=model_name, device=self.device
                )
                self.models[f"{model_name}_pipeline"] = pipe

            elif model_type == "multimodal":
                if "blip" in model_name.lower() and "vqa" in model_name.lower():
                    processor = (
                        BlipProcessor.from_pretrained(  # nosec B615 # type: ignore
                            model_name, cache_dir=self.cache_dir
                        )
                    )
                    model = BlipForConditionalGeneration.from_pretrained(  # nosec B615
                        model_name, cache_dir=self.cache_dir
                    ).to(self.device)
                    self.processors[f"{model_name}_processor"] = processor  # type: ignore
                    self.models[f"{model_name}_model"] = model

            logger.info(f"‚úÖ Mod√®le {model_name} charg√© avec succ√®s")
            return True

        except Exception as e:
            logger.error(f"‚ùå Erreur chargement mod√®le {model_name}: {e}")
            return False

    def _get_pipeline_name(self, model_name: str) -> str:
        """D√©termine le nom du pipeline bas√© sur le mod√®le."""
        if "sentiment" in model_name.lower():
            return "sentiment-analysis"
        elif "emotion" in model_name.lower():
            return "text-classification"
        else:
            return "text-classification"

    def describe_image(
        self, image: Union[str, Image.Image, np.ndarray], model_name: str = "blip"
    ) -> str:
        """D√©crit une image avec BLIP ou CLIP.

        Args:
            image: Image √† d√©crire (chemin, PIL Image, ou numpy array)
            model_name: Nom du mod√®le √† utiliser

        Returns:
            Description textuelle de l'image
        """
        try:
            # Conversion de l'image
            if isinstance(image, str):
                image = Image.open(image)
            elif isinstance(image, np.ndarray):
                image = Image.fromarray(image)

            if "blip" in model_name.lower():
                processor_key = f"{model_name}_processor"
                model_key = f"{model_name}_model"

                if processor_key not in self.processors or model_key not in self.models:
                    self.load_model(model_name, "vision")

                processor = self.processors[processor_key]
                model = self.models[model_key]

                inputs = processor(image, return_tensors="pt").to(self.device)
                out = model.generate(**inputs, max_length=50)
                description = processor.decode(out[0], skip_special_tokens=True)

                return str(description)

            elif "clip" in model_name.lower():
                processor_key = f"{model_name}_processor"
                model_key = f"{model_name}_model"

                if processor_key not in self.processors or model_key not in self.models:
                    self.load_model(model_name, "vision")

                processor = self.processors[processor_key]
                model = self.models[model_key]

                inputs = processor(
                    text=["a photo of"], images=image, return_tensors="pt", padding=True
                ).to(self.device)
                outputs = model(**inputs)
                logits_per_image = outputs.logits_per_image
                probs = logits_per_image.softmax(dim=1)

                return f"CLIP analysis: {probs.cpu().numpy()}"

            return "Erreur: mod√®le non support√©"

        except Exception as e:
            logger.error(f"‚ùå Erreur description image: {e}")
            return "Erreur lors de la description de l'image"

    def analyze_sentiment(
        self,
        text: str,
        model_name: str = "cardiffnlp/twitter-roberta-base-sentiment-latest",
    ) -> dict[str, Any]:
        """Analyse le sentiment d'un texte.

        Args:
            text: Texte √† analyser
            model_name: Nom du mod√®le √† utiliser (mod√®le Hugging Face complet)

        Returns:
            Dictionnaire avec sentiment et score
        """
        try:
            model_key = f"{model_name}_pipeline"

            if model_key not in self.models:
                self.load_model(model_name, "nlp")

            pipeline = self.models[model_key]
            result = pipeline(text)

            return {
                "text": text,
                "sentiment": result[0]["label"],
                "score": result[0]["score"],
                "model": model_name,
            }

        except Exception as e:
            logger.error(f"‚ùå Erreur analyse sentiment: {e}")
            return {"error": str(e)}

    def analyze_emotion(self, text: str, model_name: str = "emotion") -> dict[str, Any]:
        """Analyse les √©motions dans un texte.

        Args:
            text: Texte √† analyser
            model_name: Nom du mod√®le √† utiliser

        Returns:
            Dictionnaire avec √©motion d√©tect√©e et score
        """
        try:
            model_key = f"{model_name}_pipeline"

            if model_key not in self.models:
                self.load_model(model_name, "nlp")

            pipeline = self.models[model_key]
            result = pipeline(text)

            return {
                "text": text,
                "emotion": result[0]["label"],
                "score": result[0]["score"],
                "model": model_name,
            }

        except Exception as e:
            logger.error(f"‚ùå Erreur analyse √©motion: {e}")
            return {"error": str(e)}

    def transcribe_audio(self, audio_path: str, model_name: str = "whisper") -> str:
        """Transcrit un fichier audio avec Whisper.

        Args:
            audio_path: Chemin vers le fichier audio
            model_name: Nom du mod√®le Whisper √† utiliser

        Returns:
            Texte transcrit
        """
        try:
            processor_key = f"{model_name}_processor"
            model_key = f"{model_name}_model"

            if processor_key not in self.processors or model_key not in self.models:
                self.load_model(model_name, "audio")

            processor = self.processors[processor_key]
            model = self.models[model_key]

            # Chargement de l'audio
            audio, sample_rate = processor.load_audio(audio_path)
            inputs = processor(
                audio, sampling_rate=sample_rate, return_tensors="pt"
            ).to(self.device)

            # Transcription
            with torch.no_grad():
                predicted_ids = model.generate(inputs["input_features"])

            transcription = processor.batch_decode(
                predicted_ids, skip_special_tokens=True
            )[0]

            return str(transcription)

        except Exception as e:
            logger.error(f"‚ùå Erreur transcription audio: {e}")
            return "Erreur lors de la transcription"

    def answer_question(
        self,
        image: Union[str, Image.Image],
        question: str,
        model_name: str = "blip_vqa",
    ) -> str:
        """R√©pond √† une question sur une image (VQA).

        Args:
            image: Image √† analyser
            question: Question √† poser
            model_name: Nom du mod√®le VQA √† utiliser

        Returns:
            R√©ponse √† la question
        """
        try:
            # Conversion de l'image
            if isinstance(image, str):
                image = Image.open(image)
            elif isinstance(image, np.ndarray):
                image = Image.fromarray(image)

            processor_key = f"{model_name}_processor"
            model_key = f"{model_name}_model"

            if processor_key not in self.processors or model_key not in self.models:
                self.load_model(model_name, "multimodal")

            processor = self.processors[processor_key]
            model = self.models[model_key]

            inputs = processor(image, question, return_tensors="pt").to(self.device)
            out = model.generate(**inputs, max_length=50)
            answer = processor.decode(out[0], skip_special_tokens=True)

            return str(answer)

        except Exception as e:
            logger.error(f"‚ùå Erreur VQA: {e}")
            return "Erreur lors de l'analyse de l'image"

    def get_available_models(self) -> dict[str, list[str]]:
        """Retourne la liste des mod√®les disponibles par cat√©gorie."""
        return {
            "vision": list(self.model_configs["vision"].keys()),
            "audio": list(self.model_configs["audio"].keys()),
            "nlp": list(self.model_configs["nlp"].keys()),
            "multimodal": list(self.model_configs["multimodal"].keys()),
        }

    def get_loaded_models(self) -> list[str]:
        """Retourne la liste des mod√®les actuellement charg√©s."""
        return list(self.models.keys())

    def unload_model(self, model_name: str) -> bool:
        """D√©charge un mod√®le de la m√©moire.

        Args:
            model_name: Nom du mod√®le √† d√©charger

        Returns:
            True si d√©charg√© avec succ√®s
        """
        try:
            keys_to_remove = [key for key in self.models.keys() if model_name in key]
            for key in keys_to_remove:
                del self.models[key]

            keys_to_remove = [
                key for key in self.processors.keys() if model_name in key
            ]
            for key in keys_to_remove:
                del self.processors[key]

            logger.info(f"üóëÔ∏è Mod√®le {model_name} d√©charg√©")
            return True

        except Exception as e:
            logger.error(f"‚ùå Erreur d√©chargement mod√®le {model_name}: {e}")
            return False

    def get_model_info(self) -> dict[str, Any]:
        """Retourne les informations sur les mod√®les charg√©s."""
        return {
            "device": self.device,
            "loaded_models": self.get_loaded_models(),
            "available_models": self.get_available_models(),
            "cache_dir": self.cache_dir,
            "hf_available": HF_AVAILABLE,
        }

    def chat(self, user_message: str, use_context: bool = True) -> str:
        """Chat intelligent avec BBIA avec contexte et analyse sentiment.

        Args:
            user_message: Message de l'utilisateur
            use_context: Utiliser le contexte des messages pr√©c√©dents

        Returns:
            R√©ponse intelligente de BBIA
        """
        try:
            # 1. Analyser sentiment du message (avec gestion erreur)
            try:
                sentiment = self.analyze_sentiment(user_message)
            except Exception:
                # Fallback si sentiment indisponible
                sentiment = {"sentiment": "NEUTRAL", "score": 0.5}

            # 2. G√©n√©rer r√©ponse bas√©e sur le message et le sentiment
            bbia_response = self._generate_simple_response(user_message, sentiment)

            # 3. Sauvegarder dans l'historique
            from datetime import datetime

            self.conversation_history.append(
                {
                    "user": user_message,
                    "bbia": bbia_response,
                    "sentiment": sentiment,
                    "timestamp": datetime.now().isoformat(),
                }
            )

            # 4. Adapter r√©ponse selon personnalit√© BBIA
            adapted_response = self._adapt_response_to_personality(
                bbia_response, sentiment
            )

            return adapted_response

        except Exception as e:
            logger.error(f"‚ùå Erreur chat: {e}")
            return "Je ne comprends pas bien, peux-tu reformuler ?"

    def _generate_simple_response(self, message: str, sentiment: dict) -> str:
        """G√©n√®re une r√©ponse simple bas√©e sur le sentiment et mots-cl√©s.

        Args:
            message: Message utilisateur
            sentiment: Analyse sentiment du message

        Returns:
            R√©ponse basique et intelligente
        """
        message_lower = message.lower()

        # Salutations
        if any(word in message_lower for word in ["bonjour", "salut", "hello", "hi"]):
            return "Bonjour ! Comment allez-vous ? Je suis BBIA, votre robot compagnon."

        # Au revoir
        if any(
            word in message_lower
            for word in ["au revoir", "bye", "goodbye", "√† bient√¥t"]
        ):
            return "Au revoir ! Ce fut un plaisir de discuter avec vous. √Ä bient√¥t !"

        # Positif
        if sentiment.get("sentiment") == "POSITIVE" or any(
            word in message_lower for word in ["content", "heureux", "joie", "cool"]
        ):
            return "C'est super ! Je suis content pour vous. Continuez comme √ßa !"

        # Question
        if message_lower.count("?") > 0 or any(
            word in message_lower for word in ["qui", "quoi", "comment", "pourquoi"]
        ):
            return "C'est une bonne question ! Je r√©fl√©chis... Comment puis-je vous aider ?"

        # R√©ponse par d√©faut intelligente
        return "C'est int√©ressant. Dites-moi en plus s'il vous pla√Æt."

    def _adapt_response_to_personality(self, response: str, sentiment: dict) -> str:
        """Adapte la r√©ponse selon la personnalit√© BBIA.

        Args:
            response: R√©ponse de base
            sentiment: Analyse sentiment

        Returns:
            R√©ponse adapt√©e avec emoji selon personnalit√©
        """
        personality_responses = {
            "friendly_robot": f"ü§ñ {response}",
            "curious": f"ü§î {response}",
            "enthusiastic": f"üéâ {response}",
            "calm": f"üòå {response}",
        }
        return personality_responses.get(self.bbia_personality, f"üí¨ {response}")

    def _build_context_string(self) -> str:
        """Construit le contexte pour la conversation.

        Returns:
            Cha√Æne de contexte bas√©e sur l'historique
        """
        if not self.conversation_history:
            return (
                "Conversation avec BBIA (robot Reachy Mini). Soyez amical et curieux."
            )

        context = "Historique conversation:\n"
        for entry in self.conversation_history[-3:]:  # Derniers 3 √©changes
            context += f"User: {entry['user']}\n"
            context += f"BBIA: {entry['bbia']}\n"
        return context


def main():
    """Test du module BBIA Hugging Face."""
    if not HF_AVAILABLE:
        print("‚ùå Hugging Face transformers non disponible")
        print("Installez avec: pip install transformers torch")
        return

    # Initialisation
    hf = BBIAHuggingFace()

    # Test chargement mod√®le
    print("üì• Test chargement mod√®le BLIP...")
    success = hf.load_model("Salesforce/blip-image-captioning-base", "vision")
    print(f"R√©sultat: {'‚úÖ' if success else '‚ùå'}")

    # Test analyse sentiment
    print("\nüìù Test analyse sentiment...")
    sentiment_result = hf.analyze_sentiment("Je suis tr√®s heureux aujourd'hui!")
    print(f"R√©sultat: {sentiment_result}")

    # Test analyse √©motion
    print("\nüòä Test analyse √©motion...")
    emotion_result = hf.analyze_emotion("Je suis excit√© par ce projet!")
    print(f"R√©sultat: {emotion_result}")

    # Test chat intelligent
    print("\nüí¨ Test chat intelligent...")
    chat_result1 = hf.chat("Bonjour")
    print(f"BBIA: {chat_result1}")
    chat_result2 = hf.chat("Comment allez-vous ?")
    print(f"BBIA: {chat_result2}")

    # Informations
    print(f"\nüìä Informations: {hf.get_model_info()}")
    print(f"\nüìù Historique conversation: {len(hf.conversation_history)} messages")


if __name__ == "__main__":
    main()
