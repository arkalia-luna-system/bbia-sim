#!/usr/bin/env python3
"""bbia_voice_whisper.py - Module Whisper STT pour BBIA
Int√©gration Speech-to-Text avec OpenAI Whisper (optionnel)
"""

import logging
import os
import threading
import time
from collections import deque
from pathlib import Path
from typing import Any, cast

import numpy.typing as npt

# D√©clarer whisper comme Any d√®s le d√©but pour √©viter conflit de types
whisper: Any

try:
    import whisper as _whisper_module

    WHISPER_AVAILABLE = True
    whisper = _whisper_module
except ImportError:
    WHISPER_AVAILABLE = False
    whisper = None

# Imports optionnels pour les patches dans les tests
try:
    from transformers import pipeline as transformers_pipeline
except ImportError:
    transformers_pipeline = None

try:
    import soundfile as sf
except ImportError:
    sf = None

try:
    import sounddevice as sd
except (ImportError, OSError):
    # OSError: PortAudio library not found (CI/headless)
    sd = None

logger = logging.getLogger(__name__)

# OPTIMISATION PERFORMANCE: Cache global pour mod√®le VAD (√©vite chargements r√©p√©t√©s entre instances)
_vad_model_cache: Any | None = None
_vad_cache_lock = threading.Lock()

# OPTIMISATION RAM: Cache global LRU pour mod√®les Whisper (max 2 mod√®les: tiny, base)
_whisper_models_cache: dict[str, Any] = {}  # model_size -> model
_whisper_model_last_used: dict[str, float] = {}
_whisper_model_cache_lock = threading.Lock()
_MAX_WHISPER_CACHE_SIZE = (
    2  # OPTIMISATION RAM: Limiter √† 2 mod√®les Whisper max (tiny, base)
)


class WhisperSTT:
    """Module Speech-to-Text utilisant OpenAI Whisper."""

    def __init__(
        self,
        model_size: str = "tiny",
        language: str = "fr",
        enable_vad: bool = True,
    ):
        """Initialise le module Whisper STT.

        Args:
            model_size: Taille du mod√®le ("tiny", "base", "small", "medium", "large")
            language: Langue cible ("fr", "en", "auto")
            enable_vad: Activer la d√©tection d'activit√© vocale (VAD) pour activation auto

        """
        self.model_size = model_size
        self.language = language
        self.model = None
        self.is_loaded = False
        self.enable_vad = enable_vad
        self._vad_model: Any | None = None
        self._vad_loaded = False

        if not WHISPER_AVAILABLE:
            logger.warning(
                "‚ö†Ô∏è Whisper non disponible. Fallback vers speech_recognition.",
            )
            return

        logger.info(
            f"üé§ Initialisation Whisper STT (mod√®le: {model_size}, langue: {language}, VAD: {enable_vad})",
        )

    def load_model(self) -> bool:
        """Charge le mod√®le Whisper (utilise cache global si disponible)."""
        if not WHISPER_AVAILABLE:
            return False

        # OPTIMISATION RAM: Utiliser cache global LRU pour √©viter chargements r√©p√©t√©s
        global _whisper_models_cache, _whisper_model_last_used
        with _whisper_model_cache_lock:
            if self.model_size in _whisper_models_cache:
                logger.debug(
                    f"‚ôªÔ∏è R√©utilisation mod√®le Whisper depuis cache ({self.model_size})",
                )
                self.model = _whisper_models_cache[self.model_size]
                # OPTIMISATION RAM: Mettre √† jour timestamp usage
                _whisper_model_last_used[self.model_size] = time.time()
                self.is_loaded = True
                return True

            # OPTIMISATION RAM: V√©rifier limite cache et d√©charger LRU si n√©cessaire
            if len(_whisper_models_cache) >= _MAX_WHISPER_CACHE_SIZE:
                # Trouver mod√®le le moins r√©cemment utilis√©
                if _whisper_model_last_used:
                    oldest_key = min(
                        _whisper_model_last_used.items(),
                        key=lambda x: x[1],
                    )[0]
                    del _whisper_models_cache[oldest_key]
                    del _whisper_model_last_used[oldest_key]
                    logger.debug(
                        f"‚ôªÔ∏è Mod√®le Whisper LRU d√©charg√©: {oldest_key} (optimisation RAM)",
                    )

        try:
            logger.info("üì• Chargement mod√®le Whisper %s...", self.model_size)
            start_time = time.time()

            model = whisper.load_model(self.model_size)

            load_time = time.time() - start_time
            logger.info("‚úÖ Mod√®le Whisper charg√© en %.1fs", load_time)

            # OPTIMISATION RAM: Mettre en cache global avec timestamp
            with _whisper_model_cache_lock:
                _whisper_models_cache[self.model_size] = model
                _whisper_model_last_used[self.model_size] = time.time()

            self.model = model
            self.is_loaded = True
            return True

        except Exception as e:
            logger.exception("‚ùå Erreur chargement Whisper: %s", e)
            return False

    def transcribe_audio(self, audio_path: str) -> str | None:
        """Transcrit un fichier audio en texte.

        Args:
            audio_path: Chemin vers le fichier audio

        Returns:
            Texte transcrit ou None si erreur

        """
        # V√©rification globale de disponibilit√©
        if not WHISPER_AVAILABLE:
            logger.error("‚ùå Whisper non disponible")
            return None

        # Charger le mod√®le si n√©cessaire
        if not self.is_loaded:
            if not self.load_model():
                logger.error("‚ùå Impossible de charger le mod√®le Whisper")
                return None

        try:
            logger.info("üéµ Transcription audio: %s", audio_path)
            start_time = time.time()

            # Transcription avec Whisper
            if self.model is None:
                logger.error("‚ùå Mod√®le Whisper non charg√©")
                return None

            result = cast(
                "dict[str, Any]",
                self.model.transcribe(
                    audio_path,
                    language=self.language if self.language != "auto" else None,
                    fp16=False,  # √âviter les probl√®mes de compatibilit√©
                ),
            )

            transcription_time = time.time() - start_time
            text = result["text"].strip()

            logger.info(
                f"‚úÖ Transcription termin√©e en {transcription_time:.1f}s: '{text}'",
            )
            return text

        except Exception as e:
            logger.exception("‚ùå Erreur transcription: %s", e)
            return None

    def transcribe_microphone(self, duration: float = 3.0) -> str | None:
        """Enregistre et transcrit depuis le microphone.

        Args:
            duration: Dur√©e d'enregistrement en secondes

        Returns:
            Texte transcrit ou None si erreur

        """
        # D√©sactivation explicite audio (CI/headless)
        if os.environ.get("BBIA_DISABLE_AUDIO", "0") == "1":
            logger.info(
                "üé§ Micro d√©sactiv√© (BBIA_DISABLE_AUDIO=1) - skip enregistrement",
            )
            return None

        # V√©rification globale de disponibilit√©
        if not WHISPER_AVAILABLE:
            logger.error("‚ùå Whisper non disponible")
            return None

        try:
            import numpy as np
            import soundfile as sf

            logger.info("üé§ Enregistrement microphone (%ss)...", duration)

            # Enregistrement audio
            sample_rate = 16000  # Whisper recommande 16kHz
            audio_data = sd.rec(
                int(duration * sample_rate),
                samplerate=sample_rate,
                channels=1,
                dtype=np.float32,
            )
            sd.wait()

            # OPTIMISATION PERFORMANCE: Sauvegarde temporaire s√©curis√©e avec cleanup garanti
            import tempfile
            import time

            # Nom unique pour √©viter collisions multi-processus
            temp_file = (
                Path(tempfile.gettempdir())
                / f"bbia_whisper_{os.getpid()}_{int(time.time() * 1000)}.wav"
            )

            try:
                sf.write(temp_file, audio_data, sample_rate)

                # Transcription
                result = self.transcribe_audio(str(temp_file))
                return result
            finally:
                # OPTIMISATION: Nettoyage garanti m√™me en cas d'erreur
                if temp_file.exists():
                    try:
                        temp_file.unlink()
                    except Exception as cleanup_error:
                        logger.debug("Nettoyage fichier Whisper (%s)", cleanup_error)

        except ImportError:
            logger.exception(
                "‚ùå sounddevice/soundfile requis pour l'enregistrement microphone",
            )
            return None
        except Exception as e:
            logger.exception("‚ùå Erreur enregistrement microphone: %s", e)
            return None

    def detect_speech_activity(self, audio_chunk: Any) -> bool:
        """D√©tecte si un chunk audio contient de la parole (VAD - Voice Activity Detection).

        Args:
            audio_chunk: Chunk audio (numpy array ou fichier)

        Returns:
            True si parole d√©tect√©e, False sinon

        """
        if not self.enable_vad:
            return True  # Si VAD d√©sactiv√©, consid√©rer toujours comme parole

        # D√©sactivation explicite audio (CI/headless)
        if os.environ.get("BBIA_DISABLE_AUDIO", "0") == "1":
            return False

        try:
            # OPTIMISATION PERFORMANCE: Utiliser cache global pour mod√®le VAD
            global _vad_model_cache
            if _vad_model_cache is not None:
                logger.debug("‚ôªÔ∏è R√©utilisation mod√®le VAD depuis cache global")
                self._vad_model = _vad_model_cache
                self._vad_loaded = True
            elif not self._vad_loaded or self._vad_model is None:
                try:
                    logger.info("üì• Chargement mod√®le VAD (silero/vad)...")
                    # Utiliser l'import au niveau module si disponible, sinon import local
                    if transformers_pipeline is None:
                        from transformers import pipeline

                        vad_pipeline_func = pipeline
                    else:
                        vad_pipeline_func = transformers_pipeline

                    vad_model = vad_pipeline_func(
                        "audio-classification",
                        model="silero/vad",
                    )

                    # Mettre en cache global et local
                    with _vad_cache_lock:
                        if _vad_model_cache is None:
                            _vad_model_cache = vad_model

                    self._vad_model = vad_model
                    self._vad_loaded = True
                    logger.info("‚úÖ Mod√®le VAD charg√©")
                except Exception as e:
                    logger.warning("‚ö†Ô∏è Impossible de charger VAD, fallback activ√©: %s", e)
                    self.enable_vad = False
                    return True  # Fallback: consid√©rer comme parole

            # Convertir audio_chunk si n√©cessaire (fichier -> array)
            import numpy as np

            # Utiliser l'import au niveau module si disponible, sinon import local
            if sf is None:
                try:
                    import soundfile as soundfile_module
                except ImportError:
                    logger.warning(
                        "‚ö†Ô∏è soundfile requis pour VAD fichier, fallback activ√©",
                    )
                    return True  # Fallback: consid√©rer comme parole
            else:
                soundfile_module = sf

            if isinstance(audio_chunk, str | Path):
                # C'est un chemin de fichier
                audio_data, sample_rate = soundfile_module.read(audio_chunk)
            elif isinstance(audio_chunk, np.ndarray):
                audio_data = audio_chunk
            else:
                logger.warning("‚ö†Ô∏è Format audio non support√© pour VAD")
                return True  # Fallback: consid√©rer comme parole

            # V√©rifier taille minimale (√©viter erreurs)
            if len(audio_data) < 100:
                return False

            # D√©tection VAD
            if self._vad_model is None:
                return True  # Fallback: consid√©rer comme parole
            result = self._vad_model(audio_data, return_timestamps=False)

            # R√©sultat typique: [{"label": "SPEECH", "score": 0.95}]
            if isinstance(result, list) and len(result) > 0:
                label = result[0].get("label", "")
                score = result[0].get("score", 0.0)

                # Seuil de confiance
                is_speech = bool(label == "SPEECH" and score > 0.5)
                logger.debug("üîç VAD: %s (score: %.2f) ‚Üí %s", label, score, is_speech)

                return is_speech

            return False

        except ImportError:
            logger.warning("‚ö†Ô∏è transformers requis pour VAD, fallback activ√©")
            self.enable_vad = False
            return True  # Fallback: consid√©rer comme parole
        except Exception as e:
            logger.debug("‚ÑπÔ∏è Erreur VAD (fallback activ√©): %s", e)
            return True  # Fallback: consid√©rer comme parole

    def transcribe_microphone_with_vad(
        self,
        duration: float = 3.0,
        silence_threshold: float = 0.3,
    ) -> str | None:
        """Enregistre et transcrit depuis le microphone avec d√©tection VAD automatique.

        Args:
            duration: Dur√©e maximale d'enregistrement en secondes
            silence_threshold: Seuil de silence avant arr√™t (secondes)

        Returns:
            Texte transcrit ou None si aucune parole d√©tect√©e

        """
        # D√©sactivation explicite audio (CI/headless)
        if os.environ.get("BBIA_DISABLE_AUDIO", "0") == "1":
            logger.info(
                "üé§ Micro d√©sactiv√© (BBIA_DISABLE_AUDIO=1) - skip enregistrement",
            )
            return None

        # V√©rification globale de disponibilit√©
        if not WHISPER_AVAILABLE:
            logger.error("‚ùå Whisper non disponible")
            return None

        try:
            import tempfile

            import numpy as np
            import soundfile as sf

            logger.info("üé§ Enregistrement microphone avec VAD (%ss max)...", duration)

            # Enregistrement audio continu avec d√©tection VAD
            sample_rate = 16000
            chunk_duration = 0.5  # Analyser par chunks de 500ms
            chunk_samples = int(chunk_duration * sample_rate)

            # OPTIMISATION RAM: Limiter taille buffer avec deque (max 10 chunks)
            audio_buffer: deque[npt.NDArray[np.float32]] = deque(maxlen=10)
            silence_duration = 0.0
            max_silence = silence_threshold
            total_duration = 0.0

            while total_duration < duration:
                # Enregistrer chunk
                chunk = sd.rec(
                    chunk_samples,
                    samplerate=sample_rate,
                    channels=1,
                    dtype=np.float32,
                )
                sd.wait()
                audio_buffer.append(chunk.flatten())

                # V√©rifier VAD sur chunk
                if self.detect_speech_activity(chunk):
                    silence_duration = 0.0  # Reset silence
                    logger.debug("üîä Parole d√©tect√©e")
                else:
                    silence_duration += chunk_duration
                    logger.debug("üîá Silence: %ss", silence_duration:.1f)

                total_duration += chunk_duration

                # Arr√™t si silence prolong√© apr√®s au moins une d√©tection de parole
                if silence_duration > max_silence and len(audio_buffer) > 2:
                    logger.info("üîá Fin d√©tect√©e (silence prolong√©)")
                    break

            if not audio_buffer:
                logger.warning("‚ö†Ô∏è Aucun audio enregistr√©")
                return None

            # Concat√©ner chunks
            audio_data = np.concatenate(audio_buffer)

            # Sauvegarder temporairement pour transcription
            temp_file = (
                Path(tempfile.gettempdir())
                / f"bbia_whisper_vad_{os.getpid()}_{int(time.time() * 1000)}.wav"
            )

            try:
                sf.write(temp_file, audio_data, sample_rate)

                # Transcription
                result = self.transcribe_audio(str(temp_file))
                return result
            finally:
                # Nettoyage
                if temp_file.exists():
                    try:
                        temp_file.unlink()
                    except Exception as cleanup_error:
                        logger.debug("Nettoyage fichier Whisper (%s)", cleanup_error)

        except ImportError:
            logger.exception(
                "‚ùå sounddevice/soundfile requis pour l'enregistrement microphone",
            )
            return None
        except Exception as e:
            logger.exception("‚ùå Erreur enregistrement microphone avec VAD: %s", e)
            return None

    def transcribe_streaming(
        self,
        callback: Any | None = None,
        chunk_duration: float = 0.5,
        max_duration: float = 30.0,
        transcription_interval: float = 1.5,
    ) -> str | None:
        """Transcription en streaming (continuelle) depuis le microphone.
        Utile pour latence r√©duite (500ms vs 1-2s).

        Args:
            callback: Fonction appel√©e √† chaque chunk transcrit (optionnel)
            chunk_duration: Dur√©e de chaque chunk en secondes (plus petit = latence plus faible)
            max_duration: Dur√©e maximale d'enregistrement
            transcription_interval: Intervalle minimum entre transcriptions (secondes).
                                   R√©duire pour latence plus faible, augmenter pour √©conomiser CPU.

        Returns:
            Texte final complet transcrit, ou None si erreur

        """
        # D√©sactivation explicite audio (CI/headless)
        if os.environ.get("BBIA_DISABLE_AUDIO", "0") == "1":
            logger.info("üé§ Micro d√©sactiv√© (BBIA_DISABLE_AUDIO=1) - skip streaming")
            return None

        # V√©rification globale de disponibilit√©
        if not WHISPER_AVAILABLE:
            logger.error("‚ùå Whisper non disponible")
            return None

        # Charger mod√®le si n√©cessaire
        if not self.is_loaded:
            if not self.load_model():
                logger.error("‚ùå Impossible de charger le mod√®le Whisper")
                return None

        try:
            import tempfile

            import numpy as np
            import soundfile as sf

            logger.info(
                f"üé§ Transcription streaming ({chunk_duration}s chunks, max {max_duration}s, intervalle {transcription_interval}s)...",
            )

            sample_rate = 16000
            chunk_samples = int(chunk_duration * sample_rate)
            all_transcriptions: list[str] = []
            total_duration = 0.0

            # OPTIMISATION RAM: Limiter taille buffer avec deque
            buffer_max_chunks = 10  # Max 10 chunks (limite s√©curit√©)
            audio_buffer: deque[npt.NDArray[np.float32]] = deque(
                maxlen=buffer_max_chunks
            )

            # OPTIMISATION PERFORMANCE: Throttling transcription pour √©viter surcharge CPU/GPU
            last_transcription_time = 0.0
            consecutive_silence_chunks = 0
            max_silence_chunks = 3  # Arr√™ter apr√®s 3 chunks de silence cons√©cutifs

            while total_duration < max_duration:
                # Enregistrer chunk
                chunk = sd.rec(
                    chunk_samples,
                    samplerate=sample_rate,
                    channels=1,
                    dtype=np.float32,
                )
                sd.wait()

                # OPTIMISATION RAM: Ajouter au buffer (deque g√®re automatiquement maxlen)
                audio_buffer.append(chunk.flatten())

                # OPTIMISATION: Utiliser VAD pour d√©cider si transcrire (√©vite traitement inutile)
                should_transcribe = True
                if self.enable_vad:
                    has_speech = self.detect_speech_activity(chunk)
                    if has_speech:
                        consecutive_silence_chunks = 0
                        logger.debug("üîä Parole d√©tect√©e")
                    else:
                        consecutive_silence_chunks += 1
                        logger.debug("üîá Silence: %s chunks", consecutive_silence_chunks)
                        # Ne pas transcrire si silence prolong√©
                        if consecutive_silence_chunks >= max_silence_chunks:
                            should_transcribe = False
                        # OPTIMISATION RAM: Buffer g√©r√© par deque (maxlen)
                        # Pas besoin de pop manuel

                # OPTIMISATION: Throttling - ne transcrire que si intervalle respect√© ET parole d√©tect√©e
                current_time = time.time()
                time_since_last_transcription = current_time - last_transcription_time
                should_transcribe = (
                    should_transcribe
                    and time_since_last_transcription >= transcription_interval
                    and len(audio_buffer) >= 2
                )

                # Transcription sur buffer complet (am√©liore pr√©cision)
                if should_transcribe:
                    audio_segment = np.concatenate(audio_buffer)

                    # OPTIMISATION RAM: Pool fichiers temporaires (r√©utiliser au lieu de cr√©er/supprimer)
                    if not hasattr(self, "_temp_file_pool"):
                        self._temp_file_pool: list[Path] = []
                        self._max_temp_files = 3  # Pool de 3 fichiers max

                    # R√©utiliser fichier depuis pool si disponible
                    temp_file = None
                    if self._temp_file_pool:
                        temp_file = self._temp_file_pool.pop(0)
                    else:
                        temp_file = (
                            Path(tempfile.gettempdir())
                            / f"bbia_whisper_stream_{os.getpid()}_"
                            f"{int(time.time() * 1000)}.wav"
                        )
                        # Limiter taille pool
                        if len(self._temp_file_pool) < self._max_temp_files:
                            pass  # Ajouter au pool apr√®s usage

                    try:
                        sf.write(temp_file, audio_segment, sample_rate)

                        # Transcription rapide (petit segment)
                        if self.model is None:
                            logger.error("‚ùå Mod√®le Whisper non charg√©")
                            break

                        result = cast(
                            "dict[str, Any]",
                            self.model.transcribe(
                                str(temp_file),
                                language=(
                                    self.language if self.language != "auto" else None
                                ),
                                fp16=False,
                                # Optimisations streaming
                                initial_prompt="",  # Pas de prompt initial pour latence
                                temperature=0.0,  # D√©terministe
                            ),
                        )

                        text = result["text"].strip()
                        if text and text.lower() not in ["", "you", "thank you"]:
                            all_transcriptions.append(text)
                            last_transcription_time = current_time
                            logger.debug("üìù Chunk transcrit: '%s'", text)

                            # Callback si fourni
                            if callback:
                                try:
                                    callback(text, total_duration)
                                except Exception as callback_error:
                                    logger.debug("Erreur callback: %s", callback_error)

                    finally:
                        # OPTIMISATION RAM: Remettre fichier dans pool au lieu
                        # de supprimer
                        if temp_file and temp_file.exists():
                            # Remettre dans pool si espace disponible
                            if len(self._temp_file_pool) < self._max_temp_files:
                                self._temp_file_pool.append(temp_file)
                            else:
                                # Pool plein - supprimer fichier
                                try:
                                    temp_file.unlink()
                                except Exception as e:
                                    # Ignorer erreur suppression fichier temporaire
                                    logger.debug(
                                        f"Impossible de supprimer fichier temporaire: {e}"
                                    )

                total_duration += chunk_duration

                # OPTIMISATION: Arr√™t si silence prolong√© (√©conomise CPU)
                if consecutive_silence_chunks >= max_silence_chunks * 2:
                    logger.info("üîá Arr√™t automatique (silence prolong√©)")
                    break

            # Concat√©ner toutes les transcriptions
            final_text = " ".join(all_transcriptions).strip()
            if final_text:
                logger.info("‚úÖ Streaming termin√©: '%s'", final_text)
                return final_text

            logger.warning("‚ö†Ô∏è Aucune transcription g√©n√©r√©e")
            return None

        except ImportError:
            logger.exception("‚ùå sounddevice/soundfile requis pour streaming")
            return None
        except Exception as e:
            logger.exception("‚ùå Erreur streaming: %s", e)
            return None


class VoiceCommandMapper:
    """Mappe les commandes vocales vers des actions RobotAPI."""

    def __init__(self) -> None:
        """Initialise le mappeur de commandes."""
        self.commands = {
            # Fran√ßais
            "salue": "greet",
            "salut": "greet",
            "bonjour": "greet",
            "regarde-moi": "look_at",
            "regarde moi": "look_at",
            "sois content": "happy",
            "sois triste": "sad",
            "sois excit√©": "excited",
            "r√©veille-toi": "wake_up",
            # Anglais
            "hello": "greet",
            "hi": "greet",
            "look at me": "look_at",
            "be happy": "happy",
            "be sad": "sad",
            "be excited": "excited",
            "wake up": "wake_up",
        }

        logger.info(
            f"üó£Ô∏è Mappeur de commandes initialis√© ({len(self.commands)} commandes)",
        )

    def map_command(self, text: str) -> dict[str, Any] | None:
        """Mappe un texte vers une action RobotAPI.

        Args:
            text: Texte √† mapper

        Returns:
            Dictionnaire d'action ou None si non reconnu

        """
        if not text:
            return None

        # Normalisation
        text_lower = text.lower().strip()

        # Recherche exacte
        if text_lower in self.commands:
            action = self.commands[text_lower]
            logger.info("üéØ Commande mapp√©e: '%s' ‚Üí %s", text, action)
            return {"action": action, "confidence": 1.0}

        # Recherche partielle
        for command, action in self.commands.items():
            if command in text_lower:
                logger.info("üéØ Commande partielle mapp√©e: '%s' ‚Üí %s", text, action)
                return {"action": action, "confidence": 0.8}

        logger.warning("‚ùì Commande non reconnue: '%s'", text)
        return None


def create_whisper_stt(
    model_size: str = "tiny",
    language: str = "fr",
) -> WhisperSTT | None:
    """Factory function pour cr√©er une instance WhisperSTT.

    Args:
        model_size: Taille du mod√®le Whisper
        language: Langue cible

    Returns:
        Instance WhisperSTT ou None si non disponible

    """
    if not WHISPER_AVAILABLE:
        logger.warning("‚ö†Ô∏è Whisper non disponible")
        return None

    return WhisperSTT(model_size=model_size, language=language)


# Test rapide
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    logger.info("üß™ Test module Whisper STT")
    logger.info("=" * 40)

    # Test disponibilit√©
    logger.info("Whisper disponible: %s", WHISPER_AVAILABLE)

    if WHISPER_AVAILABLE:
        # Test cr√©ation
        stt = create_whisper_stt("tiny", "fr")
        if stt:
            logger.info("‚úÖ Module Whisper cr√©√©")

            # Test mappeur
            mapper = VoiceCommandMapper()
            test_commands = [
                "salue",
                "regarde-moi",
                "sois content",
                "commande inconnue",
            ]

            for cmd in test_commands:
                result = mapper.map_command(cmd)
                logger.info("  '%s' ‚Üí %s", cmd, result)
        else:
            logger.error("‚ùå Impossible de cr√©er le module Whisper")
    else:
        logger.error("‚ùå Whisper non install√©")
