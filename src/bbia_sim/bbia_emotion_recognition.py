#!/usr/bin/env python3
"""
BBIA Emotion Recognition - Module de reconnaissance des Ã©motions humaines
DÃ©tection et analyse des Ã©motions faciales et vocales en temps rÃ©el
"""

import logging
import threading
import time
from typing import Any

import cv2
import numpy as np

logger = logging.getLogger(__name__)

# OPTIMISATION PERFORMANCE: Cache global pour pipelines transformers (Ã©vite chargements rÃ©pÃ©tÃ©s)
_emotion_pipelines_cache: dict[str, dict[str, Any]] = {}  # device -> models dict
_emotion_cache_lock = threading.Lock()

# RÃ©duction du bruit de logs TensorFlow/MediaPipe (avant tout import MediaPipe)
try:
    import os as _os  # noqa: F401

    _os.environ.setdefault("GLOG_minloglevel", "2")
    _os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")
    _os.environ.setdefault("MEDIAPIPE_DISABLE_GPU", "1")
except Exception:
    pass

# Import conditionnel des dÃ©pendances ML
try:
    import mediapipe as mp
    import torch
    from transformers import pipeline

    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    logger.warning(
        "DÃ©pendances ML non disponibles. Installez avec: pip install mediapipe torch transformers"
    )


class BBIAEmotionRecognition:
    """Module de reconnaissance des Ã©motions humaines pour BBIA-SIM.

    FonctionnalitÃ©s :
    - DÃ©tection des Ã©motions faciales (MediaPipe + modÃ¨les prÃ©-entraÃ®nÃ©s)
    - Analyse des Ã©motions vocales (Whisper + analyse sentiment)
    - Fusion multimodale des Ã©motions
    - DÃ©tection en temps rÃ©el
    """

    def __init__(self, device: str = "auto") -> None:
        """Initialise le module de reconnaissance d'Ã©motions.

        Args:
            device: Device pour les modÃ¨les ML ("cpu", "cuda", "auto")
        """
        if not ML_AVAILABLE:
            raise ImportError(
                "DÃ©pendances ML requises. Installez avec: pip install mediapipe torch transformers"
            )

        self.device = self._get_device(device)
        self.is_initialized = False

        # Configuration MediaPipe
        self.mp_face_detection: Any = None
        self.mp_face_mesh: Any = None
        self.mp_drawing: Any = None

        # ModÃ¨les d'Ã©motion
        self.emotion_models: dict[str, Any] = {}
        self.emotion_processors: dict[str, Any] = {}

        # Ã‰motions supportÃ©es
        self.supported_emotions = [
            "happy",
            "sad",
            "angry",
            "surprised",
            "fearful",
            "disgusted",
            "neutral",
            "excited",
            "calm",
            "confused",
        ]

        # Configuration de dÃ©tection
        self.detection_config: dict[str, Any] = {
            "face_detection_confidence": 0.7,
            "emotion_confidence_threshold": 0.6,
            "temporal_window_size": 5,  # Frames pour moyennage temporel
            "fusion_weights": {"facial": 0.7, "vocal": 0.3},
        }

        # Historique des Ã©motions pour moyennage temporel
        self.emotion_history: list[dict[str, Any]] = []

        logger.info(f"ğŸ˜Š BBIA Emotion Recognition initialisÃ© (device: {self.device})")

    def _get_device(self, device: str) -> str:
        """DÃ©termine le device optimal."""
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            elif torch.backends.mps.is_available():
                return "mps"  # Apple Silicon
            else:
                return "cpu"
        return device

    def initialize(self) -> bool:
        """Initialise tous les modÃ¨les et composants."""
        try:
            # Initialisation MediaPipe
            self.mp_face_detection = mp.solutions.face_detection
            self.mp_face_mesh = mp.solutions.face_mesh
            self.mp_drawing = mp.solutions.drawing_utils

            # Chargement des modÃ¨les d'Ã©motion
            self._load_emotion_models()

            self.is_initialized = True
            logger.info("âœ… BBIA Emotion Recognition initialisÃ© avec succÃ¨s")
            return True

        except Exception as e:
            logger.error(f"âŒ Erreur initialisation: {e}")
            return False

    def _load_emotion_models(self) -> None:
        """Charge les modÃ¨les de reconnaissance d'Ã©motion (utilise cache global si disponible)."""
        # OPTIMISATION PERFORMANCE: Utiliser cache global pour Ã©viter chargements rÃ©pÃ©tÃ©s
        global _emotion_pipelines_cache
        with _emotion_cache_lock:
            if self.device in _emotion_pipelines_cache:
                logger.debug(f"â™»ï¸ RÃ©utilisation modÃ¨les Ã©motion depuis cache (device: {self.device})")
                self.emotion_models = _emotion_pipelines_cache[self.device].copy()
                logger.info("ğŸ“¥ ModÃ¨les d'Ã©motion chargÃ©s (cache)")
                return

        try:
            # ModÃ¨le de sentiment pour analyse vocale
            sentiment_model = pipeline(
                "sentiment-analysis",
                model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                device=self.device,
            )

            # ModÃ¨le d'Ã©motion pour analyse textuelle
            emotion_model = pipeline(
                "text-classification",
                model="j-hartmann/emotion-english-distilroberta-base",
                device=self.device,
            )

            # Stocker dans l'instance
            self.emotion_models["sentiment"] = sentiment_model
            self.emotion_models["emotion"] = emotion_model

            # Mettre en cache global
            with _emotion_cache_lock:
                _emotion_pipelines_cache[self.device] = {
                    "sentiment": sentiment_model,
                    "emotion": emotion_model,
                }

            logger.info("ğŸ“¥ ModÃ¨les d'Ã©motion chargÃ©s")

        except Exception as e:
            logger.error(f"âŒ Erreur chargement modÃ¨les Ã©motion: {e}")

    def detect_faces(self, image: np.ndarray | str) -> list[dict[str, Any]]:
        """DÃ©tecte les visages dans une image.

        Args:
            image: Image Ã  analyser (numpy array ou chemin)

        Returns:
            Liste des visages dÃ©tectÃ©s avec coordonnÃ©es
        """
        if not self.is_initialized:
            self.initialize()

        try:
            # Conversion de l'image
            processed_image: np.ndarray | None = None

            if isinstance(image, str):
                loaded_image = cv2.imread(image)
                if loaded_image is not None:
                    processed_image = cv2.cvtColor(loaded_image, cv2.COLOR_BGR2RGB)
            elif isinstance(image, np.ndarray):
                if len(image.shape) == 3 and image.shape[2] == 3:
                    processed_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                else:
                    processed_image = image

            if processed_image is None:
                return []

            faces = []

            with self.mp_face_detection.FaceDetection(
                model_selection=0,
                min_detection_confidence=self.detection_config[
                    "face_detection_confidence"
                ],
            ) as face_detection:

                results = face_detection.process(processed_image)

                if results.detections:
                    for detection in results.detections:
                        bbox = detection.location_data.relative_bounding_box
                        h, w, _ = processed_image.shape

                        face_info = {
                            "bbox": {
                                "x": int(bbox.xmin * w),
                                "y": int(bbox.ymin * h),
                                "width": int(bbox.width * w),
                                "height": int(bbox.height * h),
                            },
                            "confidence": detection.score[0],
                            "landmarks": None,  # Ã€ implÃ©menter avec face_mesh
                        }
                        faces.append(face_info)

            return faces

        except Exception as e:
            logger.error(f"âŒ Erreur dÃ©tection visages: {e}")
            return []

    def analyze_facial_emotion(
        self,
        image: np.ndarray | str,  # noqa: ARG002
        face_bbox: dict | None = None,  # noqa: ARG002
    ) -> dict[str, Any]:
        """Analyse les Ã©motions faciales dans une image.

        Args:
            image: Image Ã  analyser
            face_bbox: CoordonnÃ©es du visage (optionnel)

        Returns:
            Dictionnaire avec Ã©motion dÃ©tectÃ©e et confiance
        """
        try:
            # Pour l'instant, simulation basÃ©e sur des patterns visuels
            # Dans une implÃ©mentation complÃ¨te, on utiliserait un modÃ¨le spÃ©cialisÃ©

            # Simulation d'analyse faciale
            emotion_scores = {
                "happy": np.random.random() * 0.8 + 0.2,
                "sad": np.random.random() * 0.6,
                "angry": np.random.random() * 0.5,
                "surprised": np.random.random() * 0.4,
                "neutral": np.random.random() * 0.7 + 0.3,
                "excited": np.random.random() * 0.6,
                "calm": np.random.random() * 0.5,
                "confused": np.random.random() * 0.4,
            }

            # Normalisation des scores
            total_score = sum(emotion_scores.values())
            normalized_scores = {k: v / total_score for k, v in emotion_scores.items()}

            # SÃ©lection de l'Ã©motion dominante
            dominant_emotion = max(
                normalized_scores, key=lambda x: normalized_scores[x]
            )
            confidence = normalized_scores[dominant_emotion]

            return {
                "emotion": dominant_emotion,
                "confidence": confidence,
                "all_scores": normalized_scores,
                "method": "facial_analysis",
                "timestamp": time.time(),
            }

        except Exception as e:
            logger.error(f"âŒ Erreur analyse Ã©motion faciale: {e}")
            return {"error": str(e)}

    def analyze_vocal_emotion(self, text: str) -> dict[str, Any]:
        """Analyse les Ã©motions dans un texte vocal transcrit.

        Args:
            text: Texte transcrit Ã  analyser

        Returns:
            Dictionnaire avec Ã©motion dÃ©tectÃ©e et confiance
        """
        try:
            if not self.emotion_models.get("emotion"):
                return {"error": "ModÃ¨le d'Ã©motion non chargÃ©"}

            # Analyse avec le modÃ¨le d'Ã©motion
            emotion_result: list[dict[str, Any]] = self.emotion_models["emotion"](text)

            # Analyse de sentiment
            sentiment_result: list[dict[str, Any]] = self.emotion_models["sentiment"](
                text
            )

            # Mapping des Ã©motions du modÃ¨le vers nos Ã©motions
            emotion_mapping = {
                "joy": "happy",
                "sadness": "sad",
                "anger": "angry",
                "fear": "fearful",
                "surprise": "surprised",
                "disgust": "disgusted",
                "neutral": "neutral",
            }

            detected_emotion = emotion_mapping.get(
                emotion_result[0]["label"], "neutral"
            )
            emotion_confidence = float(emotion_result[0]["score"])

            sentiment_label = sentiment_result[0]["label"]
            sentiment_score = float(sentiment_result[0]["score"])

            return {
                "emotion": detected_emotion,
                "confidence": emotion_confidence,
                "sentiment": sentiment_label,
                "sentiment_score": sentiment_score,
                "method": "vocal_analysis",
                "timestamp": time.time(),
            }

        except Exception as e:
            logger.error(f"âŒ Erreur analyse Ã©motion vocale: {e}")
            return {"error": str(e)}

    def fuse_emotions(
        self, facial_result: dict[str, Any], vocal_result: dict[str, Any]
    ) -> dict[str, Any]:
        """Fusionne les rÃ©sultats d'Ã©motion faciale et vocale.

        Args:
            facial_result: RÃ©sultat de l'analyse faciale
            vocal_result: RÃ©sultat de l'analyse vocale

        Returns:
            RÃ©sultat fusionnÃ© avec Ã©motion finale
        """
        try:
            weights = self.detection_config["fusion_weights"]

            # Extraction des Ã©motions et confiances
            facial_emotion = facial_result.get("emotion", "neutral")
            facial_confidence = facial_result.get("confidence", 0.0)

            vocal_emotion = vocal_result.get("emotion", "neutral")
            vocal_confidence = vocal_result.get("confidence", 0.0)

            # Score pondÃ©rÃ© pour chaque Ã©motion
            emotion_scores: dict[str, float] = {}

            # Score facial pondÃ©rÃ©
            facial_weight = weights["facial"] * facial_confidence
            emotion_scores[facial_emotion] = (
                emotion_scores.get(facial_emotion, 0) + facial_weight
            )

            # Score vocal pondÃ©rÃ©
            vocal_weight = weights["vocal"] * vocal_confidence
            emotion_scores[vocal_emotion] = (
                emotion_scores.get(vocal_emotion, 0) + vocal_weight
            )

            # Ã‰motion finale
            final_emotion = max(emotion_scores, key=lambda x: emotion_scores[x])
            final_confidence = float(emotion_scores[final_emotion])

            return {
                "emotion": final_emotion,
                "confidence": final_confidence,
                "facial_emotion": facial_emotion,
                "facial_confidence": facial_confidence,
                "vocal_emotion": vocal_emotion,
                "vocal_confidence": vocal_confidence,
                "fusion_method": "weighted_average",
                "timestamp": time.time(),
            }

        except Exception as e:
            logger.error(f"âŒ Erreur fusion Ã©motions: {e}")
            return {"error": str(e)}

    def analyze_emotion_realtime(
        self, image: np.ndarray | str, text: str | None = None
    ) -> dict[str, Any]:
        """Analyse complÃ¨te des Ã©motions en temps rÃ©el.

        Args:
            image: Image Ã  analyser
            text: Texte vocal transcrit (optionnel)

        Returns:
            RÃ©sultat complet d'analyse d'Ã©motion
        """
        try:
            # Analyse faciale
            facial_result = self.analyze_facial_emotion(image)

            # Analyse vocale si texte fourni
            vocal_result = None
            if text:
                vocal_result = self.analyze_vocal_emotion(text)

            # Fusion si les deux analyses sont disponibles
            if vocal_result and not vocal_result.get("error"):
                final_result = self.fuse_emotions(facial_result, vocal_result)
            else:
                final_result = facial_result
                final_result["method"] = "facial_only"

            # Ajout Ã  l'historique pour moyennage temporel
            self._update_emotion_history(final_result)

            # Moyennage temporel si suffisamment d'historique
            temporal_window_size = int(
                self.detection_config.get("temporal_window_size", 5)
            )
            if len(self.emotion_history) >= temporal_window_size:
                final_result = self._apply_temporal_smoothing(final_result)

            return final_result

        except Exception as e:
            logger.error(f"âŒ Erreur analyse temps rÃ©el: {e}")
            return {"error": str(e)}

    def _update_emotion_history(self, emotion_result: dict[str, Any]) -> None:
        """Met Ã  jour l'historique des Ã©motions."""
        self.emotion_history.append(emotion_result)

        # Limitation de la taille de l'historique
        temporal_window_size = int(self.detection_config.get("temporal_window_size", 5))
        max_history = temporal_window_size * 2
        if len(self.emotion_history) > max_history:
            self.emotion_history = self.emotion_history[-max_history:]

    def _apply_temporal_smoothing(
        self, current_result: dict[str, Any]
    ) -> dict[str, Any]:
        """Applique un lissage temporel sur les Ã©motions."""
        try:
            if len(self.emotion_history) < 3:
                return current_result

            # Moyennage des Ã©motions rÃ©centes
            temporal_window_size = int(
                self.detection_config.get("temporal_window_size", 5)
            )
            recent_emotions = self.emotion_history[-temporal_window_size:]

            emotion_counts: dict[str, int] = {}
            total_confidence = 0

            for result in recent_emotions:
                emotion = result.get("emotion", "neutral")
                confidence = result.get("confidence", 0.0)

                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + confidence
                total_confidence += confidence

            # Ã‰motion la plus frÃ©quente avec confiance moyenne
            if emotion_counts:
                smoothed_emotion = max(emotion_counts, key=lambda x: emotion_counts[x])
                smoothed_confidence = emotion_counts[smoothed_emotion] / len(
                    recent_emotions
                )

                current_result["emotion"] = smoothed_emotion
                current_result["confidence"] = smoothed_confidence
                current_result["temporal_smoothing"] = True

            return current_result

        except Exception as e:
            logger.error(f"âŒ Erreur lissage temporel: {e}")
            return current_result

    def get_emotion_statistics(self) -> dict[str, Any]:
        """Retourne les statistiques d'Ã©motion."""
        if not self.emotion_history:
            return {"message": "Aucun historique d'Ã©motion"}

        # Comptage des Ã©motions
        emotion_counts: dict[str, int] = {}
        for result in self.emotion_history:
            emotion = result.get("emotion", "neutral")
            emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1

        # Confiance moyenne
        avg_confidence = sum(
            r.get("confidence", 0) for r in self.emotion_history
        ) / len(self.emotion_history)

        return {
            "total_analyses": len(self.emotion_history),
            "emotion_distribution": emotion_counts,
            "average_confidence": avg_confidence,
            "supported_emotions": self.supported_emotions,
            "detection_config": self.detection_config,
        }

    def reset_history(self) -> None:
        """Remet Ã  zÃ©ro l'historique des Ã©motions."""
        self.emotion_history = []
        logger.info("ğŸ”„ Historique des Ã©motions rÃ©initialisÃ©")


def main() -> None:
    """Test du module BBIA Emotion Recognition."""
    if not ML_AVAILABLE:
        print("âŒ DÃ©pendances ML non disponibles")
        print("Installez avec: pip install mediapipe torch transformers")
        return

    # Initialisation
    emotion_rec = BBIAEmotionRecognition()

    # Test initialisation
    print("ğŸš€ Test initialisation...")
    success = emotion_rec.initialize()
    print(f"RÃ©sultat: {'âœ…' if success else 'âŒ'}")

    # Test analyse Ã©motion vocale
    print("\nğŸ—£ï¸ Test analyse Ã©motion vocale...")
    vocal_result = emotion_rec.analyze_vocal_emotion(
        "Je suis trÃ¨s heureux aujourd'hui!"
    )
    print(f"RÃ©sultat: {vocal_result}")

    # Test analyse Ã©motion faciale (simulation)
    print("\nğŸ˜Š Test analyse Ã©motion faciale...")
    facial_result = emotion_rec.analyze_facial_emotion(
        np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
    )
    print(f"RÃ©sultat: {facial_result}")

    # Test fusion
    print("\nğŸ”„ Test fusion Ã©motions...")
    fusion_result = emotion_rec.fuse_emotions(facial_result, vocal_result)
    print(f"RÃ©sultat: {fusion_result}")

    # Statistiques
    print(f"\nğŸ“Š Statistiques: {emotion_rec.get_emotion_statistics()}")


if __name__ == "__main__":
    main()
