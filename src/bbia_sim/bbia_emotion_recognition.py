#!/usr/bin/env python3
"""BBIA Emotion Recognition - Module de reconnaissance des √©motions humaines
D√©tection et analyse des √©motions faciales et vocales en temps r√©el
"""

import logging
import sys
import threading
import time
from typing import Any

import numpy as np

logger = logging.getLogger(__name__)

# Import conditionnel de cv2
try:
    import cv2  # type: ignore[import-untyped]

    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    logger.warning(
        "OpenCV (cv2) non disponible. " "Installez avec: pip install opencv-python",
    )

# OPTIMISATION PERFORMANCE: Cache global pour pipelines transformers
# (√©vite chargements r√©p√©t√©s)
_emotion_pipelines_cache: dict[str, dict[str, Any]] = {}  # device -> models dict
_emotion_cache_lock = threading.Lock()

# R√©duction du bruit de logs TensorFlow/MediaPipe (avant tout import MediaPipe)
try:
    import os as _os

    _os.environ.setdefault("GLOG_minloglevel", "2")
    _os.environ.setdefault(
        "TF_CPP_MIN_LOG_LEVEL", "3"
    )  # 0=INFO,1=WARNING,2=ERROR,3=FATAL
    _os.environ.setdefault("MEDIAPIPE_DISABLE_GPU", "1")
    # Supprimer les logs TensorFlow Lite
    _os.environ.setdefault("TFLITE_LOG_VERBOSITY", "0")  # 0=ERROR, 1=WARNING, 2=INFO
    # Supprimer les logs OpenGL (ne pas d√©finir MUJOCO_GL sur macOS, utilise la valeur par d√©faut)
    # Sur macOS, laisser MuJoCo choisir automatiquement (glfw ou egl)
    if sys.platform != "darwin":  # Pas macOS
        _os.environ.setdefault("MUJOCO_GL", "egl")  # Utiliser EGL sur Linux/Windows
except (OSError, KeyError, AttributeError) as e:
    logger.debug(
        "Impossible de configurer variables d'environnement MediaPipe/TensorFlow: %s", e
    )

# Import conditionnel des d√©pendances ML
try:
    import mediapipe as mp  # type: ignore[import-untyped]
    import torch
    from transformers import pipeline

    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    logger.warning(
        "D√©pendances ML non disponibles. "
        "Installez avec: pip install mediapipe torch transformers",
    )


class BBIAEmotionRecognition:
    """Module de reconnaissance des √©motions humaines pour BBIA-SIM.

    Fonctionnalit√©s :
    - D√©tection des √©motions faciales (MediaPipe + mod√®les pr√©-entra√Æn√©s)
    - Analyse des √©motions vocales (Whisper + analyse sentiment)
    - Fusion multimodale des √©motions
    - D√©tection en temps r√©el
    """

    def __init__(self, device: str = "auto") -> None:
        """Initialise le module de reconnaissance d'√©motions.

        Args:
            device: Device pour les mod√®les ML ("cpu", "cuda", "auto")

        """
        if not ML_AVAILABLE:
            raise ImportError(
                "D√©pendances ML requises. "
                "Installez avec: pip install mediapipe torch transformers",
            )

        self.device = self._get_device(device)
        self.is_initialized = False

        # OPTIMISATION RAM: Lazy loading MediaPipe - ne charger que si
        # d√©tection visage demand√©e
        self.mp_face_detection: Any = None
        self.mp_face_mesh: Any = None
        self.mp_drawing: Any = None
        self._mediapipe_loaded = False  # Flag pour lazy loading

        # Mod√®les d'√©motion
        self.emotion_models: dict[str, Any] = {}
        self.emotion_processors: dict[str, Any] = {}

        # √âmotions support√©es
        self.supported_emotions = [
            "happy",
            "sad",
            "angry",
            "surprised",
            "fearful",
            "disgusted",
            "neutral",
            "excited",
            "calm",
            "confused",
        ]

        # Configuration de d√©tection
        self.detection_config: dict[str, Any] = {
            "face_detection_confidence": 0.7,
            "emotion_confidence_threshold": 0.6,
            "temporal_window_size": 5,  # Frames pour moyennage temporel
            "fusion_weights": {"facial": 0.7, "vocal": 0.3},
        }

        # Historique des √©motions pour moyennage temporel
        self.emotion_history: list[dict[str, Any]] = []

        logger.info("üòä BBIA Emotion Recognition initialis√© (device: %s)", self.device)

    def _get_device(self, device: str) -> str:
        """D√©termine le device optimal."""
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            if torch.backends.mps.is_available():
                return "mps"  # Apple Silicon
            return "cpu"
        return device

    def initialize(self) -> bool:
        """Initialise tous les mod√®les et composants."""
        try:
            # Initialisation MediaPipe
            self.mp_face_detection = mp.solutions.face_detection
            self.mp_face_mesh = mp.solutions.face_mesh
            self.mp_drawing = mp.solutions.drawing_utils

            # Chargement des mod√®les d'√©motion
            self._load_emotion_models()

            self.is_initialized = True
            logger.info("‚úÖ BBIA Emotion Recognition initialis√© avec succ√®s")
            return True

        except Exception as e:
            logger.exception("‚ùå Erreur initialisation: %s", e)
            return False

    def _load_emotion_models(self) -> None:
        """Charge les mod√®les de reconnaissance d'√©motion (utilise cache
        global si disponible)."""
        # OPTIMISATION PERFORMANCE: Utiliser cache global pour √©viter
        # chargements r√©p√©t√©s
        global _emotion_pipelines_cache
        with _emotion_cache_lock:
            if self.device in _emotion_pipelines_cache:
                logger.debug(
                    f"‚ôªÔ∏è R√©utilisation mod√®les √©motion depuis cache "
                    f"(device: {self.device})",
                )
                self.emotion_models = _emotion_pipelines_cache[self.device].copy()
                logger.info("üì• Mod√®les d'√©motion charg√©s (cache)")
                return

        try:
            # Mod√®le de sentiment pour analyse vocale
            sentiment_model = pipeline(
                "sentiment-analysis",
                model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                device=self.device,
            )

            # Mod√®le d'√©motion pour analyse textuelle
            emotion_model = pipeline(
                "text-classification",
                model="j-hartmann/emotion-english-distilroberta-base",
                device=self.device,
            )

            # Stocker dans l'instance
            self.emotion_models["sentiment"] = sentiment_model
            self.emotion_models["emotion"] = emotion_model

            # Mettre en cache global
            with _emotion_cache_lock:
                _emotion_pipelines_cache[self.device] = {
                    "sentiment": sentiment_model,
                    "emotion": emotion_model,
                }

            logger.info("üì• Mod√®les d'√©motion charg√©s")

        except Exception as e:
            logger.exception("‚ùå Erreur chargement mod√®les √©motion: %s", e)

    def detect_faces(self, image: np.ndarray | str) -> list[dict[str, Any]]:
        """D√©tecte les visages dans une image.

        Args:
            image: Image √† analyser (numpy array ou chemin)

        Returns:
            Liste des visages d√©tect√©s avec coordonn√©es

        """
        if not self.is_initialized:
            self.initialize()

        if not CV2_AVAILABLE:
            logger.error("OpenCV (cv2) requis pour la d√©tection de visages")
            return []

        try:
            # Conversion de l'image
            processed_image: np.ndarray | None = None

            if isinstance(image, str):
                loaded_image = cv2.imread(image)  # type: ignore[name-defined]
                if loaded_image is not None:
                    processed_image = cv2.cvtColor(loaded_image, cv2.COLOR_BGR2RGB)  # type: ignore[name-defined]
            elif isinstance(image, np.ndarray):
                if len(image.shape) == 3 and image.shape[2] == 3:
                    processed_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # type: ignore[name-defined]
                else:
                    processed_image = image

            if processed_image is None:
                return []

            faces = []

            with self.mp_face_detection.FaceDetection(
                model_selection=0,
                min_detection_confidence=self.detection_config[
                    "face_detection_confidence"
                ],
            ) as face_detection:
                results = face_detection.process(processed_image)

                if results.detections:
                    for detection in results.detections:
                        bbox = detection.location_data.relative_bounding_box
                        h, w, _ = processed_image.shape

                        face_info = {
                            "bbox": {
                                "x": int(bbox.xmin * w),
                                "y": int(bbox.ymin * h),
                                "width": int(bbox.width * w),
                                "height": int(bbox.height * h),
                            },
                            "confidence": detection.score[0],
                            "landmarks": None,  # √Ä impl√©menter avec face_mesh
                        }
                        faces.append(face_info)

            return faces

        except Exception as e:
            logger.exception("‚ùå Erreur d√©tection visages: %s", e)
            return []

    def analyze_facial_emotion(
        self,
        image: np.ndarray | str,  # noqa: ARG002
        face_bbox: dict | None = None,  # noqa: ARG002
    ) -> dict[str, Any]:
        """Analyse les √©motions faciales dans une image.

        Args:
            image: Image √† analyser
            face_bbox: Coordonn√©es du visage (optionnel)

        Returns:
            Dictionnaire avec √©motion d√©tect√©e et confiance

        """
        try:
            # Pour l'instant, simulation bas√©e sur des patterns visuels
            # Dans une impl√©mentation compl√®te, on utiliserait un mod√®le sp√©cialis√©

            # Simulation d'analyse faciale
            emotion_scores = {
                "happy": np.random.random() * 0.8 + 0.2,
                "sad": np.random.random() * 0.6,
                "angry": np.random.random() * 0.5,
                "surprised": np.random.random() * 0.4,
                "neutral": np.random.random() * 0.7 + 0.3,
                "excited": np.random.random() * 0.6,
                "calm": np.random.random() * 0.5,
                "confused": np.random.random() * 0.4,
            }

            # Normalisation des scores
            total_score = sum(emotion_scores.values())
            normalized_scores = {k: v / total_score for k, v in emotion_scores.items()}

            # S√©lection de l'√©motion dominante
            dominant_emotion = max(
                normalized_scores,
                key=lambda x: normalized_scores[x],
            )
            confidence = normalized_scores[dominant_emotion]

            return {
                "emotion": dominant_emotion,
                "confidence": confidence,
                "all_scores": normalized_scores,
                "method": "facial_analysis",
                "timestamp": time.time(),
            }

        except Exception as e:
            logger.exception("‚ùå Erreur analyse √©motion faciale: %s", e)
            return {"error": str(e)}

    def analyze_vocal_emotion(self, text: str) -> dict[str, Any]:
        """Analyse les √©motions dans un texte vocal transcrit.

        Args:
            text: Texte transcrit √† analyser

        Returns:
            Dictionnaire avec √©motion d√©tect√©e et confiance

        """
        try:
            if not self.emotion_models.get("emotion"):
                return {"error": "Mod√®le d'√©motion non charg√©"}

            # Analyse avec le mod√®le d'√©motion
            emotion_result: list[dict[str, Any]] = self.emotion_models["emotion"](text)

            # Analyse de sentiment
            sentiment_result: list[dict[str, Any]] = self.emotion_models["sentiment"](
                text,
            )

            # Mapping des √©motions du mod√®le vers nos √©motions
            emotion_mapping = {
                "joy": "happy",
                "sadness": "sad",
                "anger": "angry",
                "fear": "fearful",
                "surprise": "surprised",
                "disgust": "disgusted",
                "neutral": "neutral",
            }

            detected_emotion = emotion_mapping.get(
                emotion_result[0]["label"],
                "neutral",
            )
            emotion_confidence = float(emotion_result[0]["score"])

            sentiment_label = sentiment_result[0]["label"]
            sentiment_score = float(sentiment_result[0]["score"])

            return {
                "emotion": detected_emotion,
                "confidence": emotion_confidence,
                "sentiment": sentiment_label,
                "sentiment_score": sentiment_score,
                "method": "vocal_analysis",
                "timestamp": time.time(),
            }

        except Exception as e:
            logger.exception("‚ùå Erreur analyse √©motion vocale: %s", e)
            return {"error": str(e)}

    def fuse_emotions(
        self,
        facial_result: dict[str, Any],
        vocal_result: dict[str, Any],
    ) -> dict[str, Any]:
        """Fusionne les r√©sultats d'√©motion faciale et vocale.

        Args:
            facial_result: R√©sultat de l'analyse faciale
            vocal_result: R√©sultat de l'analyse vocale

        Returns:
            R√©sultat fusionn√© avec √©motion finale

        """
        try:
            weights = self.detection_config["fusion_weights"]

            # Extraction des √©motions et confiances
            facial_emotion = facial_result.get("emotion", "neutral")
            facial_confidence = facial_result.get("confidence", 0.0)

            vocal_emotion = vocal_result.get("emotion", "neutral")
            vocal_confidence = vocal_result.get("confidence", 0.0)

            # Score pond√©r√© pour chaque √©motion
            emotion_scores: dict[str, float] = {}

            # Score facial pond√©r√©
            facial_weight = weights["facial"] * facial_confidence
            emotion_scores[facial_emotion] = (
                emotion_scores.get(facial_emotion, 0) + facial_weight
            )

            # Score vocal pond√©r√©
            vocal_weight = weights["vocal"] * vocal_confidence
            emotion_scores[vocal_emotion] = (
                emotion_scores.get(vocal_emotion, 0) + vocal_weight
            )

            # √âmotion finale
            final_emotion = max(emotion_scores, key=lambda x: emotion_scores[x])
            final_confidence = float(emotion_scores[final_emotion])

            return {
                "emotion": final_emotion,
                "confidence": final_confidence,
                "facial_emotion": facial_emotion,
                "facial_confidence": facial_confidence,
                "vocal_emotion": vocal_emotion,
                "vocal_confidence": vocal_confidence,
                "fusion_method": "weighted_average",
                "timestamp": time.time(),
            }

        except Exception as e:
            logger.exception("‚ùå Erreur fusion √©motions: %s", e)
            return {"error": str(e)}

    def analyze_emotion_realtime(
        self,
        image: np.ndarray | str,
        text: str | None = None,
    ) -> dict[str, Any]:
        """Analyse compl√®te des √©motions en temps r√©el.

        Args:
            image: Image √† analyser
            text: Texte vocal transcrit (optionnel)

        Returns:
            R√©sultat complet d'analyse d'√©motion

        """
        try:
            # Analyse faciale
            facial_result = self.analyze_facial_emotion(image)

            # Analyse vocale si texte fourni
            vocal_result = None
            if text:
                vocal_result = self.analyze_vocal_emotion(text)

            # Fusion si les deux analyses sont disponibles
            if vocal_result and not vocal_result.get("error"):
                final_result = self.fuse_emotions(facial_result, vocal_result)
            else:
                final_result = facial_result
                final_result["method"] = "facial_only"

            # Ajout √† l'historique pour moyennage temporel
            self._update_emotion_history(final_result)

            # Moyennage temporel si suffisamment d'historique
            temporal_window_size = int(
                self.detection_config.get("temporal_window_size", 5),
            )
            if len(self.emotion_history) >= temporal_window_size:
                final_result = self._apply_temporal_smoothing(final_result)

            return final_result

        except Exception as e:
            logger.exception("‚ùå Erreur analyse temps r√©el: %s", e)
            return {"error": str(e)}

    def _update_emotion_history(self, emotion_result: dict[str, Any]) -> None:
        """Met √† jour l'historique des √©motions."""
        self.emotion_history.append(emotion_result)

        # Limitation de la taille de l'historique
        temporal_window_size = int(self.detection_config.get("temporal_window_size", 5))
        max_history = temporal_window_size * 2
        if len(self.emotion_history) > max_history:
            self.emotion_history = self.emotion_history[-max_history:]

    def _apply_temporal_smoothing(
        self,
        current_result: dict[str, Any],
    ) -> dict[str, Any]:
        """Applique un lissage temporel sur les √©motions."""
        try:
            if len(self.emotion_history) < 3:
                return current_result

            # Moyennage des √©motions r√©centes
            temporal_window_size = int(
                self.detection_config.get("temporal_window_size", 5),
            )
            recent_emotions = self.emotion_history[-temporal_window_size:]

            emotion_counts: dict[str, int] = {}
            total_confidence = 0

            for result in recent_emotions:
                emotion = result.get("emotion", "neutral")
                confidence = result.get("confidence", 0.0)

                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + confidence
                total_confidence += confidence

            # √âmotion la plus fr√©quente avec confiance moyenne
            if emotion_counts:
                smoothed_emotion = max(emotion_counts, key=lambda x: emotion_counts[x])
                smoothed_confidence = emotion_counts[smoothed_emotion] / len(
                    recent_emotions,
                )

                current_result["emotion"] = smoothed_emotion
                current_result["confidence"] = smoothed_confidence
                current_result["temporal_smoothing"] = True

            return current_result

        except Exception as e:
            logger.exception("‚ùå Erreur lissage temporel: %s", e)
            return current_result

    def get_emotion_statistics(self) -> dict[str, Any]:
        """Retourne les statistiques d'√©motion."""
        if not self.emotion_history:
            return {"message": "Aucun historique d'√©motion"}

        # Comptage des √©motions
        emotion_counts: dict[str, int] = {}
        for result in self.emotion_history:
            emotion = result.get("emotion", "neutral")
            emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1

        # Confiance moyenne
        avg_confidence = sum(
            r.get("confidence", 0) for r in self.emotion_history
        ) / len(self.emotion_history)

        return {
            "total_analyses": len(self.emotion_history),
            "emotion_distribution": emotion_counts,
            "average_confidence": avg_confidence,
            "supported_emotions": self.supported_emotions,
            "detection_config": self.detection_config,
        }

    def reset_history(self) -> None:
        """Remet √† z√©ro l'historique des √©motions."""
        self.emotion_history = []
        logger.info("üîÑ Historique des √©motions r√©initialis√©")


def main() -> None:
    """Test du module BBIA Emotion Recognition."""
    if not ML_AVAILABLE:
        logging.error("‚ùå D√©pendances ML non disponibles")
        logging.info("Installez avec: pip install mediapipe torch transformers")
        return

    # Initialisation
    emotion_rec = BBIAEmotionRecognition()

    # Test initialisation
    logging.info("üöÄ Test initialisation...")
    success = emotion_rec.initialize()
    logging.info(f"R√©sultat: {'‚úÖ' if success else '‚ùå'}")

    # Test analyse √©motion vocale
    logging.info("\nüó£Ô∏è Test analyse √©motion vocale...")
    vocal_result = emotion_rec.analyze_vocal_emotion(
        "Je suis tr√®s heureux aujourd'hui!",
    )
    logging.info(f"R√©sultat: {vocal_result}")

    # Test analyse √©motion faciale (simulation)
    logging.info("\nüòä Test analyse √©motion faciale...")
    facial_result = emotion_rec.analyze_facial_emotion(
        np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8),
    )
    logging.info(f"R√©sultat: {facial_result}")

    # Test fusion
    logging.info("\nüîÑ Test fusion √©motions...")
    fusion_result = emotion_rec.fuse_emotions(facial_result, vocal_result)
    logging.info(f"R√©sultat: {fusion_result}")

    # Statistiques
    logging.info(f"\nüìä Statistiques: {emotion_rec.get_emotion_statistics()}")


if __name__ == "__main__":
    main()
