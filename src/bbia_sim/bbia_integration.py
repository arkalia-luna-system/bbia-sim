#!/usr/bin/env python3
"""BBIA Integration - Module d'int√©gration BBIA ‚Üî Robot Reachy Mini
Connecte tous les modules BBIA au simulateur MuJoCo pour cr√©er une simulation compl√®te.
"""

import asyncio
import logging
from typing import TYPE_CHECKING, Optional

from .bbia_audio import detecter_son, enregistrer_audio, lire_audio

if TYPE_CHECKING:
    pass
from .bbia_behavior import BBIABehaviorManager
from .bbia_emotions import BBIAEmotions
from .bbia_vision import BBIAVision
from .bbia_voice import dire_texte, reconnaitre_parole
from .daemon.simulation_service import SimulationService

# Import conditionnel SDK Reachy-mini pour optimisations
try:
    from reachy_mini.utils import create_head_pose

    REACHY_MINI_UTILS_AVAILABLE = True
except ImportError:
    REACHY_MINI_UTILS_AVAILABLE = False
    create_head_pose = None

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BBIAIntegration:
    """Module d'int√©gration principal qui connecte tous les modules BBIA au robot Reachy Mini.

    Fonctionnalit√©s :
    - Mapping √©motions ‚Üí articulations du robot
    - R√©actions visuelles ‚Üí mouvements
    - Synchronisation audio ‚Üî mouvements
    - Gestion des comportements complexes
    """

    def __init__(self, simulation_service: Optional[SimulationService] = None) -> None:
        """Initialise l'int√©gration BBIA avec le service de simulation."""
        # Service de simulation
        self.simulation_service = simulation_service or SimulationService()

        # Modules BBIA
        # OPTIMISATION SDK: Passer robot_api aux modules pour utiliser robot.media
        robot_api = None
        if (
            hasattr(self.simulation_service, "robot_api")
            and self.simulation_service.robot_api
        ):
            robot_api = self.simulation_service.robot_api

        self.emotions = BBIAEmotions()
        self.vision = BBIAVision(
            robot_api=robot_api
        )  # Passer robot_api pour camera SDK

        # OPTIMISATION EXPERT: Passer robot_api au BBIABehaviorManager si disponible
        # pour que les comportements puissent contr√¥ler directement le robot
        self.behavior = BBIABehaviorManager(robot_api=robot_api)

        # Fonctions audio et voice (pas de classes)
        self.audio_functions = {
            "enregistrer": enregistrer_audio,
            "lire": lire_audio,
            "detecter": detecter_son,
        }
        self.voice_functions = {"dire": dire_texte, "reconnaitre": reconnaitre_parole}

        # √âtat de l'int√©gration
        self.is_active = False
        self.current_emotion = "neutral"
        self.emotion_intensity = 0.5

        # Mapping √©motions ‚Üí articulations
        self.emotion_mappings = self._create_emotion_mappings()

        # Configuration des r√©actions
        self.reaction_config = {
            "face_detection": {"head_turn_speed": 0.5, "tracking_active": True},
            "object_detection": {"point_speed": 0.3, "focus_duration": 2.0},
            "sound_detection": {"turn_speed": 0.4, "reaction_delay": 0.2},
        }

        logger.info("üé≠ BBIA Integration initialis√©e")
        logger.info(f"   ‚Ä¢ √âmotions disponibles : {len(self.emotions.emotions)}")
        logger.info("   ‚Ä¢ Articulations contr√¥lables : 16")
        logger.info(
            f"   ‚Ä¢ Service simulation : {'‚úÖ' if self.simulation_service else '‚ùå'}"
        )

    def _create_emotion_mappings(self) -> dict[str, dict[str, float]]:
        """Cr√©e le mapping des √©motions vers les positions d'articulations.

        NOTE: Ce mapping est compatible avec le SDK officiel Reachy Mini.
        Les valeurs utilisent create_head_pose (pitch, yaw) et yaw_body uniquement.
        Les joints interdits (antennes, stewart_4,5,6) ne sont PAS utilis√©s.
        """
        # Mapping conforme SDK officiel : utilise pitch/yaw pour la t√™te via create_head_pose
        # et yaw_body pour le corps. Les valeurs respectent les limites r√©elles du mod√®le.
        return {
            "neutral": {
                "yaw_body": 0.0,
                "head_pitch": 0.0,  # Pour create_head_pose
                "head_yaw": 0.0,  # Pour create_head_pose
            },
            "happy": {
                "yaw_body": 0.05,  # L√©g√®re rotation corps (limit√©e √† 0.3 rad max)
                "head_pitch": 0.1,  # Conforme SDK officiel
                "head_yaw": 0.0,  # Conforme SDK officiel
            },
            "sad": {
                "yaw_body": -0.05,
                "head_pitch": -0.1,  # Conforme SDK officiel
                "head_yaw": 0.0,
            },
            "angry": {
                "yaw_body": 0.0,
                "head_pitch": 0.15,  # L√©g√®rement relev√©e
                "head_yaw": 0.0,
            },
            "surprised": {
                "yaw_body": 0.08,
                "head_pitch": 0.15,  # T√™te relev√©e
                "head_yaw": 0.0,
            },
            "curious": {
                "yaw_body": 0.12,  # Rotation corps pour regarder
                "head_pitch": 0.05,  # Conforme SDK officiel
                "head_yaw": 0.2,  # Conforme SDK officiel
            },
            "excited": {
                "yaw_body": 0.15,
                "head_pitch": 0.2,  # Conforme SDK officiel
                "head_yaw": 0.1,  # Conforme SDK officiel
            },
            "fearful": {
                "yaw_body": -0.12,
                "head_pitch": -0.15,  # T√™te baiss√©e
                "head_yaw": 0.0,
            },
            # √âmotions suppl√©mentaires BBIA (mapp√©es vers les 6 √©motions SDK officiel)
            "confused": {
                "yaw_body": 0.1,
                "head_pitch": 0.05,
                "head_yaw": 0.15,  # Regard oblique (curieux)
            },
            "determined": {
                "yaw_body": 0.0,
                "head_pitch": 0.0,
                "head_yaw": 0.0,
            },
            "nostalgic": {
                "yaw_body": -0.08,
                "head_pitch": -0.05,  # Calme
                "head_yaw": 0.0,
            },
            "proud": {
                "yaw_body": 0.1,
                "head_pitch": 0.15,  # T√™te relev√©e (excited)
                "head_yaw": 0.05,
            },
            "calm": {
                "yaw_body": 0.0,
                "head_pitch": -0.05,  # Conforme SDK officiel
                "head_yaw": 0.0,
            },
        }

    async def start_integration(self) -> bool:
        """D√©marre l'int√©gration BBIA avec le simulateur."""
        try:
            logger.info("üöÄ D√©marrage de l'int√©gration BBIA...")

            # D√©marrer la simulation si n√©cessaire
            if not self.simulation_service.is_running:
                logger.info("üì¶ D√©marrage du service de simulation...")
                await self.simulation_service.start_simulation(headless=False)

                # Attendre que la simulation soit pr√™te
                await asyncio.sleep(2)

            # V√©rifier que la simulation est pr√™te
            if not self.simulation_service.is_simulation_ready():
                logger.error("‚ùå Service de simulation non pr√™t")
                return False

            # Initialiser l'√©tat BBIA
            self.is_active = True
            self.current_emotion = "neutral"

            # Appliquer l'√©motion neutre initiale
            await self.apply_emotion_to_robot("neutral", 0.5)

            logger.info("‚úÖ Int√©gration BBIA d√©marr√©e avec succ√®s")
            logger.info(f"   ‚Ä¢ √âmotion initiale : {self.current_emotion}")
            logger.info(f"   ‚Ä¢ Intensit√© : {self.emotion_intensity}")

            return True

        except Exception as e:
            logger.error(f"‚ùå Erreur d√©marrage int√©gration : {e}")
            return False

    async def stop_integration(self) -> None:
        """Arr√™te l'int√©gration BBIA."""
        logger.info("üõë Arr√™t de l'int√©gration BBIA...")

        self.is_active = False

        # Remettre le robot en position neutre
        await self.apply_emotion_to_robot("neutral", 0.5)

        logger.info("‚úÖ Int√©gration BBIA arr√™t√©e")

    async def apply_emotion_to_robot(
        self, emotion: str, intensity: float = 0.5
    ) -> bool:
        """Applique une √©motion au robot via les articulations.

        Args:
            emotion: Nom de l'√©motion √† appliquer
            intensity: Intensit√© de l'√©motion (0.0 √† 1.0)

        Returns:
            True si l'√©motion a √©t√© appliqu√©e avec succ√®s

        """
        if not self.is_active:
            logger.warning("‚ö†Ô∏è Int√©gration BBIA non active")
            return False

        if emotion not in self.emotion_mappings:
            logger.error(f"‚ùå √âmotion inconnue : {emotion}")
            return False

        try:
            logger.info(f"üé≠ Application √©motion '{emotion}' (intensit√©: {intensity})")

            # Mettre √† jour l'√©tat BBIA
            self.emotions.set_emotion(emotion, intensity)
            self.current_emotion = emotion
            self.emotion_intensity = intensity

            # R√©cup√©rer le mapping de l'√©motion
            emotion_mapping = self.emotion_mappings[emotion]

            # Appliquer les positions conform√©ment au SDK officiel Reachy Mini
            # Si le service utilise RobotAPI avec ReachyMiniBackend, utiliser set_emotion
            # Sinon, appliquer manuellement via create_head_pose

            # V√©rifier si on peut utiliser l'API directe
            if hasattr(self.simulation_service, "robot_api") and hasattr(
                self.simulation_service.robot_api, "set_emotion"
            ):
                # Utiliser l'API du backend directement (plus propre)
                # Mapper les √©motions BBIA vers les 6 √©motions SDK officiel
                sdk_emotion_map = {
                    "happy": "happy",
                    "sad": "sad",
                    "neutral": "neutral",
                    "excited": "excited",
                    "curious": "curious",
                    "calm": "calm",
                    # Mapper les autres vers l'√©motion SDK la plus proche
                    "angry": "excited",  # Excitation n√©gative
                    "surprised": "curious",  # Curiosit√© extr√™me
                    "fearful": "sad",  # Tristesse avec peur
                    "confused": "curious",  # Curiosit√©
                    "determined": "neutral",  # Neutre avec d√©termination
                    "nostalgic": "sad",  # Tristesse douce
                    "proud": "happy",  # Joie avec fiert√©
                }
                sdk_emotion = sdk_emotion_map.get(emotion, "neutral")
                robot_api = self.simulation_service.robot_api

                # OPTIMISATION EXPERT: Calculer les angles AVANT d'appliquer pour pouvoir utiliser goto_target
                # avec interpolation fluide (plus naturel et expressif que set_emotion directe)
                head_pitch = emotion_mapping.get("head_pitch", 0.0) * intensity
                head_yaw = emotion_mapping.get("head_yaw", 0.0) * intensity
                adjusted_yaw = emotion_mapping.get("yaw_body", 0.0) * intensity

                # OPTIMISATION INTELLIGENCE: S√©lection intelligente de la technique d'interpolation selon l'√©motion
                # CARTOON pour √©motions expressives, EASE_IN_OUT pour douces, MIN_JERK pour naturelles
                emotion_interpolation_map = {
                    "happy": "cartoon",  # Expressif et anim√©
                    "excited": "cartoon",  # Tr√®s expressif
                    "surprised": "cartoon",  # Sautillant
                    "calm": "ease_in_out",  # Doux et fluide
                    "sad": "ease_in_out",  # Lent et m√©lancolique
                    "nostalgic": "ease_in_out",  # Doux
                    "neutral": "minjerk",  # Naturel
                    "curious": "minjerk",  # Naturel
                    "angry": "cartoon",  # Expressif
                    "fearful": "ease_in_out",  # Doux mais inquiet
                    "determined": "minjerk",  # Naturel mais ferme
                    "proud": "cartoon",  # Expressif
                }
                interpolation_method = emotion_interpolation_map.get(emotion, "minjerk")

                # M√©thode 1 (pr√©f√©r√©e): goto_target avec interpolation optimis√©e selon l'√©motion pour transitions expressives
                # et mouvement combin√© t√™te+corps synchronis√© (meilleure expressivit√© √©motionnelle)
                if hasattr(robot_api, "goto_target") and hasattr(
                    robot_api, "get_current_head_pose"
                ):
                    try:
                        from reachy_mini.utils import create_head_pose

                        # Cr√©er pose t√™te avec angles de l'√©motion
                        pose = create_head_pose(
                            pitch=head_pitch, yaw=head_yaw, degrees=False
                        )

                        # Duration adaptative selon l'intensit√© (plus lente = plus expressive)
                        transition_duration = 0.5 + (
                            intensity * 0.5
                        )  # 0.5 √† 1.0 secondes

                        # Mouvement combin√© t√™te+corps avec interpolation optimis√©e selon l'√©motion
                        robot_api.goto_target(
                            head=pose,
                            body_yaw=adjusted_yaw,
                            duration=transition_duration,
                            method=interpolation_method,  # Interpolation intelligente selon √©motion
                        )

                        logger.info(
                            f"‚úÖ √âmotion '{emotion}' appliqu√©e via goto_target expressif "
                            f"(pitch={head_pitch:.3f}, yaw={head_yaw:.3f}, body={adjusted_yaw:.3f}, "
                            f"duration={transition_duration:.2f}s, method={interpolation_method})"
                        )
                        return True
                    except (ImportError, AttributeError, Exception) as e:
                        logger.debug(
                            f"goto_target non disponible, fallback set_emotion: {e}"
                        )

                # M√©thode 2 (fallback): set_emotion directe (moins fluide mais fonctionne)
                success = robot_api.set_emotion(sdk_emotion, intensity)

                # Appliquer yaw_body s√©par√©ment si goto_target n'a pas √©t√© utilis√©
                if "yaw_body" in emotion_mapping and success:
                    # Utiliser goto_target si disponible pour mouvement fluide combin√©
                    if (
                        hasattr(robot_api, "goto_target")
                        and REACHY_MINI_UTILS_AVAILABLE
                    ):
                        try:
                            # Utiliser la pose actuelle de la t√™te (pr√©serv√©e par set_emotion)
                            # et ajouter seulement le mouvement corps
                            robot_api.goto_target(
                                body_yaw=adjusted_yaw,
                                duration=0.6,  # Dur√©e fluide
                                method="minjerk",
                            )
                            logger.debug(
                                f"Body yaw ajust√© via goto_target (optimis√©): {adjusted_yaw:.3f}"
                            )
                        except (ImportError, AttributeError, Exception) as e:
                            logger.debug(f"goto_target non disponible (fallback): {e}")
                            # Fallback: application s√©par√©e
                            if hasattr(robot_api, "set_joint_pos"):
                                robot_api.set_joint_pos("yaw_body", adjusted_yaw)
                            else:
                                self.simulation_service.set_joint_position(
                                    "yaw_body", adjusted_yaw
                                )
                    else:
                        # Fallback: application s√©par√©e directe
                        if hasattr(robot_api, "set_joint_pos"):
                            robot_api.set_joint_pos("yaw_body", adjusted_yaw)
                        else:
                            self.simulation_service.set_joint_position(
                                "yaw_body", adjusted_yaw
                            )

                if success:
                    logger.info(f"‚úÖ √âmotion '{emotion}' appliqu√©e via SDK officiel")
                    return True

            # Fallback: Application manuelle (mode simulation sans SDK)
            # OPTIMISATION EXPERT: Utiliser goto_target pour mouvement combin√© fluide
            # au lieu de set_target_head_pose + set_joint_pos s√©par√©s
            head_pitch = emotion_mapping.get("head_pitch", 0.0) * intensity
            head_yaw = emotion_mapping.get("head_yaw", 0.0) * intensity
            adjusted_yaw = (
                emotion_mapping["yaw_body"] * intensity
                if "yaw_body" in emotion_mapping
                else 0.0
            )

            # IMPORTANT (Expert Robotique): Les joints stewart NE PEUVENT PAS √™tre contr√¥l√©s individuellement
            # car la plateforme Stewart utilise la cin√©matique inverse (IK).
            # Utiliser goto_target (m√©thode recommand√©e SDK) pour mouvement combin√© fluide
            if (
                hasattr(self.simulation_service, "robot_api")
                and self.simulation_service.robot_api
            ):
                try:
                    from reachy_mini.utils import create_head_pose

                    pose = create_head_pose(
                        pitch=head_pitch, yaw=head_yaw, degrees=False
                    )

                    # M√©thode 1 (pr√©f√©r√©e): goto_target avec interpolation fluide
                    if hasattr(self.simulation_service.robot_api, "goto_target"):
                        self.simulation_service.robot_api.goto_target(
                            head=pose,
                            body_yaw=adjusted_yaw,
                            duration=0.8,
                            method="minjerk",  # Interpolation recommand√©e SDK
                        )
                        logger.info(
                            f"‚úÖ √âmotion '{emotion}' appliqu√©e via goto_target SDK "
                            f"(pitch={head_pitch:.3f}, yaw={head_yaw:.3f}, body={adjusted_yaw:.3f})"
                        )
                    # M√©thode 2 (fallback): set_target_head_pose + set_joint_pos s√©par√©s
                    elif hasattr(
                        self.simulation_service.robot_api, "set_target_head_pose"
                    ):
                        self.simulation_service.robot_api.set_target_head_pose(pose)
                        if adjusted_yaw != 0.0:
                            if hasattr(
                                self.simulation_service.robot_api, "set_joint_pos"
                            ):
                                self.simulation_service.robot_api.set_joint_pos(
                                    "yaw_body", adjusted_yaw
                                )
                            else:
                                self.simulation_service.set_joint_position(
                                    "yaw_body", adjusted_yaw
                                )
                        logger.info(
                            f"‚úÖ √âmotion '{emotion}' appliqu√©e via pose t√™te SDK "
                            f"(pitch={head_pitch:.3f}, yaw={head_yaw:.3f})"
                        )
                except (ImportError, AttributeError, Exception) as e:
                    logger.warning(
                        f"Impossible d'utiliser create_head_pose/goto_target: {e}. "
                        f"Utilisation fallback joints directs."
                    )
                    # Fallback final: utiliser seulement yaw_body
                    if adjusted_yaw != 0.0:
                        self.simulation_service.set_joint_position(
                            "yaw_body", adjusted_yaw
                        )

            logger.info(
                f"√âmotion '{emotion}' : pitch={head_pitch:.3f}, "
                f"yaw={head_yaw:.3f}, body={emotion_mapping.get('yaw_body', 0.0)*intensity:.3f}"
            )

            logger.info(f"‚úÖ √âmotion '{emotion}' appliqu√©e au robot")
            return True

        except Exception as e:
            logger.error(f"‚ùå Erreur application √©motion : {e}")
            return False

    async def react_to_vision_detection(self, detection_data: dict) -> bool:
        """R√©agit aux d√©tections visuelles en contr√¥lant le robot.

        Args:
            detection_data: Donn√©es de d√©tection (objets, visages, etc.)

        Returns:
            True si la r√©action a √©t√© appliqu√©e

        """
        if not self.is_active:
            return False

        try:
            # R√©action √† la d√©tection de visage
            if "faces" in detection_data and detection_data["faces"]:
                logger.info("üë§ Visage d√©tect√© - R√©action de curiosit√©")
                await self.apply_emotion_to_robot("curious", 0.7)

                # OPTIMISATION EXPERT: Utiliser look_at_world/look_at_image du SDK si disponible
                # pour un suivi de visage fluide et pr√©cis (au lieu de set_joint_position direct)
                face_data = detection_data["faces"][0]
                if (
                    hasattr(self.simulation_service, "robot_api")
                    and self.simulation_service.robot_api
                ):
                    robot_api = self.simulation_service.robot_api
                    try:
                        # M√©thode 1 (pr√©f√©r√©e): look_at_world si position 3D disponible
                        pos_3d = face_data.get("position_3d", {})
                        if pos_3d and hasattr(robot_api, "look_at_world"):
                            x = float(pos_3d.get("x", 0.3))
                            y = float(pos_3d.get("y", 0.0))
                            z = float(pos_3d.get("z", 0.2))
                            if (
                                -2.0 <= x <= 2.0
                                and -2.0 <= y <= 2.0
                                and -1.0 <= z <= 1.0
                            ):
                                robot_api.look_at_world(
                                    x, y, z, duration=1.0, perform_movement=True
                                )
                                logger.info(
                                    f"Look_at_world vers visage: ({x:.2f}, {y:.2f}, {z:.2f})"
                                )
                            else:
                                raise ValueError("Position 3D hors limites")
                        # M√©thode 2: look_at_image si coordonn√©es image disponibles
                        elif hasattr(robot_api, "look_at_image"):
                            bbox = face_data.get("bbox", {})
                            if bbox:
                                u = int(bbox.get("center_x", 320))
                                v = int(bbox.get("center_y", 240))
                                if 0 <= u <= 640 and 0 <= v <= 480:
                                    robot_api.look_at_image(u, v, duration=1.0)
                                    logger.info(
                                        f"Look_at_image vers visage: ({u}, {v})"
                                    )
                        # M√©thode 3 (fallback): rotation corps
                        elif hasattr(robot_api, "set_joint_pos"):
                            face_position = face_data.get("position", (0, 0))
                            head_turn = (
                                float(face_position[0]) * 0.3
                            )  # Ajuster selon la position
                            robot_api.set_joint_pos("yaw_body", head_turn)
                    except Exception as e:
                        logger.warning(f"Erreur suivi visage SDK (fallback): {e}")
                        # Fallback final: m√©thode originale
                        face_position = face_data.get("position", (0, 0))
                        head_turn = float(face_position[0]) * 0.3
                        self.simulation_service.set_joint_position(
                            "yaw_body", head_turn
                        )
                else:
                    # Fallback: m√©thode originale sans SDK
                    face_position = face_data.get("position", (0, 0))
                    head_turn = float(face_position[0]) * 0.3
                    self.simulation_service.set_joint_position("yaw_body", head_turn)

                return True

            # R√©action √† la d√©tection d'objet int√©ressant
            if "objects" in detection_data and detection_data["objects"]:
                logger.info("üì¶ Objet d√©tect√© - R√©action de surprise")
                await self.apply_emotion_to_robot("surprised", 0.6)

                return True

            return False

        except Exception as e:
            logger.error(f"‚ùå Erreur r√©action visuelle : {e}")
            return False

    async def sync_voice_with_movements(
        self, text: str, emotion: str = "neutral"
    ) -> bool:
        """Synchronise la voix avec les mouvements du robot.

        Args:
            text: Texte √† prononcer
            emotion: √âmotion √† exprimer pendant la parole

        Returns:
            True si la synchronisation a √©t√© appliqu√©e

        """
        if not self.is_active:
            return False

        try:
            logger.info(f"üó£Ô∏è Synchronisation voix + mouvements : '{text[:30]}...'")

            # Appliquer l'√©motion pendant la parole
            await self.apply_emotion_to_robot(emotion, 0.6)

            # OPTIMISATION EXPERT: Utiliser goto_target pour mouvements fluides synchronis√©s
            # au lieu de set_joint_position r√©p√©t√©s (meilleure fluidit√© et performance)
            words = text.split()
            robot_api = None
            if (
                hasattr(self.simulation_service, "robot_api")
                and self.simulation_service.robot_api
            ):
                robot_api = self.simulation_service.robot_api

            for i, _word in enumerate(words):
                # Petit mouvement de t√™te pour chaque mot important
                if i % 3 == 0:  # Tous les 3 mots
                    head_movement = 0.1 if i % 6 == 0 else -0.1

                    # Utiliser goto_target si disponible (meilleure fluidit√©)
                    if robot_api and hasattr(robot_api, "goto_target"):
                        try:
                            robot_api.goto_target(
                                body_yaw=head_movement,
                                duration=0.15,  # Dur√©e courte pour mouvement subtil
                                method="minjerk",
                            )
                        except Exception as e:
                            logger.debug(f"Erreur goto_target voix (fallback): {e}")
                            # Fallback vers set_joint_pos
                            if hasattr(robot_api, "set_joint_pos"):
                                robot_api.set_joint_pos("yaw_body", head_movement)
                            else:
                                self.simulation_service.set_joint_position(
                                    "yaw_body", head_movement
                                )
                    else:
                        # Fallback: m√©thode originale
                        self.simulation_service.set_joint_position(
                            "yaw_body", head_movement
                        )

                # Petite pause entre les mots
                await asyncio.sleep(0.1)

            # Retour √† l'√©motion normale apr√®s la parole
            await self.apply_emotion_to_robot(
                self.current_emotion, self.emotion_intensity
            )

            return True

        except Exception as e:
            logger.error(f"‚ùå Erreur synchronisation voix : {e}")
            return False

    async def execute_behavior_sequence(self, behavior_name: str) -> bool:
        """Ex√©cute une s√©quence de comportement compl√®te.

        Args:
            behavior_name: Nom du comportement √† ex√©cuter

        Returns:
            True si la s√©quence a √©t√© ex√©cut√©e

        """
        if not self.is_active:
            return False

        try:
            logger.info(f"üé¨ Ex√©cution s√©quence comportement : {behavior_name}")

            # Ex√©cuter le comportement BBIA
            self.behavior.add_to_queue(behavior_name)

            # Appliquer les √©motions correspondantes au robot
            if behavior_name == "greeting":
                await self.apply_emotion_to_robot("happy", 0.8)
                await asyncio.sleep(1.0)
                await self.apply_emotion_to_robot("neutral", 0.5)

            elif behavior_name == "hide":
                await self.apply_emotion_to_robot("fearful", 0.7)
                await asyncio.sleep(2.0)
                await self.apply_emotion_to_robot("neutral", 0.5)

            elif behavior_name == "antenna_animation":
                await self.apply_emotion_to_robot("excited", 0.9)
                await asyncio.sleep(1.5)
                await self.apply_emotion_to_robot("neutral", 0.5)

            logger.info(f"‚úÖ S√©quence '{behavior_name}' ex√©cut√©e")
            return True

        except Exception as e:
            logger.error(f"‚ùå Erreur ex√©cution comportement : {e}")
            return False

    def get_integration_status(self) -> dict:
        """Retourne le statut de l'int√©gration BBIA."""
        return {
            "is_active": self.is_active,
            "current_emotion": self.current_emotion,
            "emotion_intensity": self.emotion_intensity,
            "simulation_ready": self.simulation_service.is_simulation_ready(),
            "available_emotions": list(self.emotion_mappings.keys()),
            "reaction_config": self.reaction_config,
        }


# Fonction utilitaire pour cr√©er une instance d'int√©gration
async def create_bbia_integration(model_path: Optional[str] = None) -> BBIAIntegration:
    """Cr√©e et initialise une instance d'int√©gration BBIA.

    Args:
        model_path: Chemin vers le mod√®le MJCF (optionnel)

    Returns:
        Instance d'int√©gration BBIA pr√™te √† utiliser

    """
    # Cr√©er le service de simulation
    simulation_service = SimulationService(model_path)

    # Cr√©er l'int√©gration BBIA
    integration = BBIAIntegration(simulation_service)

    # D√©marrer l'int√©gration
    success = await integration.start_integration()

    if not success:
        raise RuntimeError("Impossible de d√©marrer l'int√©gration BBIA")

    return integration
