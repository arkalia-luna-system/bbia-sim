#!/usr/bin/env python3
"""BBIA Chat - Module conversationnel intelligent avec LLM l√©ger.

Int√©gration de mod√®les LLM l√©gers (Phi-2, TinyLlama) pour remplacer
le syst√®me de r√®gles basiques par un v√©ritable assistant conversationnel.
"""

import logging
import re
import time
from collections import deque
from typing import TYPE_CHECKING, Any

logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    from .robot_api import RobotAPI

# Import conditionnel des d√©pendances Hugging Face
try:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer

    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    logger.warning(
        "D√©pendances Hugging Face non disponibles. "
        "Installez avec: pip install transformers torch accelerate"
    )


class BBIAChat:
    """Module conversationnel intelligent avec LLM l√©ger.

    Remplace le syst√®me de r√®gles basiques par un LLM conversationnel
    (Phi-2 ou TinyLlama) pour des r√©ponses intelligentes et contextuelles.

    Attributes:
        llm_model: Mod√®le LLM charg√© (AutoModelForCausalLM)
        llm_tokenizer: Tokenizer associ√© (AutoTokenizer)
        robot_api: Interface robotique (optionnel)
        context: Historique conversation (deque, max 10 messages)
        personality: Personnalit√© actuelle (friendly, professional, etc.)
        user_preferences: Pr√©f√©rences utilisateur apprises
    """

    def __init__(self, robot_api: "RobotAPI | None" = None) -> None:
        """Initialise le module de chat avec LLM.

        Args:
            robot_api: Interface robotique pour ex√©cuter actions (optionnel)
        """
        self.robot_api = robot_api
        self.llm_model: Any | None = None
        self.llm_tokenizer: Any | None = None
        self.context: deque[dict[str, Any]] = deque(maxlen=10)
        self.personality = "friendly"
        self.user_preferences: dict[str, Any] = {}
        self.preferences_file = "bbia_memory/user_preferences.json"

        # Charger pr√©f√©rences sauvegard√©es
        self._load_preferences()

        # Personnalit√©s disponibles
        self.PERSONALITIES = {
            "friendly": {
                "system_prompt": (
                    "Tu es BBIA, un assistant robotique amical et chaleureux. "
                    "Tu communiques en fran√ßais de mani√®re naturelle, chaleureuse "
                    "et authentique, comme un v√©ritable compagnon. "
                    "Tu √©vites les phrases r√©p√©titives ou trop g√©n√©riques. "
                    "Tes r√©ponses sont concises (max 2-3 phrases), engageantes "
                    "et montrent que tu comprends vraiment l'interlocuteur."
                ),
                "tone": "chaleureux, empathique",
            },
            "professional": {
                "system_prompt": (
                    "Tu es BBIA, un assistant robotique professionnel et efficace. "
                    "Tu communiques en fran√ßais de mani√®re formelle, pr√©cise "
                    "et structur√©e. Tes r√©ponses sont concises et factuelles, "
                    "sans fioritures. Tu restes courtois mais direct."
                ),
                "tone": "formel, pr√©cis",
            },
            "playful": {
                "system_prompt": (
                    "Tu es BBIA, un assistant robotique joueur et amusant. "
                    "Tu communiques en fran√ßais de mani√®re d√©contract√©e, "
                    "humoristique et l√©g√®re. Tu utilises des expressions "
                    "naturelles et tu n'h√©sites pas √† faire des blagues "
                    "appropri√©es. Tes r√©ponses sont √©nergiques et positives."
                ),
                "tone": "d√©contract√©, humoristique",
            },
            "calm": {
                "system_prompt": (
                    "Tu es BBIA, un assistant robotique serein et apaisant. "
                    "Tu communiques en fran√ßais avec douceur et profondeur, "
                    "en prenant le temps n√©cessaire. Tes r√©ponses refl√®tent "
                    "une sagesse tranquille et une approche r√©fl√©chie."
                ),
                "tone": "doux, apaisant",
            },
            "enthusiastic": {
                "system_prompt": (
                    "Tu es BBIA, un assistant robotique plein d'enthousiasme "
                    "et d'√©nergie positive. Tu communiques en fran√ßais avec "
                    "joie et dynamisme. Tu transmets ta passion pour communiquer "
                    "et tu encourages l'interaction de mani√®re vivante et authentique."
                ),
                "tone": "√©nergique, positif",
            },
        }

        # OPTIMISATION RAM: Lazy loading strict - ne pas charger LLM √† l'init
        # Le LLM sera charg√© seulement au premier appel de chat()
        # self._load_llm()  # D√âSACTIV√â pour lazy loading strict

    def _load_llm(self) -> None:
        """Charge le LLM l√©ger (Phi-2 ou TinyLlama avec fallback).

        Essaie d'abord Phi-2, puis TinyLlama si √©chec.
        Gestion m√©moire optimis√©e (torch.float16, device_map="auto").
        """
        if not HF_AVAILABLE:
            logger.warning("Hugging Face non disponible - mode fallback activ√©")
            return

        # Essayer Phi-2 d'abord (recommand√©)
        try:
            logger.info("üì• Chargement Phi-2 (2.7B param√®tres)...")
            model_name = "microsoft/phi-2"
            self.llm_tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
            )

            if self.llm_tokenizer is not None and self.llm_tokenizer.pad_token is None:
                self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token

            self.llm_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                dtype=torch.float16,  # R√©duire RAM (torch_dtype deprecated)
                device_map="auto",  # Distribution automatique
                trust_remote_code=True,
            )
            logger.info("‚úÖ Phi-2 charg√© avec succ√®s")
            return

        except Exception as e:
            logger.warning("‚ö†Ô∏è Impossible de charger Phi-2: %s", e)
            logger.info("Tentative de chargement TinyLlama (fallback)...")

        # Fallback: TinyLlama (ultra-l√©ger)
        try:
            logger.info("üì• Chargement TinyLlama (1.1B param√®tres)...")
            model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
            self.llm_tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
            )

            if self.llm_tokenizer is not None and self.llm_tokenizer.pad_token is None:
                self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token

            self.llm_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                dtype=torch.float16,  # R√©duire RAM (torch_dtype deprecated)
                device_map="auto",
                trust_remote_code=True,
            )
            logger.info("‚úÖ TinyLlama charg√© avec succ√®s")

        except Exception as e:
            logger.exception("‚ùå Impossible de charger TinyLlama: %s", e)
            logger.warning("Mode fallback: r√©ponses basiques (sans LLM)")

    def generate(
        self,
        prompt: str,
        max_length: int = 200,
        temperature: float = 0.7,
        timeout: float = 5.0,
    ) -> str:
        """G√©n√®re une r√©ponse avec le LLM.

        Args:
            prompt: Prompt utilisateur
            max_length: Longueur maximale de la r√©ponse (tokens)
            temperature: Temp√©rature pour g√©n√©ration (0.0-1.0)
            timeout: Timeout maximum (secondes)

        Returns:
            R√©ponse g√©n√©r√©e par le LLM, ou message d'erreur si √©chec
        """
        if not self.llm_model or not self.llm_tokenizer:
            return "D√©sol√©, le mod√®le LLM n'est pas disponible."

        try:
            # Valider longueur prompt (max 2000 tokens)
            inputs = self.llm_tokenizer(prompt, return_tensors="pt")
            if inputs.input_ids.shape[1] > 2000:
                logger.warning("Prompt trop long, tronqu√© √† 2000 tokens")
                inputs = self.llm_tokenizer(
                    prompt[:2000],
                    return_tensors="pt",
                    truncation=True,
                    max_length=2000,
                )

            # G√©n√©ration avec timeout
            start_time = time.time()
            with torch.no_grad():
                outputs = self.llm_model.generate(
                    inputs.input_ids.to(self.llm_model.device),
                    max_length=max_length,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.llm_tokenizer.eos_token_id,
                )

            # V√©rifier timeout
            if time.time() - start_time > timeout:
                logger.warning("G√©n√©ration LLM d√©pass√©e timeout")
                return "D√©sol√©, la g√©n√©ration prend trop de temps."

            # D√©coder r√©ponse
            response: str = str(
                self.llm_tokenizer.decode(outputs[0], skip_special_tokens=True)
            )

            # Nettoyer r√©ponse (retirer prompt si pr√©sent)
            if prompt in response:
                response = response.replace(prompt, "").strip()

            # Sanitizer: retirer code ex√©cutable potentiel
            return self._sanitize_response(response)

        except Exception as e:
            logger.exception("‚ùå Erreur g√©n√©ration LLM: %s", e)
            return "D√©sol√©, une erreur s'est produite lors de la g√©n√©ration."

    def _sanitize_response(self, response: str) -> str:
        """Nettoie la r√©ponse LLM pour s√©curit√©.

        Retire code ex√©cutable potentiel et limite longueur.

        Args:
            response: R√©ponse brute du LLM

        Returns:
            R√©ponse nettoy√©e et s√©curis√©e
        """
        # Retirer blocs de code (```...```)
        response = re.sub(r"```[\s\S]*?```", "", response)
        # Retirer imports Python
        response = re.sub(r"^import\s+\w+", "", response, flags=re.MULTILINE)
        response = re.sub(r"^from\s+\w+", "", response, flags=re.MULTILINE)
        # Limiter longueur (max 500 caract√®res)
        if len(response) > 500:
            response = response[:500] + "..."
        return response.strip()

    def chat(self, user_message: str) -> str:
        """Chat intelligent avec contexte et actions robot.

        Args:
            user_message: Message utilisateur

        Returns:
            R√©ponse intelligente de BBIA
        """
        # OPTIMISATION RAM: Lazy loading strict - charger LLM seulement au premier appel
        if not self.llm_model or not self.llm_tokenizer:
            logger.info("üì• Chargement LLM √† la demande (lazy loading)...")
            self._load_llm()
            # Si √©chec chargement, continuer avec fallback
            if not self.llm_model or not self.llm_tokenizer:
                logger.warning("LLM non disponible, utilisation mode fallback")

        if not user_message or not user_message.strip():
            return "Je n'ai pas compris votre message. Pouvez-vous reformuler ?"

        # Valider input utilisateur
        user_message = user_message.strip()[:1000]  # Limiter longueur

        try:
            # 1. D√©tecter actions robot ("tourne la t√™te", etc.)
            action = self._detect_action(user_message)
            action_confirmation = ""
            if action:
                self._execute_action(action)
                # Confirmer ex√©cution dans r√©ponse
                action_confirmation = f"J'ai ex√©cut√© l'action: {action['action']}. "

            # 2. Extraire et appliquer √©motions
            emotion = self._extract_emotion(user_message)
            if emotion:
                self._apply_emotion(emotion)

            # 3. Construire prompt avec contexte et personnalit√©
            prompt = self._build_context_prompt(user_message)

            # Ajouter confirmation action au prompt si pr√©sente
            if action_confirmation:
                prompt = prompt.replace(
                    f"Utilisateur: {user_message}\nBBIA: ",
                    f"Utilisateur: {user_message}\n{action_confirmation}BBIA: ",
                )

            # 4. G√©n√©rer r√©ponse avec LLM
            response = self.generate(prompt, max_length=200, temperature=0.7)

            # 5. Adapter r√©ponse selon pr√©f√©rences utilisateur
            response = self._adapt_to_preferences(response)

            # 6. Sauvegarder dans contexte
            self.context.append(
                {
                    "user": user_message,
                    "assistant": response,
                    "timestamp": time.time(),
                }
            )

            return response

        except Exception as e:
            logger.exception("‚ùå Erreur chat: %s", e)
            return "Je ne comprends pas bien, peux-tu reformuler ?"

    def _build_context_prompt(self, user_message: str) -> str:
        """Construit prompt avec contexte historique et personnalit√©.

        Args:
            user_message: Message utilisateur

        Returns:
            Prompt complet avec contexte
        """
        # Prompt syst√®me avec personnalit√©
        personality_config = self.PERSONALITIES.get(
            self.personality, self.PERSONALITIES["friendly"]
        )
        system_prompt = f"{personality_config['system_prompt']}\n\n"

        # Ajouter historique (5 derniers messages)
        context_text = ""
        if self.context:
            for entry in list(self.context)[-5:]:
                context_text += f"Utilisateur: {entry.get('user', '')}\n"
                context_text += f"BBIA: {entry.get('assistant', '')}\n\n"

        # Message actuel
        prompt = f"{system_prompt}{context_text}Utilisateur: {user_message}\nBBIA: "

        return prompt

    def _detect_action(self, user_message: str) -> dict[str, Any] | None:
        """D√©tecte action robot dans message utilisateur.

        Args:
            user_message: Message utilisateur

        Returns:
            Dictionnaire avec action d√©tect√©e, ou None
        """
        # Patterns de d√©tection (regex)
        patterns = {
            "look_right": r"(tourne|regarde|dirige).*(droite|right)",
            "look_left": r"(tourne|regarde|dirige).*(gauche|left)",
            "look_up": r"(tourne|regarde|dirige).*(haut|up)",
            "look_down": r"(tourne|regarde|dirige).*(bas|down)",
            "wake_up": r"(r√©veille|wake|allume)",
            "sleep": r"(endors|sleep|√©teins)",
        }

        for action, pattern in patterns.items():
            if re.search(pattern, user_message, re.IGNORECASE):
                return {"action": action, "confidence": 0.9}

        return None

    def _execute_action(self, action: dict[str, Any]) -> None:
        """Ex√©cute action robot via robot_api.

        Args:
            action: Dictionnaire avec action √† ex√©cuter
        """
        if not self.robot_api:
            logger.debug("robot_api non disponible - action non ex√©cut√©e")
            return

        action_name = action.get("action")
        if not action_name:
            return

        try:
            # Importer create_head_pose du SDK officiel
            try:
                from reachy_mini.utils import (  # type: ignore[import-untyped]
                    create_head_pose,
                )
            except ImportError:
                logger.warning("SDK officiel non disponible - action non ex√©cut√©e")
                return

            # Ex√©cuter action selon type
            if action_name == "look_right":
                pose = create_head_pose(yaw=0.3, pitch=0.0, degrees=False)
                if hasattr(self.robot_api, "goto_target"):
                    self.robot_api.goto_target(head=pose, duration=1.0)
                logger.info("‚úÖ Action ex√©cut√©e: look_right")

            elif action_name == "look_left":
                pose = create_head_pose(yaw=-0.3, pitch=0.0, degrees=False)
                if hasattr(self.robot_api, "goto_target"):
                    self.robot_api.goto_target(head=pose, duration=1.0)
                logger.info("‚úÖ Action ex√©cut√©e: look_left")

            elif action_name == "look_up":
                pose = create_head_pose(yaw=0.0, pitch=0.3, degrees=False)
                if hasattr(self.robot_api, "goto_target"):
                    self.robot_api.goto_target(head=pose, duration=1.0)
                logger.info("‚úÖ Action ex√©cut√©e: look_up")

            elif action_name == "look_down":
                pose = create_head_pose(yaw=0.0, pitch=-0.3, degrees=False)
                if hasattr(self.robot_api, "goto_target"):
                    self.robot_api.goto_target(head=pose, duration=1.0)
                logger.info("‚úÖ Action ex√©cut√©e: look_down")

            elif action_name == "wake_up":
                # R√©veiller robot (position neutre)
                pose = create_head_pose(yaw=0.0, pitch=0.0, degrees=False)
                if hasattr(self.robot_api, "goto_target"):
                    self.robot_api.goto_target(head=pose, duration=1.0)
                logger.info("‚úÖ Action ex√©cut√©e: wake_up")

            elif action_name == "sleep":
                # Endormir robot (position basse)
                pose = create_head_pose(yaw=0.0, pitch=-0.2, degrees=False)
                if hasattr(self.robot_api, "goto_target"):
                    self.robot_api.goto_target(head=pose, duration=1.0)
                logger.info("‚úÖ Action ex√©cut√©e: sleep")

        except Exception as e:
            logger.exception("‚ùå Erreur ex√©cution action %s: %s", action_name, e)

    def _extract_emotion(self, user_message: str) -> str | None:
        """Extrait √©motion du message utilisateur.

        Args:
            user_message: Message utilisateur

        Returns:
            Nom de l'√©motion d√©tect√©e, ou None
        """
        emotion_keywords = {
            "happy": ["content", "heureux", "joyeux", "sourire", "super", "g√©nial"],
            "sad": ["triste", "malheureux", "d√©prim√©", "d√©√ßu"],
            "angry": ["√©nerv√©", "f√¢ch√©", "col√®re", "frustr√©"],
            "excited": ["excit√©", "enthousiaste", "impatient", "impatience"],
            "curious": ["curieux", "int√©ress√©", "question"],
            "surprised": ["surpris", "√©tonn√©", "choqu√©"],
            "fearful": ["peur", "craintif", "inquiet", "anxieux"],
            "calm": ["calme", "serein", "paisible", "tranquille"],
        }

        user_lower = user_message.lower()
        for emotion, keywords in emotion_keywords.items():
            if any(kw in user_lower for kw in keywords):
                return emotion

        return None

    def _apply_emotion(self, emotion: str) -> None:
        """Applique √©motion au robot via BBIAEmotions.

        Args:
            emotion: Nom de l'√©motion √† appliquer
        """
        if not self.robot_api:
            logger.debug("robot_api non disponible - √©motion non appliqu√©e")
            return

        try:
            from .bbia_emotions import BBIAEmotions

            emotions_module = BBIAEmotions()
            emotions_module.set_emotion(emotion, intensity=0.7)

            # Appliquer aussi via robot_api si disponible
            if hasattr(self.robot_api, "set_emotion"):
                self.robot_api.set_emotion(emotion, 0.7)

            logger.info("‚úÖ √âmotion appliqu√©e: %s", emotion)

        except ImportError:
            logger.warning("BBIAEmotions non disponible - √©motion non appliqu√©e")
        except Exception as e:
            logger.exception("‚ùå Erreur application √©motion %s: %s", emotion, e)

    def set_personality(self, personality: str) -> None:
        """Change la personnalit√© du chat.

        Args:
            personality: Nom de la personnalit√© (friendly, professional, playful, calm, enthusiastic)
        """
        if personality in self.PERSONALITIES:
            self.personality = personality
            logger.info("‚úÖ Personnalit√© chang√©e: %s", personality)
        else:
            logger.warning(
                f"‚ö†Ô∏è Personnalit√© invalide: {personality}. "
                f"Personnalit√©s disponibles: {list(self.PERSONALITIES.keys())}"
            )

    def learn_preference(self, user_action: str, context: dict[str, Any]) -> None:
        """Apprend pr√©f√©rence utilisateur.

        Args:
            user_action: Action ou pr√©f√©rence exprim√©e par l'utilisateur
            context: Contexte de la pr√©f√©rence
        """
        user_lower = user_action.lower()

        # D√©tecter pr√©f√©rences courantes
        if "court" in user_lower or "bref" in user_lower or "court" in user_lower:
            self.user_preferences["response_length"] = "short"
        elif "d√©taill√©" in user_lower or "long" in user_lower:
            self.user_preferences["response_length"] = "long"

        if "formel" in user_lower or "professionnel" in user_lower:
            self.user_preferences["tone"] = "formal"
        elif "d√©contract√©" in user_lower or "casual" in user_lower:
            self.user_preferences["tone"] = "casual"

        # Fusionner avec contexte
        self.user_preferences.update(context)

        # Sauvegarder pr√©f√©rences
        self._save_preferences()

        logger.info("‚úÖ Pr√©f√©rence apprise: %s", self.user_preferences)

    def _adapt_to_preferences(self, response: str) -> str:
        """Adapte r√©ponse selon pr√©f√©rences utilisateur.

        Args:
            response: R√©ponse brute du LLM

        Returns:
            R√©ponse adapt√©e selon pr√©f√©rences
        """
        # Adapter longueur
        if self.user_preferences.get("response_length") == "short":
            sentences = response.split(".")
            if len(sentences) > 2:
                response = ". ".join(sentences[:2]) + "."
        elif self.user_preferences.get("response_length") == "long":
            # Garder r√©ponse compl√®te
            pass

        # Adapter ton (d√©j√† g√©r√© par personnalit√©, mais peut √™tre affin√©)
        if self.user_preferences.get("tone") == "formal":
            # Remplacer expressions d√©contract√©es
            response = response.replace("salut", "bonjour")
            response = response.replace("√ßa", "cela")
        elif self.user_preferences.get("tone") == "casual":
            # Rendre plus d√©contract√©
            pass

        return response

    def _save_preferences(self) -> None:
        """Sauvegarde pr√©f√©rences utilisateur dans fichier JSON."""
        try:
            import json
            import os

            # Cr√©er r√©pertoire si n√©cessaire
            prefs_dir = os.path.dirname(self.preferences_file)
            if prefs_dir and not os.path.exists(prefs_dir):
                os.makedirs(prefs_dir, exist_ok=True)

            # Sauvegarder
            with open(self.preferences_file, "w", encoding="utf-8") as f:
                json.dump(self.user_preferences, f, indent=2, ensure_ascii=False)

            logger.debug("‚úÖ Pr√©f√©rences sauvegard√©es: %s", self.preferences_file)

        except Exception as e:
            logger.warning("‚ö†Ô∏è Erreur sauvegarde pr√©f√©rences: %s", e)

    def _load_preferences(self) -> None:
        """Charge pr√©f√©rences utilisateur depuis fichier JSON."""
        try:
            import json
            import os

            if os.path.exists(self.preferences_file):
                with open(self.preferences_file, encoding="utf-8") as f:
                    loaded_prefs = json.load(f)
                    # Fusionner avec pr√©f√©rences existantes (ne pas √©craser)
                    if isinstance(loaded_prefs, dict):
                        self.user_preferences.update(loaded_prefs)
                logger.debug("‚úÖ Pr√©f√©rences charg√©es: %s", self.preferences_file)

        except Exception as e:
            logger.debug(
                "Pr√©f√©rences non charg√©es (normal si premi√®re utilisation): %s", e
            )
