#!/usr/bin/env python3
"""BBIA Chat - Module conversationnel intelligent avec LLM l√©ger.

Int√©gration de mod√®les LLM l√©gers (Phi-2, TinyLlama) pour remplacer
le syst√®me de r√®gles basiques par un v√©ritable assistant conversationnel.
"""

import hashlib
import json
import logging
import re
import time
from collections import OrderedDict, deque
from functools import lru_cache
from pathlib import Path
from typing import TYPE_CHECKING, Any

logger = logging.getLogger(__name__)


# OPTIMISATION PERFORMANCE: Cache pour regex compil√©es (comme bbia_huggingface.py)
@lru_cache(maxsize=32)
def _get_compiled_regex(pattern: str, flags: int = 0) -> re.Pattern[str]:
    """Retourne regex compil√©e depuis cache (√©vite recompilation r√©p√©t√©e).

    Args:
        pattern: Pattern regex
        flags: Flags re (ex: re.MULTILINE)

    Returns:
        Regex compil√©e (cach√©e ou nouvellement compil√©e)

    """
    return re.compile(pattern, flags)


# OPTIMISATION PERFORMANCE: Cache LRU pour r√©ponses LLM fr√©quentes
class LLMResponseCache:
    """Cache LRU avec TTL pour r√©ponses LLM.

    Optimise les performances en √©vitant de r√©g√©n√©rer des r√©ponses identiques.
    Utilise un cache LRU (Least Recently Used) avec expiration temporelle.

    Attributes:
        max_size: Taille maximale du cache (nombre d'entr√©es)
        ttl_seconds: Dur√©e de vie des entr√©es en secondes (Time To Live)
        hits: Nombre de hits (r√©ponses trouv√©es dans le cache)
        misses: Nombre de misses (r√©ponses non trouv√©es)
    """

    def __init__(self, max_size: int = 100, ttl_seconds: float = 3600.0) -> None:
        """Initialise le cache LRU.

        Args:
            max_size: Nombre maximum d'entr√©es dans le cache (d√©faut: 100)
            ttl_seconds: Dur√©e de vie des entr√©es en secondes (d√©faut: 1h)
        """
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self._cache: OrderedDict[str, tuple[str, float]] = OrderedDict()
        self.hits = 0
        self.misses = 0

    def _normalize_prompt(self, prompt: str) -> str:
        """Normalise un prompt pour cr√©er une cl√© de cache stable.

        Args:
            prompt: Prompt original

        Returns:
            Prompt normalis√© (minuscules, espaces normalis√©s)
        """
        # Normaliser: minuscules, espaces multiples ‚Üí espace unique
        normalized = re.sub(r"\s+", " ", prompt.lower().strip())
        return normalized

    def _get_cache_key(self, prompt: str, max_length: int, temperature: float) -> str:
        """G√©n√®re une cl√© de cache unique pour un prompt et param√®tres.

        Args:
            prompt: Prompt utilisateur
            max_length: Longueur maximale de r√©ponse
            temperature: Temp√©rature de g√©n√©ration

        Returns:
            Cl√© de cache (hash SHA256)
        """
        normalized = self._normalize_prompt(prompt)
        # Inclure param√®tres dans la cl√© pour √©viter collisions
        key_data = f"{normalized}|{max_length}|{temperature:.2f}"
        return hashlib.sha256(key_data.encode()).hexdigest()

    def get(
        self, prompt: str, max_length: int = 200, temperature: float = 0.7
    ) -> str | None:
        """R√©cup√®re une r√©ponse depuis le cache si disponible et valide.

        Args:
            prompt: Prompt utilisateur
            max_length: Longueur maximale de r√©ponse
            temperature: Temp√©rature de g√©n√©ration

        Returns:
            R√©ponse en cache si disponible et valide, None sinon
        """
        cache_key = self._get_cache_key(prompt, max_length, temperature)
        current_time = time.time()

        if cache_key in self._cache:
            response, timestamp = self._cache[cache_key]

            # V√©rifier expiration TTL
            if current_time - timestamp < self.ttl_seconds:
                # D√©placer en fin (LRU: most recently used)
                self._cache.move_to_end(cache_key)
                self.hits += 1
                logger.debug("‚ôªÔ∏è Cache hit pour prompt: %s...", prompt[:50])
                return response
            # Expir√©, supprimer
            del self._cache[cache_key]

        self.misses += 1
        return None

    def put(
        self,
        prompt: str,
        response: str,
        max_length: int = 200,
        temperature: float = 0.7,
    ) -> None:
        """Ajoute une r√©ponse au cache.

        Args:
            prompt: Prompt utilisateur
            response: R√©ponse g√©n√©r√©e
            max_length: Longueur maximale de r√©ponse
            temperature: Temp√©rature de g√©n√©ration
        """
        cache_key = self._get_cache_key(prompt, max_length, temperature)
        current_time = time.time()

        # Supprimer entr√©es expir√©es si cache plein
        if len(self._cache) >= self.max_size:
            # Supprimer entr√©es expir√©es d'abord
            expired_keys = [
                key
                for key, (_, timestamp) in self._cache.items()
                if current_time - timestamp >= self.ttl_seconds
            ]
            for key in expired_keys:
                del self._cache[key]

            # Si toujours plein, supprimer LRU (premi√®re entr√©e)
            if len(self._cache) >= self.max_size:
                self._cache.popitem(last=False)  # Supprimer oldest (LRU)

        # Ajouter nouvelle entr√©e
        self._cache[cache_key] = (response, current_time)
        logger.debug("üíæ R√©ponse mise en cache: %s...", prompt[:50])

    def get_stats(self) -> dict[str, Any]:
        """Retourne les statistiques du cache.

        Returns:
            Dictionnaire avec hits, misses, hit_rate, size
        """
        total = self.hits + self.misses
        hit_rate = (self.hits / total * 100) if total > 0 else 0.0

        return {
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": round(hit_rate, 2),
            "size": len(self._cache),
            "max_size": self.max_size,
            "ttl_seconds": self.ttl_seconds,
        }

    def clear(self) -> None:
        """Vide le cache."""
        self._cache.clear()
        self.hits = 0
        self.misses = 0
        logger.debug("üóëÔ∏è Cache LLM vid√©")


# Cache global partag√© pour toutes les instances BBIAChat
_global_llm_cache: LLMResponseCache | None = None


def _get_global_llm_cache() -> LLMResponseCache:
    """Retourne le cache global LLM (cr√©√© √† la demande).

    Returns:
        Instance du cache global LLM
    """
    global _global_llm_cache
    if _global_llm_cache is None:
        # Configuration via variables d'environnement
        import os

        max_size = int(os.environ.get("BBIA_LLM_CACHE_SIZE", "100"))
        ttl_seconds = float(os.environ.get("BBIA_LLM_CACHE_TTL", "3600.0"))
        _global_llm_cache = LLMResponseCache(max_size=max_size, ttl_seconds=ttl_seconds)
        logger.info(
            "üíæ Cache LLM initialis√© (max_size=%d, ttl=%.0fs)",
            max_size,
            ttl_seconds,
        )
    return _global_llm_cache


if TYPE_CHECKING:
    from .robot_api import RobotAPI

# Import conditionnel des d√©pendances Hugging Face
try:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer

    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    logger.warning(
        "D√©pendances Hugging Face non disponibles. "
        "Installez avec: pip install transformers torch accelerate",
    )


class BBIAChat:
    """Module conversationnel intelligent avec LLM l√©ger.

    Remplace le syst√®me de r√®gles basiques par un LLM conversationnel
    (Phi-2 ou TinyLlama) pour des r√©ponses intelligentes et contextuelles.

    Attributes:
        llm_model: Mod√®le LLM charg√© (AutoModelForCausalLM)
        llm_tokenizer: Tokenizer associ√© (AutoTokenizer)
        robot_api: Interface robotique (optionnel)
        context: Historique conversation (deque, max 10 messages)
        personality: Personnalit√© actuelle (friendly, professional, etc.)
        user_preferences: Pr√©f√©rences utilisateur apprises

    """

    # Constantes pour limites de longueur
    MAX_USER_MESSAGE_LENGTH = 1000  # Longueur maximale message utilisateur
    MAX_RESPONSE_LENGTH = 500  # Longueur maximale r√©ponse g√©n√©r√©e

    def __init__(self, robot_api: "RobotAPI | None" = None) -> None:
        """Initialise le module de chat avec LLM.

        Args:
            robot_api: Interface robotique pour ex√©cuter actions (optionnel)

        """
        self.robot_api = robot_api
        self.llm_model: Any | None = None
        self.llm_tokenizer: Any | None = None
        self.context: deque[dict[str, Any]] = deque(maxlen=10)
        self.personality = "friendly"
        self.user_preferences: dict[str, Any] = {}
        self.preferences_file = "bbia_memory/user_preferences.json"

        # Charger pr√©f√©rences sauvegard√©es
        self._load_preferences()

        # Personnalit√©s disponibles
        self.PERSONALITIES = {
            "friendly": {
                "system_prompt": (
                    "Tu es BBIA, un assistant robotique amical et chaleureux. "
                    "Tu communiques en fran√ßais de mani√®re naturelle, chaleureuse "
                    "et authentique, comme un v√©ritable compagnon. "
                    "Tu √©vites les phrases r√©p√©titives ou trop g√©n√©riques. "
                    "Tes r√©ponses sont concises (max 2-3 phrases), engageantes "
                    "et montrent que tu comprends vraiment l'interlocuteur."
                ),
                "tone": "chaleureux, empathique",
            },
            "professional": {
                "system_prompt": (
                    "Tu es BBIA, un assistant robotique professionnel et efficace. "
                    "Tu communiques en fran√ßais de mani√®re formelle, pr√©cise "
                    "et structur√©e. Tes r√©ponses sont concises et factuelles, "
                    "sans fioritures. Tu restes courtois mais direct."
                ),
                "tone": "formel, pr√©cis",
            },
            "playful": {
                "system_prompt": (
                    "Tu es BBIA, un assistant robotique joueur et amusant. "
                    "Tu communiques en fran√ßais de mani√®re d√©contract√©e, "
                    "humoristique et l√©g√®re. Tu utilises des expressions "
                    "naturelles et tu n'h√©sites pas √† faire des blagues "
                    "appropri√©es. Tes r√©ponses sont √©nergiques et positives."
                ),
                "tone": "d√©contract√©, humoristique",
            },
            "calm": {
                "system_prompt": (
                    "Tu es BBIA, un assistant robotique serein et apaisant. "
                    "Tu communiques en fran√ßais avec douceur et profondeur, "
                    "en prenant le temps n√©cessaire. Tes r√©ponses refl√®tent "
                    "une sagesse tranquille et une approche r√©fl√©chie."
                ),
                "tone": "doux, apaisant",
            },
            "enthusiastic": {
                "system_prompt": (
                    "Tu es BBIA, un assistant robotique plein d'enthousiasme "
                    "et d'√©nergie positive. Tu communiques en fran√ßais avec "
                    "joie et dynamisme. Tu transmets ta passion pour communiquer "
                    "et tu encourages l'interaction de mani√®re vivante et authentique."
                ),
                "tone": "√©nergique, positif",
            },
        }

        # OPTIMISATION RAM: Lazy loading strict - ne pas charger LLM √† l'init
        # Le LLM sera charg√© seulement au premier appel de chat()
        # self._load_llm()  # D√âSACTIV√â pour lazy loading strict

    def _load_llm(self) -> None:
        """Charge le LLM l√©ger (Phi-2 ou TinyLlama avec fallback).

        Essaie d'abord Phi-2, puis TinyLlama si √©chec.
        Gestion m√©moire optimis√©e (torch.float16, device_map="auto").
        """
        if not HF_AVAILABLE:
            logger.warning("Hugging Face non disponible - mode fallback activ√©")
            return

        # Essayer Phi-2 d'abord (recommand√©)
        try:
            logger.info("üì• Chargement Phi-2 (2.7B param√®tres)...")
            model_name = "microsoft/phi-2"
            self.llm_tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
            )  # nosec B615 - Mod√®le stable, revision pinning optionnel

            if self.llm_tokenizer is not None and self.llm_tokenizer.pad_token is None:
                self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token

            self.llm_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                dtype=torch.float16,  # R√©duire RAM (torch_dtype deprecated)
                device_map="auto",  # Distribution automatique
                trust_remote_code=True,
            )  # nosec B615 - Mod√®le stable, revision pinning optionnel
            logger.info("‚úÖ Phi-2 charg√© avec succ√®s")
            return

        except (
            ImportError,
            ModuleNotFoundError,
            RuntimeError,
            OSError,
            ValueError,
        ) as e:
            logger.warning("‚ö†Ô∏è Impossible de charger Phi-2: %s", e)
            logger.info("Tentative de chargement TinyLlama (fallback)...")
        except (TypeError, KeyError, AttributeError) as e:
            logger.warning("‚ö†Ô∏è Erreur inattendue chargement Phi-2: %s", e)
            logger.info("Tentative de chargement TinyLlama (fallback)...")
        except Exception as e:
            # Capturer toutes les autres exceptions (y compris ModuleNotFoundError imbriqu√©es)
            logger.warning("‚ö†Ô∏è Erreur inattendue chargement Phi-2: %s", e)
            logger.info("Tentative de chargement TinyLlama (fallback)...")

        # Fallback: TinyLlama (ultra-l√©ger)
        try:
            logger.info("üì• Chargement TinyLlama (1.1B param√®tres)...")
            model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
            self.llm_tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
            )  # nosec B615 - Mod√®le stable, revision pinning optionnel

            if self.llm_tokenizer is not None and self.llm_tokenizer.pad_token is None:
                self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token

            self.llm_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                dtype=torch.float16,  # R√©duire RAM (torch_dtype deprecated)
                device_map="auto",
                trust_remote_code=True,
            )  # nosec B615 - Mod√®le stable, revision pinning optionnel
            logger.info("‚úÖ TinyLlama charg√© avec succ√®s")

        except (
            ImportError,
            ModuleNotFoundError,
            RuntimeError,
            OSError,
            ValueError,
        ) as e:
            logger.warning("‚ùå Impossible de charger TinyLlama: %s", e)
            logger.warning("Mode fallback: r√©ponses basiques (sans LLM)")
            # R√©initialiser les attributs pour √©viter les √©tats incoh√©rents
            self.llm_model = None
            self.llm_tokenizer = None
        except Exception as e:
            logger.warning("‚ùå Erreur inattendue chargement TinyLlama: %s", e)
            logger.warning("Mode fallback: r√©ponses basiques (sans LLM)")
            # R√©initialiser les attributs pour √©viter les √©tats incoh√©rents
            self.llm_model = None
            self.llm_tokenizer = None

    def generate(
        self,
        prompt: str,
        max_length: int = 200,
        temperature: float = 0.7,
        timeout: float = 30.0,
    ) -> str:
        """G√©n√®re une r√©ponse avec le LLM (avec cache LRU).

        Args:
            prompt: Prompt utilisateur
            max_length: Longueur maximale de la r√©ponse (tokens)
            temperature: Temp√©rature pour g√©n√©ration (0.0-1.0)
            timeout: Timeout maximum (secondes)

        Returns:
            R√©ponse g√©n√©r√©e par le LLM, ou message d'erreur si √©chec

        """
        if not self.llm_model or not self.llm_tokenizer:
            return "D√©sol√©, le mod√®le LLM n'est pas disponible."

        # OPTIMISATION PERFORMANCE: V√©rifier cache LRU avant g√©n√©ration
        llm_cache = _get_global_llm_cache()
        cached_response = llm_cache.get(
            prompt, max_length=max_length, temperature=temperature
        )
        if cached_response is not None:
            logger.debug("‚ôªÔ∏è R√©ponse LLM r√©cup√©r√©e depuis cache")
            return cached_response

        try:
            # Valider longueur prompt (max 2000 tokens)
            inputs = self.llm_tokenizer(prompt, return_tensors="pt")
            if inputs.input_ids.shape[1] > 2000:
                logger.warning("Prompt trop long, tronqu√© √† 2000 tokens")
                inputs = self.llm_tokenizer(
                    prompt[:2000],
                    return_tensors="pt",
                    truncation=True,
                    max_length=2000,
                )

            # G√©n√©ration avec timeout
            start_time = time.time()
            try:
                with torch.no_grad():
                    outputs = self.llm_model.generate(
                        inputs.input_ids.to(self.llm_model.device),
                        max_length=max_length,
                        temperature=temperature,
                        do_sample=True,
                        pad_token_id=self.llm_tokenizer.eos_token_id,
                    )

                # V√©rifier timeout apr√®s g√©n√©ration
                elapsed_time = time.time() - start_time
                if elapsed_time > timeout:
                    logger.warning(
                        "G√©n√©ration LLM d√©pass√©e timeout (%.2fs > %.2fs)",
                        elapsed_time,
                        timeout,
                    )
                    return "D√©sol√©, la g√©n√©ration prend trop de temps."
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    logger.warning("M√©moire insuffisante pour g√©n√©ration LLM")
                    return "D√©sol√©, m√©moire insuffisante pour g√©n√©rer une r√©ponse."
                raise

            # D√©coder r√©ponse
            response: str = str(
                self.llm_tokenizer.decode(outputs[0], skip_special_tokens=True),
            )

            # Nettoyer r√©ponse (retirer prompt si pr√©sent)
            if prompt in response:
                response = response.replace(prompt, "").strip()

            # Sanitizer: retirer code ex√©cutable potentiel
            sanitized_response = self._sanitize_response(response)

            # OPTIMISATION PERFORMANCE: Mettre en cache la r√©ponse g√©n√©r√©e
            llm_cache.put(
                prompt,
                sanitized_response,
                max_length=max_length,
                temperature=temperature,
            )

            return sanitized_response

        except Exception:
            logger.exception("‚ùå Erreur g√©n√©ration LLM")
            return "D√©sol√©, une erreur s'est produite lors de la g√©n√©ration."

    def _sanitize_response(self, response: str) -> str:
        """Nettoie la r√©ponse LLM pour s√©curit√©.

        Retire code ex√©cutable potentiel et limite longueur.

        Args:
            response: R√©ponse brute du LLM

        Returns:
            R√©ponse nettoy√©e et s√©curis√©e

        """
        # Retirer blocs de code (```...```) - OPTIMISATION: regex compil√©e
        code_block_pattern = _get_compiled_regex(r"```[\s\S]*?```")
        response = code_block_pattern.sub("", response)
        # Retirer imports Python - OPTIMISATION: regex compil√©es
        import_pattern = _get_compiled_regex(r"^import\s+\w+", flags=re.MULTILINE)
        from_pattern = _get_compiled_regex(r"^from\s+\w+", flags=re.MULTILINE)
        response = import_pattern.sub("", response)
        response = from_pattern.sub("", response)
        # Limiter longueur - OPTIMISATION: constante de classe
        if len(response) > self.MAX_RESPONSE_LENGTH:
            response = response[: self.MAX_RESPONSE_LENGTH] + "..."
        return response.strip()

    def chat(self, user_message: str) -> str:
        """Chat intelligent avec contexte et actions robot.

        Args:
            user_message: Message utilisateur

        Returns:
            R√©ponse intelligente de BBIA

        """
        # OPTIMISATION RAM: Lazy loading strict - charger LLM seulement au premier appel
        if not self.llm_model or not self.llm_tokenizer:
            logger.info("üì• Chargement LLM √† la demande (lazy loading)...")
            self._load_llm()
            # Si √©chec chargement, continuer avec fallback
            if not self.llm_model or not self.llm_tokenizer:
                logger.warning("LLM non disponible, utilisation mode fallback")

        if not user_message or not user_message.strip():
            return "Je n'ai pas compris votre message. Pouvez-vous reformuler ?"

        # Valider input utilisateur - OPTIMISATION: constante de classe
        user_message = user_message.strip()[: self.MAX_USER_MESSAGE_LENGTH]

        try:
            # 1. D√©tecter actions robot ("tourne la t√™te", etc.)
            action = self._detect_action(user_message)
            action_confirmation = ""
            if action:
                self._execute_action(action)
                # Confirmer ex√©cution dans r√©ponse
                action_confirmation = f"J'ai ex√©cut√© l'action: {action['action']}. "

            # 2. Extraire et appliquer √©motions
            emotion = self._extract_emotion(user_message)
            if emotion:
                self._apply_emotion(emotion)

            # 3. Construire prompt avec contexte et personnalit√©
            prompt = self._build_context_prompt(user_message)

            # Ajouter confirmation action au prompt si pr√©sente
            if action_confirmation:
                prompt = prompt.replace(
                    f"Utilisateur: {user_message}\nBBIA: ",
                    f"Utilisateur: {user_message}\n{action_confirmation}BBIA: ",
                )

            # 4. G√©n√©rer r√©ponse avec LLM
            response = self.generate(prompt, max_length=200, temperature=0.7)

            # 5. Adapter r√©ponse selon pr√©f√©rences utilisateur
            response = self._adapt_to_preferences(response)

            # 6. Sauvegarder dans contexte
            self.context.append(
                {
                    "user": user_message,
                    "assistant": response,
                    "timestamp": time.time(),
                },
            )

            return response

        except Exception:
            logger.exception("‚ùå Erreur chat")
            return "Je ne comprends pas bien, peux-tu reformuler ?"

    def get_cache_stats(self) -> dict[str, Any]:
        """Retourne les statistiques du cache LLM.

        Returns:
            Dictionnaire avec hits, misses, hit_rate, size, etc.
        """
        return _get_global_llm_cache().get_stats()

    def _build_context_prompt(self, user_message: str) -> str:
        """Construit prompt avec contexte historique et personnalit√©.

        Args:
            user_message: Message utilisateur

        Returns:
            Prompt complet avec contexte

        """
        # Prompt syst√®me avec personnalit√©
        personality_config = self.PERSONALITIES.get(
            self.personality,
            self.PERSONALITIES["friendly"],
        )
        system_prompt = f"{personality_config['system_prompt']}\n\n"

        # Ajouter historique (5 derniers messages)
        context_text = ""
        if self.context:
            for entry in list(self.context)[-5:]:
                context_text += f"Utilisateur: {entry.get('user', '')}\n"
                context_text += f"BBIA: {entry.get('assistant', '')}\n\n"

        # Message actuel
        return f"{system_prompt}{context_text}Utilisateur: {user_message}\nBBIA: "

    def _detect_action(self, user_message: str) -> dict[str, Any] | None:
        """D√©tecte action robot dans message utilisateur.

        Args:
            user_message: Message utilisateur

        Returns:
            Dictionnaire avec action d√©tect√©e, ou None

        """
        # Patterns de d√©tection (regex)
        patterns = {
            "look_right": r"(tourne|regarde|dirige).*(droite|right)",
            "look_left": r"(tourne|regarde|dirige).*(gauche|left)",
            "look_up": r"(tourne|regarde|dirige).*(haut|up)",
            "look_down": r"(tourne|regarde|dirige).*(bas|down)",
            "wake_up": r"(r√©veille|wake|allume)",
            "sleep": r"(endors|sleep|√©teins)",
        }

        for action, pattern in patterns.items():
            if re.search(pattern, user_message, re.IGNORECASE):
                return {"action": action, "confidence": 0.9}

        return None

    def _execute_action(self, action: dict[str, Any]) -> None:
        """Ex√©cute action robot via robot_api.

        Args:
            action: Dictionnaire avec action √† ex√©cuter

        """
        if not self.robot_api:
            logger.debug("robot_api non disponible - action non ex√©cut√©e")
            return

        action_name = action.get("action")
        if not action_name:
            return

        try:
            # Importer create_head_pose du SDK officiel
            try:
                from reachy_mini.utils import (  # type: ignore[import-untyped]
                    create_head_pose,
                )
            except ImportError:
                logger.warning("SDK officiel non disponible - action non ex√©cut√©e")
                return

            # Ex√©cuter action selon type
            if action_name == "look_right":
                pose = create_head_pose(yaw=0.3, pitch=0.0, degrees=False)
                if hasattr(self.robot_api, "goto_target"):
                    self.robot_api.goto_target(head=pose, duration=1.0)
                logger.info("‚úÖ Action ex√©cut√©e: look_right")

            elif action_name == "look_left":
                pose = create_head_pose(yaw=-0.3, pitch=0.0, degrees=False)
                if hasattr(self.robot_api, "goto_target"):
                    self.robot_api.goto_target(head=pose, duration=1.0)
                logger.info("‚úÖ Action ex√©cut√©e: look_left")

            elif action_name == "look_up":
                pose = create_head_pose(yaw=0.0, pitch=0.3, degrees=False)
                if hasattr(self.robot_api, "goto_target"):
                    self.robot_api.goto_target(head=pose, duration=1.0)
                logger.info("‚úÖ Action ex√©cut√©e: look_up")

            elif action_name == "look_down":
                pose = create_head_pose(yaw=0.0, pitch=-0.3, degrees=False)
                if hasattr(self.robot_api, "goto_target"):
                    self.robot_api.goto_target(head=pose, duration=1.0)
                logger.info("‚úÖ Action ex√©cut√©e: look_down")

            elif action_name == "wake_up":
                # R√©veiller robot (position neutre)
                pose = create_head_pose(yaw=0.0, pitch=0.0, degrees=False)
                if hasattr(self.robot_api, "goto_target"):
                    self.robot_api.goto_target(head=pose, duration=1.0)
                logger.info("‚úÖ Action ex√©cut√©e: wake_up")

            elif action_name == "sleep":
                # Endormir robot avec pose sommeil naturelle (Issue #410)
                if hasattr(self.robot_api, "set_sleeping_pose"):
                    self.robot_api.set_sleeping_pose(duration=2.0)
                else:
                    # Fallback: pose sommeil simplifi√©e
                    pose = create_head_pose(yaw=0.0, pitch=-0.25, degrees=False)
                    if hasattr(self.robot_api, "goto_target"):
                        self.robot_api.goto_target(head=pose, duration=2.0)
                logger.info("‚úÖ Action ex√©cut√©e: sleep (pose sommeil am√©lior√©e)")

        except Exception:
            logger.exception("‚ùå Erreur ex√©cution action %s:", action_name)

    def _extract_emotion(self, user_message: str) -> str | None:
        """Extrait √©motion du message utilisateur.

        Args:
            user_message: Message utilisateur

        Returns:
            Nom de l'√©motion d√©tect√©e, ou None

        """
        emotion_keywords = {
            "happy": ["content", "heureux", "joyeux", "sourire", "super", "g√©nial"],
            "sad": ["triste", "malheureux", "d√©prim√©", "d√©√ßu"],
            "angry": ["√©nerv√©", "f√¢ch√©", "col√®re", "frustr√©"],
            "excited": ["excit√©", "enthousiaste", "impatient", "impatience"],
            "curious": ["curieux", "int√©ress√©", "question"],
            "surprised": ["surpris", "√©tonn√©", "choqu√©"],
            "fearful": ["peur", "craintif", "inquiet", "anxieux"],
            "calm": ["calme", "serein", "paisible", "tranquille"],
        }

        user_lower = user_message.lower()
        for emotion, keywords in emotion_keywords.items():
            if any(kw in user_lower for kw in keywords):
                return emotion

        return None

    def _apply_emotion(self, emotion: str) -> None:
        """Applique √©motion au robot via BBIAEmotions.

        Args:
            emotion: Nom de l'√©motion √† appliquer

        """
        if not self.robot_api:
            logger.debug("robot_api non disponible - √©motion non appliqu√©e")
            return

        try:
            from .bbia_emotions import BBIAEmotions

            emotions_module = BBIAEmotions()
            emotions_module.set_emotion(emotion, intensity=0.7)

            # Appliquer aussi via robot_api si disponible
            if hasattr(self.robot_api, "set_emotion"):
                self.robot_api.set_emotion(emotion, 0.7)

            logger.info("‚úÖ √âmotion appliqu√©e: %s", emotion)

        except ImportError:
            logger.warning("BBIAEmotions non disponible - √©motion non appliqu√©e")
        except Exception:
            logger.exception("‚ùå Erreur application √©motion %s:", emotion)

    def set_personality(self, personality: str) -> None:
        """Change la personnalit√© du chat.

        Args:
            personality: Nom de la personnalit√©
                (friendly, professional, playful, calm, enthusiastic)

        """
        if personality in self.PERSONALITIES:
            self.personality = personality
            logger.info("‚úÖ Personnalit√© chang√©e: %s", personality)
        else:
            # Log en debug en CI (warning attendu dans les tests)
            import os

            if os.environ.get("CI", "false").lower() == "true":
                logger.debug(
                    f"Personnalit√© invalide: {personality}. "
                    f"Personnalit√©s disponibles: {list(self.PERSONALITIES.keys())}",
                )
            else:
                logger.warning(
                    f"‚ö†Ô∏è Personnalit√© invalide: {personality}. "
                    f"Personnalit√©s disponibles: {list(self.PERSONALITIES.keys())}",
                )

    def learn_preference(self, user_action: str, context: dict[str, Any]) -> None:
        """Apprend pr√©f√©rence utilisateur.

        Args:
            user_action: Action ou pr√©f√©rence exprim√©e par l'utilisateur
            context: Contexte de la pr√©f√©rence

        """
        user_lower = user_action.lower()

        # D√©tecter pr√©f√©rences courantes
        if "court" in user_lower or "bref" in user_lower:
            self.user_preferences["response_length"] = "short"
        elif "d√©taill√©" in user_lower or "long" in user_lower:
            self.user_preferences["response_length"] = "long"

        if "formel" in user_lower or "professionnel" in user_lower:
            self.user_preferences["tone"] = "formal"
        elif "d√©contract√©" in user_lower or "casual" in user_lower:
            self.user_preferences["tone"] = "casual"

        # Fusionner avec contexte
        self.user_preferences.update(context)

        # Sauvegarder pr√©f√©rences
        self._save_preferences()

        logger.info("‚úÖ Pr√©f√©rence apprise: %s", self.user_preferences)

    def _adapt_to_preferences(self, response: str) -> str:
        """Adapte r√©ponse selon pr√©f√©rences utilisateur.

        Args:
            response: R√©ponse brute du LLM

        Returns:
            R√©ponse adapt√©e selon pr√©f√©rences

        """
        # Adapter longueur
        if self.user_preferences.get("response_length") == "short":
            sentences = response.split(".")
            if len(sentences) > 2:
                response = ". ".join(sentences[:2]) + "."
        elif self.user_preferences.get("response_length") == "long":
            # Garder r√©ponse compl√®te (pas de modification n√©cessaire)
            pass

        # Adapter ton (d√©j√† g√©r√© par personnalit√©, mais peut √™tre affin√©)
        if self.user_preferences.get("tone") == "formal":
            # Remplacer expressions d√©contract√©es
            response = response.replace("salut", "bonjour")
            response = response.replace("√ßa", "cela")
        elif self.user_preferences.get("tone") == "casual":
            # Ton d√©contract√© d√©j√† g√©r√© par personnalit√©
            # (pas de modification n√©cessaire)
            pass

        return response

    def _save_preferences(self) -> None:
        """Sauvegarde pr√©f√©rences utilisateur dans fichier JSON."""
        try:
            # Cr√©er r√©pertoire si n√©cessaire
            prefs_dir = Path(self.preferences_file).parent
            if prefs_dir and not prefs_dir.exists():
                prefs_dir.mkdir(parents=True, exist_ok=True)

            # Sauvegarder
            with open(self.preferences_file, "w", encoding="utf-8") as f:
                json.dump(self.user_preferences, f, indent=2, ensure_ascii=False)

            logger.debug("‚úÖ Pr√©f√©rences sauvegard√©es: %s", self.preferences_file)

        except (OSError, TypeError, ValueError) as e:
            logger.warning("‚ö†Ô∏è Erreur sauvegarde pr√©f√©rences: %s", e)

    def _load_preferences(self) -> None:
        """Charge pr√©f√©rences utilisateur depuis fichier JSON."""
        try:
            if Path(self.preferences_file).exists():
                with open(self.preferences_file, encoding="utf-8") as f:
                    loaded_prefs = json.load(f)
                    # Fusionner avec pr√©f√©rences existantes (ne pas √©craser)
                    if isinstance(loaded_prefs, dict):
                        self.user_preferences.update(loaded_prefs)
                logger.debug("‚úÖ Pr√©f√©rences charg√©es: %s", self.preferences_file)

        except (OSError, json.JSONDecodeError, KeyError) as e:
            logger.debug(
                "Pr√©f√©rences non charg√©es (normal si premi√®re utilisation): %s",
                e,
            )
