#!/usr/bin/env python3
"""
Audit complet des fichiers Markdown - 26 Janvier 2026
VÃ©rifie:
1. Dates obsolÃ¨tes (doivent Ãªtre mises Ã  jour Ã  26 Janvier 2026 si pertinentes)
2. Versions cohÃ©rentes (1.4.0)
3. Redondances entre fichiers
4. Fichiers obsolÃ¨tes Ã  archiver
5. Liens brisÃ©s
6. CohÃ©rence des informations
"""

import re
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List, Set, Tuple
from datetime import datetime

# Configuration
CURRENT_DATE = "26 Janvier 2026"
CURRENT_VERSION = "1.4.0"
ROOT_DIR = Path(__file__).parent.parent

# Patterns de dates Ã  vÃ©rifier
DATE_PATTERNS = [
    r"DerniÃ¨re mise Ã  jour.*?(\d{1,2}\s+\w+\s+\d{4})",
    r"derniÃ¨re mise Ã  jour.*?(\d{1,2}\s+\w+\s+\d{4})",
    r"mise Ã  jour.*?(\d{1,2}\s+\w+\s+\d{4})",
    r"Mise Ã  jour.*?(\d{1,2}\s+\w+\s+\d{4})",
]

# Dates qui doivent Ãªtre mises Ã  jour (avant janvier 2026)
OBSOLETE_DATES = [
    "8 DÃ©cembre 2025",
    "15 DÃ©cembre 2025",
    "22 DÃ©cembre 2025",
    "DÃ©cembre 2025",
    "Novembre 2025",
    "Octobre 2025",
]

# Dates qui sont correctes (historiques ou spÃ©cifiques)
CORRECT_DATES = [
    "18 DÃ©cembre 2025",  # RÃ©ception robot
    "20 DÃ©cembre 2025",  # Montage robot
    "17 Janvier 2026",   # RÃ©ception moteurs
    "21 Janvier 2026",   # VÃ©rification QC
    "26 Janvier 2026",   # Date actuelle
    "20 Janvier 2026",   # Analyse repo
]


def find_all_md_files(root_dir: Path) -> List[Path]:
    """Trouve tous les fichiers MD."""
    md_files = []
    for path in root_dir.rglob("*.md"):
        # Ignorer venv, .git, fichiers cachÃ©s, archives
        parts = path.parts
        if any(
            part.startswith(".")
            or part == "venv"
            or part == "__pycache__"
            or "_archive" in part
            or "_archived" in part
            for part in parts
        ):
            continue
        if path.name.startswith("._"):
            continue
        md_files.append(path)
    return sorted(md_files)


def extract_dates_from_file(file_path: Path) -> List[Tuple[str, int]]:
    """Extrait toutes les dates d'un fichier."""
    dates = []
    try:
        content = file_path.read_text(encoding="utf-8")
        for pattern in DATE_PATTERNS:
            for match in re.finditer(pattern, content, re.IGNORECASE):
                date_str = match.group(1)
                line_num = content[: match.start()].count("\n") + 1
                dates.append((date_str, line_num))
    except Exception as e:
        print(f"âš ï¸  Erreur lecture {file_path}: {e}")
    return dates


def check_version_consistency(file_path: Path) -> List[Tuple[str, int]]:
    """VÃ©rifie la cohÃ©rence des versions."""
    issues = []
    try:
        content = file_path.read_text(encoding="utf-8")
        lines = content.split("\n")
        
        # Chercher toutes les mentions de version
        version_pattern = r"version\s*[:=]\s*([0-9]+\.[0-9]+\.[0-9]+)"
        for i, line in enumerate(lines, 1):
            matches = re.finditer(version_pattern, line, re.IGNORECASE)
            for match in matches:
                version = match.group(1)
                if version != CURRENT_VERSION and version not in ["1.3.0", "1.3.1", "1.3.2"]:
                    # Accepter les versions historiques dans CHANGELOG/RELEASE_NOTES
                    if "CHANGELOG" not in file_path.name and "RELEASE_NOTES" not in file_path.name:
                        issues.append((f"Version {version} trouvÃ©e (attendu {CURRENT_VERSION})", i))
    except Exception as e:
        print(f"âš ï¸  Erreur vÃ©rification version {file_path}: {e}")
    return issues


def find_duplicate_content(files: List[Path]) -> Dict[str, List[Path]]:
    """Trouve les contenus dupliquÃ©s entre fichiers."""
    content_hash = defaultdict(list)
    
    for file_path in files:
        try:
            content = file_path.read_text(encoding="utf-8")
            # Normaliser le contenu (supprimer espaces, dates variables)
            normalized = re.sub(r"\d{1,2}\s+\w+\s+\d{4}", "DATE", content)
            normalized = re.sub(r"\s+", " ", normalized)
            # Prendre les 500 premiers caractÃ¨res comme signature
            signature = normalized[:500]
            if len(signature) > 100:  # Ignorer les fichiers trop courts
                content_hash[signature].append(file_path)
        except Exception:
            pass
    
    # Retourner seulement les duplications
    return {sig: paths for sig, paths in content_hash.items() if len(paths) > 1}


def check_links(file_path: Path) -> List[Tuple[str, int]]:
    """VÃ©rifie les liens internes."""
    broken_links = []
    try:
        content = file_path.read_text(encoding="utf-8")
        lines = content.split("\n")
        
        # Pattern pour liens markdown
        link_pattern = r"\[([^\]]+)\]\(([^)]+)\)"
        
        for i, line in enumerate(lines, 1):
            for match in re.finditer(link_pattern, line):
                link_text = match.group(1)
                link_path = match.group(2)
                
                # Ignorer les liens externes
                if link_path.startswith("http"):
                    continue
                
                # Ignorer les ancres
                if link_path.startswith("#"):
                    continue
                
                # RÃ©soudre le chemin relatif
                if link_path.startswith("../") or link_path.startswith("./"):
                    target = (file_path.parent / link_path).resolve()
                else:
                    target = (file_path.parent / link_path).resolve()
                
                # VÃ©rifier si le fichier existe
                if not target.exists():
                    broken_links.append((f"Lien brisÃ©: {link_path}", i))
    except Exception as e:
        print(f"âš ï¸  Erreur vÃ©rification liens {file_path}: {e}")
    return broken_links


def audit_file(file_path: Path) -> Dict[str, Any]:
    """Audit complet d'un fichier."""
    issues = {
        "obsolete_dates": [],
        "version_issues": [],
        "broken_links": [],
    }
    
    # VÃ©rifier les dates
    dates = extract_dates_from_file(file_path)
    for date_str, line_num in dates:
        # VÃ©rifier si la date est obsolÃ¨te
        is_obsolete = False
        for obsolete in OBSOLETE_DATES:
            if obsolete.lower() in date_str.lower():
                is_obsolete = True
                break
        
        if is_obsolete:
            # VÃ©rifier si c'est une date historique correcte
            is_correct_historical = False
            for correct in CORRECT_DATES:
                if correct.lower() in date_str.lower():
                    is_correct_historical = True
                    break
            
            if not is_correct_historical:
                issues["obsolete_dates"].append((date_str, line_num))
    
    # VÃ©rifier les versions
    issues["version_issues"] = check_version_consistency(file_path)
    
    # VÃ©rifier les liens
    issues["broken_links"] = check_links(file_path)
    
    return issues


def main():
    """Fonction principale."""
    print("ğŸ” Audit complet des fichiers Markdown - 26 Janvier 2026\n")
    
    md_files = find_all_md_files(ROOT_DIR)
    print(f"ğŸ“ {len(md_files)} fichiers MD trouvÃ©s\n")
    
    all_issues = defaultdict(list)
    files_with_issues = []
    
    # Auditer chaque fichier
    for file_path in md_files:
        relative_path = file_path.relative_to(ROOT_DIR)
        issues = audit_file(file_path)
        
        total_issues = (
            len(issues["obsolete_dates"])
            + len(issues["version_issues"])
            + len(issues["broken_links"])
        )
        
        if total_issues > 0:
            files_with_issues.append((relative_path, issues))
            all_issues["obsolete_dates"].extend(
                [(relative_path, date, line) for date, line in issues["obsolete_dates"]]
            )
            all_issues["version_issues"].extend(
                [(relative_path, msg, line) for msg, line in issues["version_issues"]]
            )
            all_issues["broken_links"].extend(
                [(relative_path, msg, line) for msg, line in issues["broken_links"]]
            )
    
    # Afficher le rÃ©sumÃ©
    print("=" * 80)
    print("ğŸ“Š RÃ‰SUMÃ‰ DE L'AUDIT")
    print("=" * 80)
    print(f"\nâœ… Fichiers auditÃ©s : {len(md_files)}")
    print(f"âš ï¸  Fichiers avec problÃ¨mes : {len(files_with_issues)}")
    print(f"\nğŸ“… Dates obsolÃ¨tes : {len(all_issues['obsolete_dates'])}")
    print(f"ğŸ”¢ ProblÃ¨mes de version : {len(all_issues['version_issues'])}")
    print(f"ğŸ”— Liens brisÃ©s : {len(all_issues['broken_links'])}")
    
    # Afficher les dÃ©tails
    if all_issues["obsolete_dates"]:
        print("\n" + "=" * 80)
        print("ğŸ“… DATES OBSOLÃˆTES Ã€ METTRE Ã€ JOUR")
        print("=" * 80)
        for file_path, date, line in all_issues["obsolete_dates"][:20]:  # Limiter Ã  20
            print(f"  {file_path}:{line} - Date: {date}")
        if len(all_issues["obsolete_dates"]) > 20:
            print(f"  ... et {len(all_issues['obsolete_dates']) - 20} autres")
    
    if all_issues["version_issues"]:
        print("\n" + "=" * 80)
        print("ğŸ”¢ PROBLÃˆMES DE VERSION")
        print("=" * 80)
        for file_path, msg, line in all_issues["version_issues"][:10]:
            print(f"  {file_path}:{line} - {msg}")
    
    if all_issues["broken_links"]:
        print("\n" + "=" * 80)
        print("ğŸ”— LIENS BRISÃ‰S")
        print("=" * 80)
        for file_path, msg, line in all_issues["broken_links"][:10]:
            print(f"  {file_path}:{line} - {msg}")
    
    # Chercher les duplications
    print("\n" + "=" * 80)
    print("ğŸ”„ RECHERCHE DE CONTENUS DUPLIQUÃ‰S")
    print("=" * 80)
    duplicates = find_duplicate_content(md_files)
    if duplicates:
        print(f"âš ï¸  {len(duplicates)} groupes de fichiers avec contenu similaire trouvÃ©s")
        for sig, paths in list(duplicates.items())[:5]:  # Limiter Ã  5
            print(f"\n  Fichiers similaires ({len(paths)}):")
            for path in paths:
                print(f"    - {path.relative_to(ROOT_DIR)}")
    else:
        print("âœ… Aucune duplication majeure dÃ©tectÃ©e")
    
    print("\n" + "=" * 80)
    print("âœ… Audit terminÃ©")
    print("=" * 80)
    
    # Sauvegarder le rapport
    report_path = ROOT_DIR / "artifacts" / "audit_md_janvier2026.json"
    report_path.parent.mkdir(parents=True, exist_ok=True)
    
    import json
    report = {
        "date_audit": CURRENT_DATE,
        "total_files": len(md_files),
        "files_with_issues": len(files_with_issues),
        "issues": {
            "obsolete_dates": len(all_issues["obsolete_dates"]),
            "version_issues": len(all_issues["version_issues"]),
            "broken_links": len(all_issues["broken_links"]),
        },
        "details": {
            "obsolete_dates": [
                {"file": str(f), "date": d, "line": l}
                for f, d, l in all_issues["obsolete_dates"]
            ],
            "version_issues": [
                {"file": str(f), "message": m, "line": l}
                for f, m, l in all_issues["version_issues"]
            ],
            "broken_links": [
                {"file": str(f), "message": m, "line": l}
                for f, m, l in all_issues["broken_links"]
            ],
        },
    }
    
    report_path.write_text(json.dumps(report, indent=2, ensure_ascii=False))
    print(f"\nğŸ“„ Rapport sauvegardÃ© : {report_path}")


if __name__ == "__main__":
    main()
