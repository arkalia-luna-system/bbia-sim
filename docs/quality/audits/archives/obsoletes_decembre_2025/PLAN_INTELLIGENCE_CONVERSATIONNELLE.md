# üß† PLAN D√âTAILL√â : Intelligence Conversationnelle

**Date** : Novembre 2024  
**Priorit√©** : üî¥ **HAUTE**  
**Dur√©e estim√©e** : 6 semaines  
**Objectif** : Transformer BBIA en v√©ritable assistant conversationnel intelligent

---

## üéØ Objectif

Remplacer le syst√®me de r√®gles basiques actuel par un **vrai LLM conversationnel** qui :
- ‚úÖ Comprend le contexte
- ‚úÖ G√©n√®re des r√©ponses intelligentes
- ‚úÖ Int√®gre les √©motions BBIA
- ‚úÖ Ex√©cute des actions robot via conversation
- ‚úÖ Apprend les pr√©f√©rences utilisateur

---

## üìä √âTAT ACTUEL

### ‚ùå Probl√®mes Identifi√©s

**Fichier actuel :** `src/bbia_sim/bbia_huggingface.py`

**Probl√®me 1 : R√®gles basiques**
```python
# Actuellement (lignes ~200-300)
def chat(self, user_message: str) -> str:
    sentiment = self.analyze_sentiment(user_message)
    # ‚ùå G√©n√©ration r√©ponse bas√©e sur r√®gles simples
    response = self._generate_response_from_sentiment(sentiment)
    return response
```

**Probl√®me 2 : Pas de contexte**
- ‚ùå Pas d'historique conversation
- ‚ùå Pas de compr√©hension r√©f√©rences ("il", "√ßa")
- ‚ùå R√©ponses isol√©es, pas de continuit√©

**Probl√®me 3 : Pas d'actions robot**
- ‚ùå Ne peut pas ex√©cuter actions via conversation
- ‚ùå Pas d'int√©gration avec `robot_api`

---

## üöÄ SOLUTION : Int√©gration LLM L√©ger

### Choix du Mod√®le

#### Option 1 : Phi-2 2.7B (Microsoft) ‚≠ê **RECOMMAND√â**

**Avantages :**
- ‚úÖ **L√©ger** : 2.7B param√®tres (~5GB RAM)
- ‚úÖ **Performant** : Qualit√© conversationnelle excellente
- ‚úÖ **Compatible RPi 5** : Fonctionne avec 8GB RAM
- ‚úÖ **Open source** : Apache 2.0
- ‚úÖ **Multilingue** : Support fran√ßais

**Mod√®le :** `microsoft/phi-2`

**Installation :**
```bash
pip install transformers accelerate bitsandbytes
```

**Code :**
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/phi-2",
    torch_dtype=torch.float16,  # R√©duire RAM
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-2")
```

---

#### Option 2 : TinyLlama 1.1B (Alternative)

**Avantages :**
- ‚úÖ **Ultra-l√©ger** : 1.1B param√®tres (~2GB RAM)
- ‚úÖ **Rapide** : Latence <1s
- ‚ö†Ô∏è **Qualit√© moindre** : Moins bon que Phi-2

**Mod√®le :** `TinyLlama/TinyLlama-1.1B-Chat-v1.0`

---

#### Option 3 : Hugging Face Inference API (Fallback)

**Avantages :**
- ‚úÖ **Aucune RAM locale** : API externe
- ‚úÖ **Gratuit** : Jusqu'√† 1000 requ√™tes/mois
- ‚ö†Ô∏è **Latence r√©seau** : ~500ms-1s
- ‚ö†Ô∏è **D√©pendance internet** : N√©cessite connexion

**Mod√®le :** `mistralai/Mistral-7B-Instruct-v0.2` (via API)

---

### Architecture Cible

```python
# src/bbia_sim/bbia_chat.py (NOUVEAU)
class BBIAChat:
    """Module conversationnel intelligent avec LLM."""
    
    def __init__(self, robot_api: RobotAPI | None = None):
        self.llm = self._load_llm()  # Phi-2 ou TinyLlama
        self.robot_api = robot_api
        self.context = deque(maxlen=10)  # Historique 10 messages
        self.personality = "friendly"  # Par d√©faut
        self.user_preferences = {}  # Apprentissage
    
    def chat(self, user_message: str) -> str:
        # 1. Analyser sentiment (existant)
        sentiment = self._analyze_sentiment(user_message)
        
        # 2. D√©tecter actions robot ("tourne la t√™te")
        action = self._detect_action(user_message)
        if action:
            self._execute_action(action)
        
        # 3. G√©n√©rer r√©ponse avec LLM
        response = self._generate_with_llm(user_message, sentiment)
        
        # 4. Int√©grer √©motions BBIA
        emotion = self._extract_emotion(user_message)
        if emotion:
            self._apply_emotion(emotion)
        
        # 5. Sauvegarder contexte
        self.context.append({"user": user_message, "assistant": response})
        
        return response
```

---

## üìã PHASES D'IMPL√âMENTATION

### Phase 1 : Int√©gration LLM de Base (Semaine 1-2)

#### T√¢che 1.1 : Cr√©er Module Chat

**Fichier :** `src/bbia_sim/bbia_chat.py`

**Fonctionnalit√©s :**
- ‚úÖ Chargement Phi-2 ou TinyLlama
- ‚úÖ G√©n√©ration r√©ponse basique
- ‚úÖ Gestion m√©moire (RAM optimis√©e)

**Code cible :**
```python
class BBIAChat:
    def __init__(self):
        self.llm_model = None
        self.llm_tokenizer = None
        self._load_llm()
    
    def _load_llm(self):
        """Charge le LLM l√©ger."""
        try:
            # Essayer Phi-2 d'abord
            model_name = "microsoft/phi-2"
            self.llm_tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.llm_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
        except Exception as e:
            logger.warning(f"Impossible de charger Phi-2: {e}")
            # Fallback TinyLlama
            model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
            # ...
    
    def generate(self, prompt: str, max_length: int = 200) -> str:
        """G√©n√®re une r√©ponse avec le LLM."""
        inputs = self.llm_tokenizer(prompt, return_tensors="pt")
        outputs = self.llm_model.generate(
            inputs.input_ids,
            max_length=max_length,
            temperature=0.7,
            do_sample=True
        )
        response = self.llm_tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response
```

---

#### T√¢che 1.2 : Remplacer R√®gles dans `bbia_huggingface.py`

**Modification :**
```python
# src/bbia_sim/bbia_huggingface.py
from .bbia_chat import BBIAChat

class BBIAHuggingFace:
    def __init__(self, tools=None):
        # ...
        # R√©cup√©rer robot_api depuis tools si disponible
        robot_api = None
        if tools and hasattr(tools, "robot_api"):
            robot_api = tools.robot_api
        
        # Initialiser BBIAChat avec robot_api
        self.bbia_chat = BBIAChat(robot_api=robot_api)
    
    def chat(self, user_message: str) -> str:
        # ‚úÖ Utiliser BBIAChat (LLM l√©ger) en priorit√©
        if self.bbia_chat and self.bbia_chat.llm_model:
            return self.bbia_chat.chat(user_message)
        # Fallback vers r√©ponses enrichies si LLM indisponible
        ...
```

---

#### T√¢che 1.3 : Tests Basiques

**Fichier :** `tests/test_bbia_chat_llm.py`

**Tests :**
- ‚úÖ Test chargement mod√®le
- ‚úÖ Test g√©n√©ration r√©ponse
- ‚úÖ Test m√©moire RAM
- ‚úÖ Test latence (<2s)

---

### Phase 2 : Compr√©hension Contextuelle (Semaine 3-4)

#### T√¢che 2.1 : Historique Conversation

**Fonctionnalit√©s :**
- ‚úÖ Stocker 10 derniers messages
- ‚úÖ Inclure contexte dans prompt LLM
- ‚úÖ Gestion r√©f√©rences ("il", "√ßa", "celui-l√†")

**Code :**
```python
def chat(self, user_message: str) -> str:
    # Construire prompt avec contexte
    context_prompt = self._build_context_prompt(user_message)
    
    # G√©n√©rer avec contexte
    response = self._generate_with_llm(context_prompt)
    
    # Sauvegarder dans historique
    self.context.append({
        "user": user_message,
        "assistant": response,
        "timestamp": time.time()
    })
    
    return response

def _build_context_prompt(self, user_message: str) -> str:
    """Construit prompt avec contexte."""
    prompt = "Tu es BBIA, un assistant robotique intelligent.\n\n"
    
    # Ajouter historique
    for entry in list(self.context)[-5:]:  # 5 derniers messages
        prompt += f"Utilisateur: {entry['user']}\n"
        prompt += f"BBIA: {entry['assistant']}\n\n"
    
    # Message actuel
    prompt += f"Utilisateur: {user_message}\n"
    prompt += "BBIA: "
    
    return prompt
```

---

#### T√¢che 2.2 : D√©tection Actions Robot

**Fonctionnalit√©s :**
- ‚úÖ D√©tecter commandes ("tourne la t√™te", "regarde √† droite")
- ‚úÖ Ex√©cuter actions via `robot_api`
- ‚úÖ Confirmer ex√©cution dans r√©ponse

**Code :**
```python
def _detect_action(self, user_message: str) -> dict | None:
    """D√©tecte action robot dans message."""
    # Patterns de d√©tection
    patterns = {
        "look_right": r"(tourne|regarde|dirige).*(droite|right)",
        "look_left": r"(tourne|regarde|dirige).*(gauche|left)",
        "look_up": r"(tourne|regarde|dirige).*(haut|up)",
        "look_down": r"(tourne|regarde|dirige).*(bas|down)",
        "wake_up": r"(r√©veille|wake|allume)",
        "sleep": r"(endors|sleep|√©teins)",
    }
    
    for action, pattern in patterns.items():
        if re.search(pattern, user_message, re.IGNORECASE):
            return {"action": action, "confidence": 0.9}
    
    return None

def _execute_action(self, action: dict):
    """Ex√©cute action robot."""
    if not self.robot_api:
        return
    
    action_name = action["action"]
    
    if action_name == "look_right":
        pose = create_head_pose(yaw=0.3)
        self.robot_api.goto_target(head=pose, duration=1.0)
    elif action_name == "look_left":
        pose = create_head_pose(yaw=-0.3)
        self.robot_api.goto_target(head=pose, duration=1.0)
    # ... autres actions
```

---

#### T√¢che 2.3 : Int√©gration √âmotions

**Fonctionnalit√©s :**
- ‚úÖ D√©tecter √©motions dans message utilisateur
- ‚úÖ Appliquer √©motion correspondante au robot
- ‚úÖ R√©pondre avec √©motion appropri√©e

**Code :**
```python
def _extract_emotion(self, user_message: str) -> str | None:
    """Extrait √©motion du message."""
    emotion_keywords = {
        "happy": ["content", "heureux", "joyeux", "sourire"],
        "sad": ["triste", "malheureux", "d√©prim√©"],
        "angry": ["√©nerv√©", "f√¢ch√©", "col√®re"],
        "excited": ["excit√©", "enthousiaste", "impatient"],
    }
    
    for emotion, keywords in emotion_keywords.items():
        if any(kw in user_message.lower() for kw in keywords):
            return emotion
    
    return None

def _apply_emotion(self, emotion: str):
    """Applique √©motion au robot."""
    if not self.robot_api:
        return
    
    # Utiliser BBIAEmotions existant
    from .bbia_emotions import BBIAEmotions
    emotions_module = BBIAEmotions()
    emotions_module.set_emotion(emotion, intensity=0.7)
```

---

### Phase 3 : Personnalit√©s Avanc√©es (Semaine 5-6)

#### T√¢che 3.1 : Syst√®me Personnalit√©s

**Personnalit√©s :**
1. **Friendly** (amical) - D√©faut
2. **Professional** (professionnel)
3. **Playful** (joueur)
4. **Calm** (calme)
5. **Enthusiastic** (enthousiaste)

**Code :**
```python
PERSONALITIES = {
    "friendly": {
        "system_prompt": "Tu es BBIA, un assistant robotique amical et chaleureux.",
        "tone": "chaleureux, empathique",
    },
    "professional": {
        "system_prompt": "Tu es BBIA, un assistant robotique professionnel et efficace.",
        "tone": "formel, pr√©cis",
    },
    "playful": {
        "system_prompt": "Tu es BBIA, un assistant robotique joueur et amusant.",
        "tone": "d√©contract√©, humoristique",
    },
    # ...
}

def set_personality(self, personality: str):
    """Change la personnalit√©."""
    if personality in PERSONALITIES:
        self.personality = personality
        self._update_system_prompt()
```

---

#### T√¢che 3.2 : Apprentissage Pr√©f√©rences

**Fonctionnalit√©s :**
- ‚úÖ Apprendre pr√©f√©rences utilisateur
- ‚úÖ Adapter style selon contexte
- ‚úÖ Sauvegarder pr√©f√©rences (JSON)

**Code :**
```python
def learn_preference(self, user_action: str, context: dict):
    """Apprend pr√©f√©rence utilisateur."""
    # Exemple : utilisateur pr√©f√®re r√©ponses courtes
    if "court" in user_action.lower():
        self.user_preferences["response_length"] = "short"
    
    # Sauvegarder
    self._save_preferences()

def _adapt_to_preferences(self, response: str) -> str:
    """Adapte r√©ponse selon pr√©f√©rences."""
    if self.user_preferences.get("response_length") == "short":
        # Raccourcir r√©ponse
        sentences = response.split(".")
        return ". ".join(sentences[:2]) + "."
    
    return response
```

---

## üß™ TESTS

### Tests Unitaires

**Fichier :** `tests/test_bbia_chat_llm.py`

```python
def test_llm_loading():
    """Test chargement LLM."""
    chat = BBIAChat()
    assert chat.llm_model is not None
    assert chat.llm_tokenizer is not None

def test_chat_generation():
    """Test g√©n√©ration r√©ponse."""
    chat = BBIAChat()
    response = chat.chat("Bonjour")
    assert len(response) > 0
    assert isinstance(response, str)

def test_context_management():
    """Test gestion contexte."""
    chat = BBIAChat()
    chat.chat("Bonjour")
    chat.chat("Comment vas-tu ?")
    assert len(chat.context) == 2

def test_action_detection():
    """Test d√©tection actions."""
    chat = BBIAChat()
    action = chat._detect_action("Tourne la t√™te √† droite")
    assert action is not None
    assert action["action"] == "look_right"
```

### Tests Performance

```python
def test_latency():
    """Test latence g√©n√©ration."""
    chat = BBIAChat()
    start = time.time()
    response = chat.chat("Bonjour")
    latency = time.time() - start
    assert latency < 2.0  # <2s acceptable

def test_memory_usage():
    """Test utilisation m√©moire."""
    chat = BBIAChat()
    import psutil
    process = psutil.Process()
    memory_mb = process.memory_info().rss / 1024 / 1024
    assert memory_mb < 6000  # <6GB pour RPi 5
```

---

## üìö DOCUMENTATION

### Guide Utilisateur

**Fichier :** `docs/guides/GUIDE_LLM_CONVERSATION.md`

**Contenu :**
- Installation et configuration
- Utilisation basique
- Personnalit√©s disponibles
- Actions robot via conversation
- Apprentissage pr√©f√©rences

---

## ‚úÖ CHECKLIST

### Phase 1 (Semaine 1-2) ‚úÖ **TERMIN√âE** - 21 Novembre 2025
- [x] Cr√©er `bbia_chat.py` ‚úÖ **FAIT**
- [x] Int√©grer Phi-2 ou TinyLlama ‚úÖ **FAIT** (avec fallback)
- [x] Remplacer r√®gles dans `bbia_huggingface.py` ‚úÖ **FAIT** (int√©gration BBIAChat)
- [x] Tests basiques ‚úÖ **FAIT** (test_bbia_chat_llm.py)
- [ ] Documentation installation ‚ö†Ô∏è **√Ä FAIRE**

### Phase 2 (Semaine 3-4) ‚úÖ **TERMIN√âE** - 21 Novembre 2025
- [x] Impl√©menter historique conversation ‚úÖ **FAIT** (deque maxlen=10)
- [x] D√©tection actions robot ‚úÖ **FAIT** (_detect_action, _execute_action)
- [x] Int√©gration √©motions ‚úÖ **FAIT** (_extract_emotion, _apply_emotion)
- [x] Tests contexte ‚úÖ **FAIT** (tests basiques existent)
- [ ] Documentation utilisation ‚ö†Ô∏è **√Ä FAIRE**

### Phase 3 (Semaine 5-6) ‚úÖ **TERMIN√âE** - 21 Novembre 2025
- [x] Syst√®me personnalit√©s ‚úÖ **FAIT** (5 personnalit√©s: friendly, professional, playful, calm, enthusiastic)
- [x] Apprentissage pr√©f√©rences ‚úÖ **FAIT** (learn_preference, _adapt_to_preferences, _save_preferences)
- [x] Tests personnalit√©s ‚úÖ **FAIT** (test_bbia_chat_personalities.py existe)
- [ ] Documentation avanc√©e ‚ö†Ô∏è **√Ä FAIRE**
- [x] Optimisation performance ‚úÖ **FAIT** (float16, device_map, timeout)

---

## üéØ M√âTRIQUES DE SUCC√àS

| M√©trique | Actuel | Objectif | Statut |
|----------|--------|----------|--------|
| **Compr√©hension Contextuelle** | ‚úÖ Oui | ‚úÖ Oui | üü¢ **FAIT** (deque maxlen=10) |
| **LLM Conversationnel** | ‚úÖ Oui | ‚úÖ Oui | üü¢ **FAIT** (Phi-2/TinyLlama) |
| **Actions Robot** | ‚úÖ Oui | ‚úÖ Oui | üü¢ **FAIT** (6 actions d√©tect√©es) |
| **Personnalit√©s** | 5 | 5+ | üü¢ **FAIT** (friendly, professional, playful, calm, enthusiastic) |
| **Latence** | <2s | <2s | üü¢ **OK** (timeout 5s) |
| **M√©moire RAM** | <6GB | <6GB | üü¢ **OK** (float16, device_map)

---

## üéâ R√âSULTAT ATTENDU

Apr√®s 6 semaines, BBIA aura :
- ‚úÖ **Vrai LLM conversationnel** (Phi-2 ou TinyLlama)
- ‚úÖ **Compr√©hension contextuelle** (historique 10 messages)
- ‚úÖ **Actions robot via conversation** ("tourne la t√™te")
- ‚úÖ **5 personnalit√©s** distinctes
- ‚úÖ **Apprentissage pr√©f√©rences** utilisateur

**BBIA sera un v√©ritable assistant conversationnel intelligent !**

---

**Document cr√©√© le :** Novembre 2024  
**Derni√®re mise √† jour :** 21 Novembre 2025  
**Version BBIA :** 1.3.2  
**Auteur :** Arkalia Luna System

**√âtat actuel :**
- ‚úÖ Phase 1 : TERMIN√âE (21 Novembre 2025)
- ‚úÖ Phase 2 : TERMIN√âE (21 Novembre 2025) - Historique + actions + √©motions
- ‚úÖ Phase 3 : TERMIN√âE (21 Novembre 2025) - Personnalit√©s + pr√©f√©rences

**Reste √† faire :**
- ‚ö†Ô∏è Documentation utilisation et avanc√©e

