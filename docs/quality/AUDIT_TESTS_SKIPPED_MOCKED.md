# Audit Complet des Tests Saut√©s et Mock√©s

**Date**: 2025-01-27  
**Objectif**: Analyser tous les tests saut√©s (`@pytest.mark.skip`, `pytest.skip`) et mock√©s (`@patch`, `MagicMock`) pour identifier les opportunit√©s d'am√©lioration.

## R√©sum√© Ex√©cutif

- **Tests saut√©s analys√©s**: 556
- **Tests mock√©s analys√©s**: 653
- **Cat√©gories principales**:
  - D√©pendances manquantes: 191 tests
  - Autres raisons: 294 tests
  - Audio: 23 tests
  - Hardware/Robot physique: 24 tests
  - Vision: 15 tests
  - Environnement (CI/macOS): 9 tests

## Top 10 Raisons de Skip

1. **Hugging Face transformers non disponible** (29 tests)
2. **Hugging Face non disponible** (25 tests)
3. **FastAPI non disponible** (18 tests)
4. **D√©pendances ML non disponibles** (17 tests)
5. **BBIAHuggingFace non disponible** (11 tests)
6. **MuJoCo non disponible** (10 tests)
7. **ReachyMiniBackend non disponible** (8 tests)
8. **Backend non disponible** (8 tests)
9. **Fichier bbia_huggingface.py introuvable** (8 tests)
10. **Vision d√©sactiv√©e par BBIA_DISABLE_VISION=1** (7 tests)

## Fichiers avec le Plus de Tests Saut√©s

1. `tests/test_dashboard_advanced.py`: 74 tests
2. `tests/test_dashboard.py`: 42 tests
3. `tests/test_bbia_phase2_modules.py`: 37 tests
4. `tests/test_daemon_bridge.py`: 34 tests
5. `tests/benchmarks/test_performance.py`: 19 tests
6. `tests/test_motion_repeatability.py`: 17 tests
7. `tests/test_huggingface_expert_conformity.py`: 16 tests
8. `tests/test_ram_optimizations_validation.py`: 16 tests
9. `tests/test_sdk_dependencies.py`: 16 tests
10. `tests/test_vision_webcam_real.py`: 15 tests

---

## Analyse par Cat√©gorie

### 1. Tests Saut√©s - D√©pendances Manquantes (191 tests)

**Probl√®me**: Tests qui n√©cessitent des d√©pendances optionnelles non install√©es.

**Exemples**:
- `Hugging Face transformers non disponible` (54 tests)
- `FastAPI non disponible` (18 tests)
- `MuJoCo non disponible` (10 tests)
- `ReachyMiniBackend non disponible` (8 tests)

**Recommandations**:

#### ‚úÖ **Bonne Pratique Actuelle**
```python
@pytest.mark.skipif(
    not HF_AVAILABLE,
    reason="Hugging Face transformers non disponible"
)
```
Cette approche est correcte et permet d'ex√©cuter les tests quand les d√©pendances sont disponibles.

#### üîß **Am√©liorations Possibles**

1. **Cr√©er des fixtures conditionnelles r√©utilisables**:
```python
# tests/conftest.py
@pytest.fixture(scope="session")
def require_huggingface():
    if not HF_AVAILABLE:
        pytest.skip("Hugging Face transformers requis")
    return True
```

2. **Utiliser des marqueurs pytest personnalis√©s**:
```python
# pyproject.toml
[tool.pytest.ini_options]
markers = [
    "requires_hf: N√©cessite Hugging Face transformers",
    "requires_fastapi: N√©cessite FastAPI",
    "requires_mujoco: N√©cessite MuJoCo",
]
```

3. **Documenter les d√©pendances optionnelles**:
   - Cr√©er `docs/development/OPTIONAL_DEPENDENCIES.md`
   - Lister toutes les d√©pendances optionnelles et leurs tests associ√©s

**Verdict**: ‚úÖ **D√©cision correcte** - Les tests doivent √™tre saut√©s si les d√©pendances ne sont pas disponibles. Aucune action requise sauf am√©lioration de la documentation.

---

### 2. Tests Saut√©s - Audio (23 tests)

**Probl√®me**: Tests audio n√©cessitent `sounddevice` ou sont d√©sactiv√©s en CI.

**Exemples**:
- `Audio d√©sactiv√© par BBIA_DISABLE_AUDIO=1`
- `sounddevice indisponible sur cet environnement`

**Fichiers concern√©s**:
- `tests/benchmarks/test_performance.py`
- `tests/test_audio_latency_e2e.py`
- `tests/test_audio_buffer_stability.py`
- `tests/test_audio_latency_loopback.py`

**Recommandations**:

#### ‚úÖ **Bonne Pratique Actuelle**
```python
if os.environ.get("BBIA_DISABLE_AUDIO", "0") == "1":
    pytest.skip("Audio d√©sactiv√© par BBIA_DISABLE_AUDIO=1")
if sd is None:
    pytest.skip("sounddevice indisponible sur cet environnement")
```

#### üîß **Am√©liorations Possibles**

1. **Cr√©er un mock de sounddevice pour les tests**:
```python
# tests/fixtures/audio_mock.py
@pytest.fixture
def mock_sounddevice():
    """Mock sounddevice pour tests sans hardware audio."""
    with patch('sounddevice.play'), \
         patch('sounddevice.rec'), \
         patch('sounddevice.wait'):
        yield
```

2. **Utiliser des tests conditionnels avec marqueurs**:
```python
@pytest.mark.audio
@pytest.mark.skipif(
    os.environ.get("BBIA_DISABLE_AUDIO") == "1",
    reason="Audio d√©sactiv√©"
)
def test_audio_functionality():
    ...
```

**Verdict**: ‚úÖ **D√©cision correcte** - Les tests audio doivent √™tre saut√©s en CI sans hardware audio. Consid√©rer l'ajout de mocks pour certains tests unitaires.

---

### 3. Tests Saut√©s - Hardware/Robot Physique (24 tests)

**Probl√®me**: Tests n√©cessitant un robot physique connect√©.

**Exemples**:
- `N√©cessite robot physique ou mock avanc√©`
- `Connexion robot √©chou√©e`
- `Robot physique requis`

**Fichiers concern√©s**:
- `tests/test_reachy_mini_backend.py`
- `tests/test_watchdog_monitoring.py`
- `tests/test_emergency_stop.py`

**Recommandations**:

#### ‚úÖ **Bonne Pratique Actuelle**
```python
@pytest.mark.skip(reason="N√©cessite robot physique ou mock avanc√©")
```

#### üîß **Am√©liorations Possibles**

1. **Cr√©er des mocks avanc√©s pour les tests hardware**:
```python
# tests/mocks/reachy_mini_mock.py
class MockReachyMini:
    """Mock avanc√© qui simule un robot physique."""
    def __init__(self):
        self.is_connected = True
        self._last_heartbeat = time.time()
    
    def get_current_joint_positions(self):
        """Simule la r√©cup√©ration des positions."""
        if not self.is_connected:
            raise ConnectionError("Robot d√©connect√©")
        return {...}
```

2. **Utiliser des marqueurs pour distinguer tests unitaires/int√©gration**:
```python
@pytest.mark.unit
def test_watchdog_logic():
    """Test unitaire avec mock."""
    ...

@pytest.mark.integration
@pytest.mark.skipif(not has_physical_robot(), reason="Robot physique requis")
def test_watchdog_real_hardware():
    """Test int√©gration avec robot r√©el."""
    ...
```

**Verdict**: ‚ö†Ô∏è **Am√©lioration possible** - Certains tests pourraient √™tre convertis en tests unitaires avec des mocks avanc√©s. Les tests n√©cessitant vraiment un robot physique doivent rester saut√©s.

---

### 4. Tests Saut√©s - Vision (15 tests)

**Probl√®me**: Tests vision d√©sactiv√©s ou n√©cessitant des mod√®les lourds.

**Exemples**:
- `Vision d√©sactiv√©e par BBIA_DISABLE_VISION=1`
- `BBIAVision non disponible`
- `YOLO non disponible`

**Recommandations**:

#### ‚úÖ **Bonne Pratique Actuelle**
```python
if os.environ.get("BBIA_DISABLE_VISION", "0") == "1":
    pytest.skip("Vision d√©sactiv√©e par BBIA_DISABLE_VISION=1")
```

#### üîß **Am√©liorations Possibles**

1. **Utiliser des mod√®les l√©gers pour les tests**:
```python
@pytest.fixture
def mock_yolo_lightweight():
    """Mock YOLO avec mod√®le minimal pour tests."""
    mock_model = MagicMock()
    mock_model.predict.return_value = []
    return mock_model
```

**Verdict**: ‚úÖ **D√©cision correcte** - Les tests vision doivent √™tre saut√©s si d√©sactiv√©s. Aucune action requise.

---

### 5. Tests Saut√©s - Environnement (9 tests)

**Probl√®me**: Tests sp√©cifiques √† une plateforme (macOS, CI).

**Exemples**:
- `Test sp√©cifique macOS`
- `CI environment`

**Recommandations**:

#### ‚úÖ **Bonne Pratique Actuelle**
```python
@pytest.mark.skipif(
    os.getenv("CI") is not None and sys.platform != "darwin",
    reason="Test sp√©cifique macOS"
)
```

**Verdict**: ‚úÖ **D√©cision correcte** - Les tests sp√©cifiques √† une plateforme doivent √™tre saut√©s sur les autres plateformes. Aucune action requise.

---

## Analyse des Tests Mock√©s

### Patterns de Mocking Identifi√©s

1. **Mocks de d√©pendances externes** (majorit√©)
   - `@patch("bbia_sim.bbia_integration.BBIAEmotions")`
   - `@patch("bbia_sim.bbia_integration.BBIAVision")`
   - `@patch("bbia_sim.bbia_integration.SimulationService")`

2. **Mocks de modules syst√®me**
   - `@patch("sounddevice.play")`
   - `@patch("wave.open")`
   - `@patch("cv2.VideoCapture")`

3. **Mocks de classes complexes**
   - `MagicMock()` pour remplacer des instances complexes
   - `AsyncMock()` pour les fonctions async

### Recommandations pour les Mocks

#### ‚úÖ **Bonne Pratique Actuelle**
```python
@patch("bbia_sim.bbia_integration.BBIAEmotions")
@patch("bbia_sim.bbia_integration.BBIAVision")
def test_integration(mock_vision, mock_emotions):
    ...
```

#### üîß **Am√©liorations Possibles**

1. **Cr√©er des fixtures r√©utilisables pour les mocks communs**:
```python
# tests/conftest.py
@pytest.fixture
def mock_bbia_modules():
    """Fixture r√©utilisable pour mocker les modules BBIA."""
    with patch("bbia_sim.bbia_integration.BBIAEmotions") as mock_emotions, \
         patch("bbia_sim.bbia_integration.BBIAVision") as mock_vision, \
         patch("bbia_sim.bbia_integration.SimulationService") as mock_service:
        yield {
            'emotions': mock_emotions,
            'vision': mock_vision,
            'service': mock_service
        }
```

2. **Utiliser des mocks plus r√©alistes**:
```python
# Au lieu de MagicMock() simple
mock_vision = MagicMock()
mock_vision.scan_environment.return_value = {
    'objects': [{'name': 'person', 'confidence': 0.9}]
}
```

3. **Documenter les mocks complexes**:
```python
@pytest.fixture
def mock_reachy_mini_backend():
    """
    Mock complet du ReachyMiniBackend pour tests.
    
    Simule:
    - Connexion/d√©connexion
    - R√©cup√©ration positions joints
    - Envoi commandes mouvement
    """
    ...
```

---

## Tests Sp√©cifiques √† Analyser

### 1. `tests/test_dashboard_advanced.py` (74 tests saut√©s)

**Probl√®me**: Beaucoup de tests saut√©s √† cause de d√©pendances manquantes.

**Recommandation**: 
- V√©rifier si toutes les d√©pendances sont vraiment n√©cessaires
- Cr√©er des fixtures pour mocker les d√©pendances manquantes
- Consid√©rer diviser le fichier en tests unitaires (avec mocks) et tests d'int√©gration

### 2. `tests/test_bbia_chat_llm.py` - `test_chat_context_management`

**Probl√®me**: Timeout d√ª au chargement r√©el du mod√®le LLM.

**Solution appliqu√©e**: ‚úÖ Mock de la m√©thode `generate` avec `patch.object`.

**Verdict**: ‚úÖ **Corrig√©** - Le test utilise maintenant un mock au lieu de charger le mod√®le r√©el.

### 3. `tests/test_watchdog_monitoring.py` - `test_watchdog_timeout_triggers_emergency_stop_real`

**Probl√®me**: N√©cessite robot physique ou mock avanc√©.

**Recommandation**: 
- Cr√©er un mock avanc√© qui simule un robot qui ne r√©pond plus
- Le mock devrait lever une exception dans `get_current_joint_positions()` apr√®s un d√©lai

---

## Plan d'Action Recommand√©

### Priorit√© Haute üî¥

1. **Documenter les d√©pendances optionnelles**
   - Cr√©er `docs/development/OPTIONAL_DEPENDENCIES.md`
   - Lister tous les tests qui n√©cessitent des d√©pendances optionnelles

2. **Cr√©er des fixtures r√©utilisables pour les mocks communs**
   - `tests/conftest.py`: Fixtures pour BBIA modules, audio, vision
   - R√©duire la duplication de code de mock

### Priorit√© Moyenne üü°

3. **Am√©liorer les mocks pour les tests hardware**
   - Cr√©er `tests/mocks/reachy_mini_mock.py` avec mock avanc√©
   - Permettre de tester la logique watchdog sans robot physique

4. **Analyser les tests avec beaucoup de skips**
   - `test_dashboard_advanced.py`: 74 tests saut√©s
   - `test_dashboard.py`: 42 tests saut√©s
   - V√©rifier si certains peuvent √™tre convertis en tests unitaires

### Priorit√© Basse üü¢

5. **Cr√©er des marqueurs pytest personnalis√©s**
   - `@pytest.mark.requires_hf`
   - `@pytest.mark.requires_audio`
   - `@pytest.mark.requires_hardware`

6. **Am√©liorer la documentation des tests**
   - Ajouter des docstrings expliquant pourquoi certains tests sont mock√©s
   - Documenter les limitations des mocks

---

## Conclusion

### D√©cisions Correctes ‚úÖ

- **Tests saut√©s pour d√©pendances manquantes**: Correct, aucune action requise
- **Tests saut√©s pour audio/hardware en CI**: Correct, n√©cessaire pour √©viter les erreurs
- **Tests saut√©s pour environnement sp√©cifique**: Correct, n√©cessaire pour la portabilit√©

### Am√©liorations Possibles üîß

1. **R√©duction de la duplication**: Cr√©er des fixtures r√©utilisables pour les mocks communs
2. **Documentation**: Documenter les d√©pendances optionnelles et les raisons des skips
3. **Mocks avanc√©s**: Cr√©er des mocks plus r√©alistes pour certains tests hardware
4. **Organisation**: Consid√©rer diviser les gros fichiers de tests avec beaucoup de skips

### M√©triques

- **Tests saut√©s**: 556 (principalement pour d√©pendances manquantes - normal)
- **Tests mock√©s**: 653 (normal pour tests unitaires)
- **Taux de skip acceptable**: ‚úÖ Oui, principalement pour d√©pendances optionnelles

**Verdict Global**: ‚úÖ **Les d√©cisions de skip/mock sont globalement correctes**. Les am√©liorations propos√©es sont principalement pour r√©duire la duplication et am√©liorer la maintenabilit√©.

---

## Analyse D√©taill√©e de Cas Sp√©cifiques

### Cas 1: `test_watchdog_timeout_triggers_emergency_stop_real`

**Fichier**: `tests/test_watchdog_monitoring.py:217`

**Probl√®me**: Test saut√© car n√©cessite robot physique ou mock avanc√©.

**Analyse**:
- Le test v√©rifie que le watchdog d√©clenche `emergency_stop()` apr√®s 2s sans heartbeat
- Actuellement saut√© avec `@pytest.mark.skip(reason="N√©cessite robot physique ou mock avanc√©")`

**Recommandation**: ‚úÖ **Cr√©er un mock avanc√©**

```python
def test_watchdog_timeout_triggers_emergency_stop_mocked(self):
    """Test watchdog avec mock avanc√© simulant robot d√©connect√©."""
    from unittest.mock import MagicMock, patch
    
    # Mock du backend qui simule un robot qui ne r√©pond plus
    mock_backend = MagicMock()
    mock_backend.is_connected = True
    mock_backend._last_heartbeat = time.time() - 3.0  # > 2s
    
    # Simuler que get_current_joint_positions l√®ve une exception
    mock_backend.get_current_joint_positions.side_effect = ConnectionError("Robot d√©connect√©")
    
    # V√©rifier que emergency_stop est appel√©
    with patch.object(mock_backend, 'emergency_stop') as mock_emergency:
        # Simuler le check watchdog
        if time.time() - mock_backend._last_heartbeat > 2.0:
            mock_backend.emergency_stop()
        
        mock_emergency.assert_called_once()
```

**Verdict**: ‚ö†Ô∏è **Am√©lioration possible** - Le test peut √™tre impl√©ment√© avec un mock avanc√© sans n√©cessiter de robot physique.

---

### Cas 2: `test_dashboard_advanced.py` (74 tests saut√©s)

**Probl√®me**: Beaucoup de tests saut√©s √† cause de `FASTAPI_AVAILABLE = False`.

**Analyse**:
- FastAPI est une d√©pendance optionnelle
- Tous les tests utilisent `@pytest.mark.skipif(not FASTAPI_AVAILABLE, ...)`
- C'est une bonne pratique pour les d√©pendances optionnelles

**Recommandation**: ‚úÖ **D√©cision correcte**

Les tests sont correctement saut√©s quand FastAPI n'est pas disponible. Cependant:

1. **Am√©lioration possible**: Cr√©er une fixture pour mocker FastAPI:
```python
@pytest.fixture
def mock_fastapi():
    """Mock FastAPI pour tests sans d√©pendance."""
    with patch("bbia_sim.dashboard_advanced.FASTAPI_AVAILABLE", True), \
         patch("bbia_sim.dashboard_advanced.FastAPI"), \
         patch("bbia_sim.dashboard_advanced.WebSocket"):
        yield
```

2. **Documentation**: Ajouter dans le README que FastAPI est optionnel pour certains tests.

**Verdict**: ‚úÖ **D√©cision correcte** - Les tests doivent √™tre saut√©s si FastAPI n'est pas disponible. Aucune action requise sauf am√©lioration optionnelle.

---

### Cas 3: Tests Audio avec `BBIA_DISABLE_AUDIO=1`

**Fichiers**: 
- `tests/benchmarks/test_performance.py`
- `tests/test_audio_latency_e2e.py`
- `tests/test_audio_buffer_stability.py`

**Probl√®me**: Tests saut√©s en CI car audio d√©sactiv√©.

**Analyse**:
- Les tests v√©rifient la latence et la stabilit√© audio
- En CI, `BBIA_DISABLE_AUDIO=1` est souvent d√©fini
- Les tests sont correctement saut√©s avec `pytest.skip()`

**Recommandation**: ‚úÖ **D√©cision correcte**

Les tests audio doivent √™tre saut√©s en CI sans hardware audio. Cependant:

**Am√©lioration possible**: Cr√©er des tests unitaires avec mocks pour la logique m√©tier:
```python
def test_audio_latency_logic_mocked(self):
    """Test de la logique de calcul de latence avec mock."""
    from unittest.mock import MagicMock, patch
    
    mock_sd = MagicMock()
    mock_sd.play.return_value = None
    mock_sd.wait.return_value = None
    
    with patch('sounddevice.play', mock_sd.play), \
         patch('sounddevice.wait', mock_sd.wait):
        # Tester la logique de calcul sans hardware r√©el
        ...
```

**Verdict**: ‚úÖ **D√©cision correcte** - Les tests n√©cessitant du hardware audio doivent √™tre saut√©s en CI. Les tests unitaires peuvent utiliser des mocks.

---

### Cas 4: Tests Hugging Face (54 tests saut√©s)

**Probl√®me**: Beaucoup de tests saut√©s car `HF_AVAILABLE = False`.

**Analyse**:
- Hugging Face transformers est une d√©pendance optionnelle lourde
- Les tests utilisent `@pytest.mark.skipif(not HF_AVAILABLE, ...)`
- C'est correct pour une d√©pendance optionnelle

**Recommandation**: ‚úÖ **D√©cision correcte**

**Am√©lioration possible**: Cr√©er des fixtures pour mocker les mod√®les HF:
```python
@pytest.fixture
def mock_huggingface():
    """Mock Hugging Face pour tests sans d√©pendance."""
    with patch("bbia_sim.bbia_huggingface.HF_AVAILABLE", True), \
         patch("bbia_sim.bbia_huggingface.AutoModelForCausalLM"), \
         patch("bbia_sim.bbia_huggingface.AutoTokenizer"):
        yield
```

**Verdict**: ‚úÖ **D√©cision correcte** - Les tests doivent √™tre saut√©s si HF n'est pas disponible. Aucune action requise.

---

## R√©sum√© des Recommandations par Priorit√©

### üî¥ Priorit√© Haute (√Ä faire rapidement)

1. **Cr√©er un mock avanc√© pour `test_watchdog_timeout_triggers_emergency_stop_real`**
   - Permet de tester la logique watchdog sans robot physique
   - Fichier: `tests/test_watchdog_monitoring.py`

2. **Documenter les d√©pendances optionnelles**
   - Cr√©er `docs/development/OPTIONAL_DEPENDENCIES.md`
   - Lister toutes les d√©pendances et leurs tests associ√©s

### üü° Priorit√© Moyenne (Am√©liorations)

3. **Cr√©er des fixtures r√©utilisables pour les mocks communs**
   - `tests/conftest.py`: Fixtures pour BBIA modules, FastAPI, HF
   - R√©duire la duplication de code

4. **Analyser les tests avec beaucoup de skips**
   - V√©rifier si certains peuvent √™tre convertis en tests unitaires
   - `test_dashboard_advanced.py`: Consid√©rer diviser en tests unitaires/int√©gration

### üü¢ Priorit√© Basse (Nice to have)

5. **Cr√©er des marqueurs pytest personnalis√©s**
   - `@pytest.mark.requires_hf`
   - `@pytest.mark.requires_audio`
   - `@pytest.mark.requires_hardware`

6. **Am√©liorer la documentation des tests**
   - Ajouter des docstrings expliquant pourquoi certains tests sont mock√©s
   - Documenter les limitations des mocks

---

## Annexes

### Commandes Utiles

```bash
# Lister tous les tests saut√©s
pytest --collect-only -q | grep "SKIPPED"

# Ex√©cuter uniquement les tests non saut√©s
pytest -m "not skip"

# Ex√©cuter les tests avec d√©pendances optionnelles
pytest -m "requires_hf"  # Si marqueur cr√©√©

# Compter les tests saut√©s par raison
pytest --collect-only -q | grep "SKIPPED" | sort | uniq -c

# Lister les fichiers avec le plus de tests saut√©s
pytest --collect-only -q | grep "SKIPPED" | cut -d: -f1 | sort | uniq -c | sort -rn
```

### R√©f√©rences

- [Pytest Skip Documentation](https://docs.pytest.org/en/stable/how-to/skipping.html)
- [Pytest Mocking Best Practices](https://docs.python.org/3/library/unittest.mock.html)
- [Testing with Optional Dependencies](https://docs.pytest.org/en/stable/example/simple.html#control-skipping-of-tests-according-to-command-line-option)
- [Pytest Fixtures](https://docs.pytest.org/en/stable/fixture.html)

