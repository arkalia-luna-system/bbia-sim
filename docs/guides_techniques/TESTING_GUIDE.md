# ğŸ§ª Guide des tests et de la couverture - BBIA Reachy Mini

## ğŸ“Š RÃ©sumÃ© des performances

**Coverage totale : validÃ©e en CI** (voir `coverage.xml` et `htmlcov/`)

- **Suite de tests complÃ¨te** exÃ©cutÃ©e par pytest (compteur variable selon CI)
- **RÃ©sultats** : voir le rÃ©capitulatif CI (pass/failed/skipped)
- **Tests skippÃ©s** justifiÃ©s (robot physique requis)

## ğŸ—ï¸ Structure des tests

```mermaid
graph TB
    subgraph "Tests Structure"
        E2E[e2e/<br/>Tests end-to-end]
        SIM[sim/<br/>Tests simulation]
        WS[ws/<br/>Tests WebSocket]
        BBIA[test_bbia_*.py<br/>Tests modules BBIA]
        DAEMON[test_daemon_*.py<br/>Tests daemon]
        WEBSOCKET[test_websocket_*.py<br/>Tests WebSocket]
        VERTICAL[test_vertical_slices.py<br/>Tests vertical slices]
        GOLDEN[test_golden_traces.py<br/>Tests golden traces]
    end

    E2E --> API[test_api_simu_roundtrip.py]
    E2E --> MODULES[test_bbia_modules_e2e.py]
    E2E --> MOTION[test_motion_roundtrip.py]
    E2E --> TELEMETRY[test_websocket_telemetry_e2e.py]

    SIM --> CLI[test_cli_help.py]
    SIM --> DURATION[test_duration.py]

    WS --> RATE[test_telemetry_rate.py]

    BBIA --> AUDIO[test_bbia_audio.py]
    BBIA --> BEHAVIOR[test_bbia_behavior.py]
    BBIA --> EMOTIONS[test_bbia_emotions.py]
    BBIA --> VISION[test_bbia_vision.py]
    BBIA --> VOICE[test_bbia_voice.py]

    DAEMON --> CONFIG[test_daemon_config.py]
    DAEMON --> MODELS[test_daemon_models.py]
    DAEMON --> SIMULATION[test_daemon_simulation_service.py]

    WEBSOCKET --> CONNECTION[test_websocket_connection.py]
    WEBSOCKET --> TELEMETRY_EXT[test_websocket_telemetry_extended.py]

    VERTICAL --> DEMO_EMOTION[test_demo_emotion_headless]
    VERTICAL --> DEMO_VOICE[test_demo_voice_headless]
    VERTICAL --> DEMO_VISION[test_demo_vision_headless]
    VERTICAL --> DEMO_BEHAVIOR[test_demo_behavior_headless]
```

## ğŸ“Š Couverture par module

```mermaid
pie title Coverage par Module (exemple)
    "bbia_audio.py" : 87.76
    "bbia_vision.py" : 88.52
    "daemon/config.py" : 100
    "daemon/models.py" : 95.35
    "daemon/simulation_service.py" : 89.83
    "daemon/ws/telemetry.py" : 78.38
    "sim/joints.py" : 72.22
    "sim/simulator.py" : 99.29
    "unity_reachy_controller.py" : 81.68
    "Autres" : 23.07
```

## ğŸ§ª Types de tests

```mermaid
graph LR
    subgraph "Tests Unitaires"
        UNIT[Tests unitaires<br/>Fonctions isolÃ©es<br/>Mocking]
    end

    subgraph "Tests d'IntÃ©gration"
        INTEGRATION[Tests d'intÃ©gration<br/>Modules ensemble<br/>API + Simulation]
    end

    subgraph "Tests End-to-End"
        E2E[Tests E2E<br/>ScÃ©narios complets<br/>CLI â†’ API â†’ Simulation]
    end

    subgraph "Tests de Performance"
        PERF[Tests performance<br/>Temps d'exÃ©cution<br/>MÃ©moire]
    end

    UNIT --> INTEGRATION
    INTEGRATION --> E2E
    E2E --> PERF
```
â”‚   â”œâ”€â”€ test_bbia_emotions.py     # Tests Ã©motions
â”‚   â”œâ”€â”€ test_bbia_emotions_extended.py
â”‚   â”œâ”€â”€ test_bbia_vision.py       # Tests vision
â”‚   â”œâ”€â”€ test_bbia_vision_extended.py
â”‚   â”œâ”€â”€ test_bbia_voice.py        # Tests voix
â”‚   â””â”€â”€ test_bbia_voice_extended.py
â”œâ”€â”€ test_api_*.py                 # Tests API
â”œâ”€â”€ test_simulator.py             # Tests simulateur MuJoCo
â”œâ”€â”€ test_unity_controller.py      # Tests contrÃ´leur Unity
â””â”€â”€ test_*.py                     # Tests unitaires
```

## Commandes de tests

### Tests complets
```bash
# Lancer tous les tests avec coverage complet
python -m pytest tests/ --cov=src --cov-report=term-missing --cov-report=html

# Tests rapides sans dÃ©tails
python -m pytest tests/ --cov=src --cov-fail-under=0 --tb=no -q

# Tests avec arrÃªt au premier Ã©chec
python -m pytest tests/ --cov=src --cov-report=term-missing -x
```

### Tests golden traces
```bash
# Tests de non-rÃ©gression golden traces
python -m pytest tests/test_golden_traces.py -v

# RÃ©gÃ©nÃ©rer une trace de rÃ©fÃ©rence
python scripts/record_trace.py --emotion happy --duration 5 --out artifacts/golden/happy_mujoco.jsonl

# Valider une trace contre rÃ©fÃ©rence
python scripts/validate_trace.py --ref artifacts/golden/happy_mujoco.jsonl --cur current_trace.jsonl
```

### Tests spÃ©cifiques
```bash
# Tests d'un module spÃ©cifique
python -m pytest tests/test_bbia_emotions.py -v

# Tests d'un sous-dossier
python -m pytest tests/e2e/ -v

# Test spÃ©cifique
python -m pytest tests/test_bbia_emotions.py::TestBBIAEmotions::test_set_emotion -v
```

### VÃ©rification de la couverture
```bash
# Ouvrir le rapport HTML (macOS)
open htmlcov/index.html

# Compter le nombre de tests collectÃ©s (variable selon CI)
python -m pytest --collect-only -q | wc -l

# Coverage d'un module spÃ©cifique
python -m pytest tests/test_bbia_emotions.py --cov=src.bbia_sim.bbia_emotions --cov-report=term-missing
```

## âš™ï¸ Configuration

### pyproject.toml
```toml
[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
norecursedirs = [".git", "log", ".venv", "venv", "__pycache__", "reachy_repos"]
minversion = "6.0"

[tool.coverage.run]
source = ["src/bbia_sim"]
omit = [
    "*/tests/*",
    "*/test_*",
    "*/__pycache__/*",
    "*/venv/*",
    "*/.venv/*",
    "*/log/*",
    "*/reachy_repos/*",
]

[tool.coverage.report]
fail_under = 1
show_missing = true
precision = 2
```

### .coveragerc
```ini
[run]
source = src
omit = */tests/*, */test_*, */__pycache__/*, */venv/*

[report]
fail_under = 1
show_missing = True
show_missing_branches = True
precision = 2
ignore_errors = True

[html]
directory = htmlcov
title = BBIA Reachy Mini Simulation Coverage Report

[xml]
output = coverage.xml
```

## ğŸ”§ RÃ©solution des problÃ¨mes

### ProblÃ¨me : couverture trop faible malgrÃ© un grand nombre de tests

**SymptÃ´mes :**
- Coverage affichÃ© bas malgrÃ© de nombreux tests
- Tests passent mais coverage ne s'amÃ©liore pas

**Causes possibles :**
1. **Configuration testpaths incorrecte**
2. **Structure de dossiers non respectÃ©e**
3. **Fichiers __init__.py manquants**
4. **Configuration coverage incorrecte**

**Solutions :**

1. **VÃ©rifier la configuration pytest :**
```bash
python -m pytest --collect-only -q | wc -l
# Nombre indicatif selon la configuration CI
```

2. **VÃ©rifier la structure des dossiers :**
```bash
find tests/ -name "test_*.py" | wc -l
```

3. **VÃ©rifier les fichiers __init__.py :**
```bash
find tests/ -name "__init__.py"
```

4. **Tester la configuration coverage :**
```bash
python -m pytest tests/test_config.py --cov=src --cov-report=term-missing
```

### ProblÃ¨me : tests qui Ã©chouent

**Tests courants qui peuvent Ã©chouer :**
- `test_get_available_joints` : Mock MuJoCo incorrect
- `test_emotional_response_*` : Mock secrets incorrect
- `test_dire_texte_*` : Mock pyttsx3 incorrect

**Solutions :**
- VÃ©rifier les mocks dans les tests
- Utiliser `--cov-fail-under=0` pour ignorer les erreurs de coverage
- Corriger les assertions trop strictes

## ğŸ“ˆ AmÃ©lioration de la couverture

### Modules Ã  amÃ©liorer
1. **bbia_voice.py** : Ajouter tests pour reconnaissance vocale
2. **bbia_awake.py** : Ajouter tests pour sÃ©quence rÃ©veil
3. **bbia_integration.py** : CrÃ©er tests d'intÃ©gration
4. **__main__.py** : Ajouter tests CLI

### StratÃ©gies d'amÃ©lioration
1. **Tests d'intÃ©gration** : Tester les interactions entre modules
2. **Tests de cas limites** : Tester les cas d'erreur
3. **Tests de performance** : Tester les performances
4. **Tests de rÃ©gression** : PrÃ©venir les rÃ©gressions

## ğŸ¯ Objectifs de couverture

- **Objectif minimum** : 70%
- **Objectif recommandÃ©** : 80%
- **Objectif ambitieux** : 90%

---

*DerniÃ¨re mise Ã  jour : Octobre 2025*
