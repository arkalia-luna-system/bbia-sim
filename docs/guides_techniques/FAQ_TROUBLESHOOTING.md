# FAQ Troubleshooting - Guide Complet

> Compatibilit√© Python: 3.11+
>
> Voir aussi: `docs/references/INDEX_THEMATIQUE.md` et `docs/status.md` (√âtat par axe)

---

## üî¥ Modules IA - Probl√®mes Critiques

### DeepFace crash ou erreur

**Sympt√¥mes** :
- `DeepFace non disponible` ou `ImportError`
- `No face detected` m√™me avec visage visible
- Performance tr√®s lente (>5s par analyse)

**Solutions** :

1. **Installation manquante** :
```bash
source venv-vision-py310/bin/activate
pip install deepface onnxruntime
```

2. **Aucun visage d√©tect√©** :
- V√©rifier √©clairage (assez lumineux)
- Visage face cam√©ra (pas de profil)
- R√©solution image suffisante (min 320x240)
- Utiliser `enforce_detection=False` (par d√©faut)
```python
# Dans face_recognition.py
recognize_person(image, enforce_detection=False)
```

3. **Performance lente sur RPi 5** :
- Utiliser backend ONNX au lieu de TensorFlow
- Mod√®le VGG-Face (plus l√©ger que Facenet)
- Backend d√©tecteur OpenCV (plus rapide que RetinaFace)
```python
# Configuration optimale RPi 5
os.environ["BBIA_DEEPFACE_MODEL"] = "VGG-Face"
os.environ["BBIA_DEEPFACE_BACKEND"] = "opencv"
```

4. **Fallback automatique** :
- Si DeepFace crash, BBIA utilise MediaPipe Face Detection
- V√©rifier logs : `‚ö†Ô∏è DeepFace non disponible: {e}`

---

### LLM (HuggingFace) timeout ou m√©moire insuffisante

**Sympt√¥mes** :
- Latence > 30s pour r√©ponse
- `OutOfMemoryError` ou crash
- Mod√®le ne charge pas

**Solutions** :

1. **Raspberry Pi 5 (8GB max)** :
- ‚ùå Mistral 7B (14GB) ‚Üí Trop lourd
- ‚ùå Llama 3 8B (16GB) ‚Üí Trop lourd
- ‚úÖ **Utiliser Phi-2 (2.7B, ~5GB)** ou **TinyLlama (1.1B, ~2GB)**
```python
# Dans bbia_huggingface.py
model_configs["chat"] = {
    "phi2": {
        "model_name": "microsoft/phi-2",
        "quantization": "8bit",  # R√©duire m√©moire
    },
    "tinyllama": {
        "model_name": "TinyLlama/TinyLlama-1.1B",
        "quantization": "8bit",
    }
}
```

2. **API externe (alternative)** :
- Utiliser Hugging Face Inference API (gratuite)
- Pas de chargement mod√®le local
```python
# Configuration API externe
os.environ["BBIA_HF_API_KEY"] = "your_api_key"
hf.chat("message", use_api=True)
```

3. **Timeout trop court** :
- Augmenter timeout (d√©faut: 30s)
```python
# Dans test_huggingface_latency.py
assert p95 < 30000.0  # 30s max (CI tol√©rant)
```

---

### Whisper STT ne d√©tecte rien ou lent

**Sympt√¥mes** :
- `Whisper non disponible`
- Transcription vide
- Latence > 10s

**Solutions** :

1. **Installation** :
```bash
source venv-voice/bin/activate  # ou venv principal
pip install openai-whisper
```

2. **Mod√®le trop lourd** :
- Utiliser `whisper-tiny` ou `whisper-base` sur RPi 5
- √âviter `whisper-medium/large` (> 5GB)
```python
# Dans voice_whisper.py
model_size = os.environ.get("BBIA_WHISPER_MODEL", "tiny")
```

3. **Microphone non d√©tect√©** :
- V√©rifier permissions macOS (R√©glages > Confidentialit√© > Micro)
- Tester avec `python scripts/test_audio_simple.py`
- Fallback automatique vers `speech_recognition` si Whisper indisponible

---

### MediaPipe Pose ne d√©tecte rien

**Sympt√¥mes** :
- Aucun point cl√© d√©tect√©
- R√©sultat vide

**Solutions** :
- Personne doit √™tre visible en entier (pas coup√©e)
- √âclairage suffisant
- Complexit√© mod√®le : `0=rapide, 1=√©quilibr√©, 2=pr√©cis` (d√©faut: 1)
```python
os.environ["BBIA_POSE_COMPLEXITY"] = "1"  # ou "0" pour plus rapide
```

---

## üü° Tests CI - Seuils RAM/CPU

### Tests budget RAM/CPU √©chouent en CI

**Sympt√¥mes** :
- `test_backend_budget_cpu_ram` √©choue
- `test_huggingface_latency` timeout
- M√©moire augmente > seuil attendu

**Explication des seuils** :

1. **Seuils flexibles pour CI** :
   - Environnements CI varient (GitHub Actions, etc.)
   - Mod√®les peuvent √™tre en cache
   - Machine CI peut avoir moins de RAM

2. **Seuils actuels (tol√©rants CI)** :
   - Backend main loop : < 120MB RAM (au lieu de 50MB strict)
   - Interface abstraite : < 90MB RAM
   - HuggingFace latence : < 30s p95 (au lieu de 5s id√©al)

3. **V√©rification locale vs CI** :
```bash
# Local : devrait √™tre < 50MB
# CI : tol√®re jusqu'√† 120MB (variations machine)
pytest tests/test_backend_budget_cpu_ram.py -v
```

4. **Si test √©choue vraiment** :
- V√©rifier fuites m√©moire avec `tracemalloc`
- Profiler avec `memory_profiler`
- Optimiser chargement mod√®les (lazy loading)

---

## üü¢ MuJoCo & Simulation

- Fen√™tre ne s'ouvre pas: utiliser `mjpython` (macOS) ou mode headless `MUJOCO_GL=disable`
- EGL headless: v√©rifier libGL/GLFW (`sudo apt-get install libglfw3-dev libgl1-mesa-dev`)

---

## üîµ PortAudio / Audio

- Erreur device: d√©sactiver en CI `BBIA_DISABLE_AUDIO=1`
- Sample rate: viser 16kHz; ajuster drivers si mismatch

---

## üü£ WebSockets & R√©seau

- D√©connexions: v√©rifier proxy/timeouts; r√©duire fr√©quence, logs c√¥t√© serveur
- Latence r√©seau robot r√©el : configurer timeout Zenoh
```python
# Dans reachy_mini_backend.py
ROBOT_TIMEOUT = 5.0  # Secondes
```

---

## üü† CORS & S√©curit√©

- D√©veloppement: autoriser `http://localhost:*`
- Production: whitelist stricte; √©viter `*` en credentials

---

## üìö R√©f√©rences

- √âtat par axe: `docs/status.md` ‚Üí Docs / Onboarding
- Guide DeepFace: `docs/guides_techniques/DEEPFACE_SETUP.md`
- Guide Webcam: `docs/guides_techniques/GUIDE_WEBCAM_MX_BRIO.md`
