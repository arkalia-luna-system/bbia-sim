## Profils d'environnement (venv) et camÃ©ra â€“ Guide rapide

### Objectif
- SÃ©parer les outils pour Ã©viter les conflits de dÃ©pendances (numpy, mediapipe, reachy, TTS avancÃ©).
- Avoir une vision qui marche tout de suite, sans casser le reste.

---

### Profil A â€“ venv principal (simulation/hardware)
- Emplacement: `venv`
- Usage: simulation MuJoCo, API, dashboard, intÃ©gration Reachy (hardware mock/rÃ©el), chat (HF si dispo)
- Commandes utiles:
  - Activer: `source venv/bin/activate`
  - DÃ©marrer dashboard: `python src/bbia_sim/dashboard_advanced.py --port 8000`
  - DÃ©mo 3D (macOS): `./LANCE_DEMO_3D.sh` (utilise `mjpython`)

Notes:
- Nâ€™installer ni Coqui TTS ni mediapipe ici pour Ã©viter conflits.

---

### Profil B â€“ venv vision (recommandÃ©)
- Emplacement: `venv-vision-py310`
- Usage: vision temps rÃ©el (MediaPipe, YOLO, OpenCV)
- CrÃ©ation (dÃ©jÃ  fait):
  - `python -m venv venv-vision-py310`
  - `source venv-vision-py310/bin/activate`
  - `pip install numpy==1.26.4 matplotlib==3.8.4 opencv-python mediapipe ultralytics`
- Nettoyage macOS (si erreurs Matplotlib sur fichiers ._*):
  - `find venv-vision-py310/lib/python3.10/site-packages/matplotlib/mpl-data/stylelib -name '._*.mplstyle' -delete`
- Test:
  - `python -c "import mediapipe, cv2; print('VISION OK')"`

CamÃ©ra:
- **Webcam USB UVC (recommandÃ©)** : Plug-and-play, compatible OpenCV (ex. Logitech C920, Logitech MX Brio).
  - Configuration : `export BBIA_CAMERA_INDEX=0` (dÃ©faut: 0 = premiÃ¨re camÃ©ra USB)
  - Test rapide : `python scripts/test_webcam_simple.py` (dans venv-vision-py310)
  - Test vision complÃ¨te : `python scripts/test_vision_webcam.py` (YOLO + MediaPipe)
- **iPad (Appareil photo de continuitÃ©)** : Pratique pour apps macOS, moins fiable via OpenCV.
- **Permissions macOS** : Au premier lancement, macOS demande l'autorisation camÃ©ra pour Terminal/Python. Autoriser dans RÃ©glages SystÃ¨me > ConfidentialitÃ© > CamÃ©ra.

---

### Profil C â€“ venv voix avancÃ©e (optionnel)
- Usage: gÃ©nÃ©rer des WAV jolis (pitch/Ã©motions) avec Coqui TTS, sans toucher au venv principal
- Conseils:
  - CrÃ©er un venv sÃ©parÃ© (ex. `venv-voice`) et y installer `TTS`/`playsound`
  - GÃ©nÃ©rer des fichiers `.wav` puis les jouer via `robot.media.play_audio`
  - Ã‰vite les conflits numpy/scipy dans le venv principal

---

### Profil D â€“ DeepFace (optionnel, dans venv-vision-py310)
- Usage: Reconnaissance visage personnalisÃ©e + dÃ©tection Ã©motions
- Installation:
  ```bash
  source venv-vision-py310/bin/activate
  pip install -r requirements/requirements-deepface.txt
  # Ou: pip install deepface onnxruntime
  ```
- Utilisation:
  - Enregistrer personnes: `python scripts/test_deepface.py --register photo.jpg --name Alice`
  - ReconnaÃ®tre: `python scripts/test_deepface.py --recognize frame.jpg`
  - DÃ©tecter Ã©motion: `python scripts/test_deepface.py --emotion photo.jpg`
- Documentation complÃ¨te: `docs/guides_techniques/DEEPFACE_SETUP.md`

---

### Lancer les dÃ©mos sans conflit
- Simulation 3D (profil A):
  - `source venv/bin/activate`
  - `./LANCE_DEMO_3D.sh` (macOS: ouvre le viewer via `mjpython`)
- Vision seule (profil B):
  - `source venv-vision-py310/bin/activate`
  - Script vision/abonnements/analyses (selon besoins)

---

### Activer l'intelligence (LLM) â€“ optionnel
- PrÃ©requis (profil A): `transformers`, `torch` installÃ©s (dÃ©jÃ  prÃ©sents)
- Exemple dâ€™activation ponctuelle:
```python
from bbia_sim.bbia_huggingface import BBIAHuggingFace
bbia = BBIAHuggingFace()
bbia.enable_llm_chat()  # TÃ©lÃ©charge/charge le LLM (internet requis au premier run)
```

---

### DÃ©pendances Optionnelles - CentralisÃ©es

**Vision et DÃ©tection :**
- **DeepFace** (reconnaissance visage + Ã©motions) :
  - Fichier : `requirements/requirements-deepface.txt`
  - Installation : `pip install -r requirements/requirements-deepface.txt`
  - Usage : Reconnaissance visage personnalisÃ©e, dÃ©tection Ã©motions (7 Ã©motions)
  - Profil : `venv-vision-py310` (recommandÃ©)
  - Documentation : `docs/guides_techniques/DEEPFACE_SETUP.md`

- **MediaPipe** (dÃ©tection postures/gestes) :
  - Installation : `pip install mediapipe` (dÃ©jÃ  inclus dans venv-vision-py310)
  - Usage : DÃ©tection 33 points clÃ©s corps, gestes (bras levÃ©s, debout/assis)
  - Profil : `venv-vision-py310`
  - Scripts : `scripts/test_pose_detection.py`

- **YOLOv8n** (dÃ©tection objets) :
  - Installation : `pip install ultralytics` (inclus dans `requirements.txt`)
  - Usage : DÃ©tection objets temps rÃ©el (COCO dataset)
  - Profil : `venv-vision-py310` (recommandÃ©) ou `venv` principal
  - IntÃ©gration : Automatique via `bbia_vision.py`

**IA et LLM :**
- **Hugging Face Transformers** (LLM conversationnel) :
  - Installation : `pip install transformers torch` (dÃ©jÃ  prÃ©sents dans `requirements.txt`)
  - ModÃ¨les disponibles :
    - Mistral 7B : `~14GB RAM` (recommandÃ© si RAM suffisante)
    - Llama 3 8B : `~16GB RAM`
    - Phi-2 : `~5GB RAM` (recommandÃ© RPi 5)
    - TinyLlama : `~2GB RAM` (ultra-lÃ©ger)
  - Activation : `python -c "from bbia_sim.bbia_huggingface import BBIAHuggingFace; hf = BBIAHuggingFace(); hf.enable_llm_chat('phi2')"`
  - Profil : `venv` principal
  - Documentation : Voir README section "IA AvancÃ©e"

- **Whisper** (reconnaissance vocale) :
  - Installation : `pip install openai-whisper` (inclus dans `requirements.txt`)
  - Usage : Transcription audio â†’ texte (STT)
  - Profil : `venv` principal
  - Backend : Auto-dÃ©tection, fallback dummy si non disponible

**TTS AvancÃ© (optionnel) :**
- **KittenTTS / Kokoro / NeuTTS** :
  - Installation : Selon backend choisi
  - Configuration : `export BBIA_TTS_BACKEND=kitten` (ou `kokoro`, `neutts`)
  - Profil : `venv-voice` sÃ©parÃ© (recommandÃ©) ou `venv` principal
  - Fallback : `pyttsx3` (toujours disponible)

- **Coqui TTS** :
  - Installation : `pip install TTS` (venv sÃ©parÃ© recommandÃ©)
  - Usage : GÃ©nÃ©ration WAV avec pitch/Ã©motions
  - Profil : `venv-voice` sÃ©parÃ© (Ã©vite conflits numpy)

**Dashboard :**
- **Gradio** (interface no-code) :
  - Fichier : `requirements/requirements-gradio.txt`
  - Installation : `pip install gradio`
  - Usage : `python scripts/dashboard_gradio.py --port 7860`
  - Profil : `venv` principal
  - FonctionnalitÃ©s : Upload images, chat, DeepFace registration

**RÃ©sumÃ© Tableau :**

| DÃ©pendance | Fichier Requirements | Profil RecommandÃ© | Usage |
|------------|---------------------|-------------------|-------|
| DeepFace | `requirements-deepface.txt` | `venv-vision-py310` | Reconnaissance visage |
| MediaPipe | `requirements.txt` | `venv-vision-py310` | DÃ©tection postures |
| YOLOv8n | `requirements.txt` | `venv-vision-py310` | DÃ©tection objets |
| Transformers/Torch | `requirements.txt` | `venv` principal | LLM conversationnel |
| Whisper | `requirements.txt` | `venv` principal | Reconnaissance vocale |
| Gradio | `requirements-gradio.txt` | `venv` principal | Dashboard no-code |
| KittenTTS/Kokoro | Variables env | `venv-voice` ou principal | TTS avancÃ© |

> ğŸ’¡ **Note** : La plupart des dÃ©pendances principales sont dÃ©jÃ  incluses dans `requirements.txt`. Les dÃ©pendances optionnelles sont documentÃ©es ici pour faciliter l'installation ciblÃ©e selon les besoins.

---

### FAQ
- Pourquoi sÃ©parer les venv ?
  - Certains paquets demandent des versions de numpy/scipy incompatibles entre eux (mediapipe vs reachy/others). Les sÃ©parer Ã©vite de tout casser.
- Puis-je utiliser lâ€™iPad comme camÃ©ra ?
  - Oui pour des apps macOS (FaceTime/Zoom). Pour OpenCV/Python, lâ€™USB UVC est plus simple.


