# ğŸ§ª Guide Complet du SystÃ¨me de Tests BBIA-SIM

**Version**: 1.0  
**Date**: Oct / Nov. 2025

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [Structure des Tests](#structure-des-tests)
3. [ExÃ©cution des Tests](#exÃ©cution-des-tests)
4. [Marqueurs pytest](#marqueurs-pytest)
5. [Couverture de Code](#couverture-de-code)
6. [Tests E2E](#tests-e2e)
7. [Mocks et Fixtures](#mocks-et-fixtures)
8. [Bonnes Pratiques](#bonnes-pratiques)
9. [CI/CD](#cicd)
10. [DÃ©pannage](#dÃ©pannage)

---

## ğŸ¯ Vue d'ensemble

Le systÃ¨me de tests BBIA-SIM utilise **pytest** comme framework principal avec une architecture modulaire et des tests couvrant :

- **Tests unitaires** : Modules individuels (vision, voice, emotions, etc.)
- **Tests d'intÃ©gration** : IntÃ©gration entre modules
- **Tests E2E** : ScÃ©narios utilisateur complets
- **Tests de performance** : Benchmarks CPU/RAM, latence
- **Tests de conformitÃ©** : SDK Reachy Mini officiel

### Statistiques

- **Total tests** : **1245 tests collectÃ©s** (pytest --collect-only)
- **Coverage global** : **68.86%** (excellent)
- **Coverage modules critiques** :
  - `dashboard_advanced.py` : **0.00%** âš ï¸ (tests existent mais ne couvrent pas)
  - `vision_yolo.py` : **17.49%** âš ï¸ (objectif 50%+ non atteint)
  - `voice_whisper.py` : **75.83%** âœ… (objectif 50%+ dÃ©passÃ©)
  - `daemon/bridge.py` : **0.00%** âš ï¸ (tests existent mais ne couvrent pas)
- **Tests E2E** : 4 scÃ©narios utilisateur

---

## ğŸ“ Structure des Tests

```
tests/
â”œâ”€â”€ conftest.py              # Configuration pytest globale (lock, fixtures)
â”œâ”€â”€ e2e/                     # Tests end-to-end scÃ©narios utilisateur
â”‚   â”œâ”€â”€ test_e2e_face_detection_greeting.py
â”‚   â”œâ”€â”€ test_e2e_voice_interaction.py
â”‚   â”œâ”€â”€ test_e2e_wake_up_sequence.py
â”‚   â””â”€â”€ test_e2e_full_interaction_loop.py
â”œâ”€â”€ test_*.py                # Tests unitaires/intÃ©gration
â”‚   â”œâ”€â”€ test_bbia_vision*.py
â”‚   â”œâ”€â”€ test_bbia_emotions*.py
â”‚   â”œâ”€â”€ test_voice_whisper*.py
â”‚   â”œâ”€â”€ test_vision_yolo*.py
â”‚   â”œâ”€â”€ test_reachy_mini*.py
â”‚   â””â”€â”€ ...
â””â”€â”€ reachy_mini_mock.py      # Mock RobotAPI pour tests sans hardware
```

### Organisation par Type

#### Tests Unitaires (`test_*_*.py`)
- **Vision** : `test_bbia_vision.py`, `test_bbia_vision_extended.py`
- **Voice** : `test_voice_whisper_comprehensive.py`
- **YOLO** : `test_vision_yolo_comprehensive.py`
- **Emotions** : `test_bbia_emotions.py`
- **Mapping** : `test_mapping_reachy_complete.py`

#### Tests ConformitÃ© SDK
- `test_reachy_mini_backend.py`
- `test_reachy_mini_strict_conformity.py`
- `test_reachy_mini_complete_conformity.py`

#### Tests Performance
- `test_backend_budget_cpu_ram.py`
- `test_vision_fps_budget.py`
- `test_vision_latency.py`
- `test_runtime_budget.py`

#### Tests E2E
- ScÃ©narios utilisateur complets avec mocks

---

## ğŸš€ ExÃ©cution des Tests

### Commandes de Base

```bash
# Activer environnement virtuel
source venv/bin/activate

# Tous les tests
pytest

# Tests rapides uniquement (unit, fast)
pytest -m "unit and fast"

# Tests lents exclus
pytest -m "not slow"

# Tests E2E uniquement
pytest -m e2e

# Test spÃ©cifique
pytest tests/test_bbia_vision.py::TestBBIAVision::test_vision_creation

# Mode verbose
pytest -v

# Afficher print statements
pytest -s
```

### Options Utiles

```bash
# ArrÃªter au premier Ã©chec
pytest -x

# ExÃ©cuter seulement les tests Ã©chouÃ©s derniÃ¨re fois
pytest --lf

# Mode parallÃ¨le (si pytest-xdist installÃ©)
pytest -n auto

# Couverture code
pytest --cov=src/bbia_sim --cov-report=html

# Timeout global (300s par dÃ©faut)
pytest --timeout=300
```

---

## ğŸ·ï¸ Marqueurs pytest

### Marqueurs Standard

```python
@pytest.mark.unit          # Test unitaire
@pytest.mark.integration   # Test d'intÃ©gration
@pytest.mark.e2e          # Test end-to-end
@pytest.mark.slow         # Test lent (> 1s)
@pytest.mark.fast         # Test rapide (< 1s)
@pytest.mark.skip         # Test ignorÃ©
@pytest.mark.skipif(...)  # Test conditionnel
```

### Utilisation

```python
import pytest

@pytest.mark.unit
@pytest.mark.fast
def test_quick_unit():
    """Test unitaire rapide."""
    pass

@pytest.mark.e2e
@pytest.mark.slow
def test_full_scenario():
    """Test E2E lent."""
    pass

@pytest.mark.skipif(
    os.environ.get("SKIP_HARDWARE_TESTS", "1") == "1",
    reason="Tests hardware dÃ©sactivÃ©s par dÃ©faut"
)
def test_real_robot():
    """Test nÃ©cessitant robot physique."""
    pass
```

### ExÃ©cution par Marqueurs

```bash
# Tests unitaires rapides
pytest -m "unit and fast"

# Tests E2E
pytest -m e2e

# Exclure tests lents
pytest -m "not slow"

# Tests hardware (si activÃ©s)
SKIP_HARDWARE_TESTS=0 pytest -m "not skip"
```

---

## ğŸ“Š Couverture de Code

### Configuration

Fichier `.coveragerc` avec seuil strict :

```ini
[report]
fail_under = 50  # Seuil minimum 50%
show_missing = True
```

### GÃ©nÃ©ration Rapports

```bash
# Rapport terminal
pytest --cov=src/bbia_sim --cov-report=term-missing

# Rapport HTML (dans htmlcov/)
pytest --cov=src/bbia_sim --cov-report=html

# Rapport XML (pour CI)
pytest --cov=src/bbia_sim --cov-report=xml
```

### Coverage par Module

Modules prioritaires :

- `dashboard_advanced.py` : **0.00%** âš ï¸ (47 tests, objectif 70%+ non atteint)
- `vision_yolo.py` : **17.49%** âš ï¸ (objectif 50%+ non atteint, 32.51% manquants)
- `voice_whisper.py` : **75.83%** âœ… (47 tests crÃ©Ã©s, objectif 50%+ dÃ©passÃ©)
- `daemon/bridge.py` : **0.00%** âš ï¸ (34 tests, objectif 30%+ non atteint)
- `bbia_audio.py` : **87.76%** âœ…
- `bbia_emotions.py` : **81.71%** âœ…

### Objectifs Coverage

- **Global projet** : 50%+ (strict)
- **Modules core** : 70%+
- **Modules critiques** : 80%+

---

## ğŸ”„ Tests E2E

### ScÃ©narios Disponibles

1. **`test_e2e_face_detection_greeting.py`**
   - DÃ©tection visage â†’ suivi â†’ salutation
   - IntÃ©gration : Vision + Behavior

2. **`test_e2e_voice_interaction.py`**
   - Ã‰coute â†’ transcription â†’ rÃ©ponse LLM â†’ mouvement
   - IntÃ©gration : Voice + HuggingFace + Behavior

3. **`test_e2e_wake_up_sequence.py`**
   - RÃ©veil â†’ Ã©motion excited â†’ mouvement
   - IntÃ©gration : Behavior + Emotions + Robot API

4. **`test_e2e_full_interaction_loop.py`**
   - ScÃ©nario complet : dÃ©tection â†’ Ã©coute â†’ rÃ©ponse â†’ mouvement
   - IntÃ©gration : Tous modules

### ExÃ©cution Tests E2E

```bash
# Tous les E2E
pytest -m e2e

# E2E spÃ©cifique
pytest tests/e2e/test_e2e_face_detection_greeting.py

# E2E avec verbose
pytest -m e2e -v
```

### Mocks E2E

Tests E2E utilisent des **mocks** pour Ã©viter dÃ©pendances hardware :

```python
from unittest.mock import MagicMock, patch

# Mock robot API
mock_robot = MagicMock()
mock_robot.set_joint_pos.return_value = True

# Mock HuggingFace
with patch("bbia_sim.bbia_huggingface.BBIAHuggingFace") as mock_hf:
    mock_hf.chat.return_value = "RÃ©ponse simulÃ©e"
```

---

## ğŸ­ Mocks et Fixtures

### Fixtures Communes

```python
# Dans conftest.py
@pytest.fixture
def mock_robot():
    """Fixture robot API mockÃ©."""
    robot = MagicMock()
    robot.set_joint_pos.return_value = True
    robot.get_joint_pos.return_value = 0.0
    robot.step.return_value = True
    return robot
```

### Utilisation Mocks

```python
from unittest.mock import MagicMock, patch

def test_with_mock():
    # Mock module externe
    with patch("module.external_function", return_value="mocked"):
        result = function_under_test()
        assert result == "expected"
```

### Mock Robot API

Fichier `tests/reachy_mini_mock.py` fournit `ReachyMiniMock` :

```python
from tests.reachy_mini_mock import ReachyMiniMock

def test_with_mock_robot():
    robot = ReachyMiniMock(use_sim=True)
    assert robot.connect() is True
    # Tests sans robot physique
```

---

## âœ… Bonnes Pratiques

### 1. Noms de Tests Clairs

```python
# âŒ MAUVAIS
def test_1():
    pass

# âœ… BON
def test_vision_detects_faces_with_high_confidence():
    pass
```

### 2. Tests IsolÃ©s

```python
# âœ… Chaque test est indÃ©pendant
def test_emotion_set_happy():
    emotions = BBIAEmotions()
    emotions.set_emotion("happy", 0.8)
    assert emotions.current_emotion == "happy"
```

### 3. Arrange-Act-Assert

```python
def test_vision_tracks_object():
    # Arrange
    vision = BBIAVision()
    vision.objects_detected = [{"name": "livre"}]
    
    # Act
    result = vision.track_object("livre")
    
    # Assert
    assert result is True
    assert vision.tracking_active is True
```

### 4. Tests DÃ©terministes

```python
# âœ… Utiliser mocks pour donnÃ©es prÃ©visibles
vision.objects_detected = [{"name": "objet", "confidence": 0.9}]

# âŒ Ã‰viter dÃ©pendances externes variables
result = vision.scan_environment()  # Peut varier selon environnement
```

### 5. Documentation Tests

```python
def test_complex_scenario():
    """Test scÃ©nario complexe: dÃ©tection â†’ tracking â†’ action.
    
    VÃ©rifie que BBIA peut dÃ©tecter un objet, le suivre,
    et dÃ©clencher une action appropriÃ©e.
    """
    # ...
```

---

## ğŸ”’ Protection ExÃ©cution SimultanÃ©e

### SystÃ¨me de Lock

Fichier `tests/conftest.py` implÃ©mente un systÃ¨me de lock pour Ã©viter l'exÃ©cution simultanÃ©e de plusieurs instances pytest (surchauffe RAM).

**FonctionnalitÃ©s** :
- Lock fichier avec `fcntl`
- DÃ©tection locks orphelins
- Timeout automatique (5 minutes)
- Messages d'erreur clairs

**Si lock actif** :
```
âŒ Tests dÃ©jÃ  en cours d'exÃ©cution !
   Processus PID: 12345
   Lock acquis il y a: 10.5s
   
ğŸ’¡ Solutions:
   1. Attendre la fin de l'autre processus
   2. VÃ©rifier: ps aux | grep 12345
   3. Si processus mort: rm .pytest.lock
```

---

## ğŸš¦ CI/CD

### GitHub Actions

Tests exÃ©cutÃ©s automatiquement dans CI avec :

- **Linting** : Ruff, Black
- **Type checking** : MyPy
- **Security** : Bandit
- **Tests** : pytest avec coverage
- **Benchmarks** : Performance tests

### Variables Environnement CI

```yaml
# .github/workflows/ci.yml
env:
  CI: "true"
  BBIA_DISABLE_AUDIO: "1"
  BBIA_DISABLE_VISION: "0"
  SKIP_HARDWARE_TESTS: "1"
```

### Seuils Adaptatifs

Tests performance adaptent leurs seuils en CI :

```python
is_ci = os.environ.get("CI", "false").lower() == "true"
max_p50 = 200.0 if is_ci else 100.0  # TolÃ©rance 2x en CI
```

---

## ğŸ”§ DÃ©pannage

### ProblÃ¨mes Courants

#### 1. Tests Ã‰chouent Localement mais Passent en CI

**Cause** : DiffÃ©rences environnement (hardware, dÃ©pendances)  
**Solution** : VÃ©rifier variables environnement, utiliser mocks

#### 2. Lock Persistant

```bash
# Supprimer lock manuellement
rm .pytest.lock

# VÃ©rifier processus actif
ps aux | grep pytest
```

#### 3. Coverage Trop Bas

**Solution** :
1. Identifier modules non couverts : `pytest --cov-report=term-missing`
2. CrÃ©er tests manquants
3. VÃ©rifier `.coveragerc` exclusions

#### 4. Tests Lents

**Solution** :
- Utiliser `-m "not slow"` pour dÃ©veloppement rapide
- Optimiser tests lents (rÃ©duire durÃ©es, itÃ©rations)
- Mode parallÃ¨le : `pytest -n auto`

#### 5. Import Errors

**Solution** :
```bash
# VÃ©rifier PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)/src"

# Installer dÃ©pendances
pip install -e .[dev,test]
```

---

## ğŸ“š Ressources

- **pytest Docs** : https://docs.pytest.org/
- **Coverage.py** : https://coverage.readthedocs.io/
- **unittest.mock** : https://docs.python.org/3/library/unittest.mock.html

---

## ğŸ¯ Prochaines Ã‰tapes

1. AmÃ©liorer coverage modules restants
2. Ajouter tests E2E supplÃ©mentaires
3. Documentation tests spÃ©cifiques par module
4. Benchmarks performance automatisÃ©s

---

**DerniÃ¨re mise Ã  jour** : Oct / Nov. 2025

