# üß† Guide NLP et SmolVLM2 - BBIA

**Date** : 8 D√©cembre 2025
**Version** : 1.4.0  
**üìö [FAQ](../getting-started/troubleshooting.md)** | **üìä [√âtat actuel](../quality/audits/RESUME_ETAT_ACTUEL_BBIA.md)** | **üîç [Comparaison](../quality/audits/COMPARAISON_APP_CONVERSATION_OFFICIELLE.md)**

---

## üìã Table des Mati√®res

1. [D√©tection NLP avec sentence-transformers](#d√©tection-outils-avec-nlp)
2. [Extraction param√®tres NER](#extraction-param√®tres-ner)
3. [Vision SmolVLM2](#smolvlm2-vision-enrichie)
4. [VAD (Voice Activity Detection)](#vad-voice-activity-detection)
5. [Whisper Streaming](#whisper-streaming)
6. [Exemples d'utilisation](#exemples-dutilisation)

## üîÑ Architecture NLP/Vision BBIA

```mermaid
flowchart TB
    USER[Utilisateur] --> MSG[Message Texte/Voice]
    
    MSG --> NLP{NLP Detection}
    NLP -->|Confiance > 0.7| TOOL[D√©tection Outil]
    NLP -->|Confiance < 0.7| KEYWORD[Mots-cl√©s Fallback]
    
    TOOL --> EXEC[Execute Tool]
    KEYWORD --> EXEC
    
    EXEC --> NER{NER Extraction?}
    NER -->|Oui| EXTRACT[Extraire Angles/Intensit√©s]
    NER -->|Non| PARAMS[Param√®tres Par D√©faut]
    
    EXTRACT --> ROBOT[RobotAPI]
    PARAMS --> ROBOT
    
    subgraph "Vision"
        IMG[Image Camera] --> VLM[SmolVLM2]
        VLM --> DESC[Description Riche]
        DESC --> CHAT[Chat LLM]
    end
    
    CHAT --> RESPONSE[R√©ponse Utilisateur]
    ROBOT --> ACTION[Action Robot]
    
    style TOOL fill:#90EE90
    style VLM fill:#87CEEB
    style NER fill:#FFD700

```

---

## üéØ D√©tection Outils avec NLP

### Description

BBIA utilise `sentence-transformers` pour une d√©tection robuste des intentions utilisateur par similarit√© s√©mantique, au lieu de simples mots-cl√©s.

```mermaid
sequenceDiagram
    participant User as Utilisateur
    participant HF as BBIAHuggingFace
    participant NLP as sentence-transformers
    participant Tools as BBIATools
    participant Robot as RobotAPI
    
    User->>HF: "peux-tu regarder √† droite"
    HF->>NLP: encode("peux-tu regarder √† droite")
    NLP-->>HF: Embedding vector
    HF->>HF: cosine_similarity(embeddings)
    HF-->>HF: Outil: "move_head" (conf: 0.85)
    HF->>Tools: execute_tool("move_head", params)
    Tools->>Robot: move_head(direction="right")
    Robot-->>User: ‚úÖ T√™te orient√©e √† droite

```

**Avantages** :

- ‚úÖ D√©tection robuste m√™me avec variantes de phrases
- ‚úÖ Support synonymes et expressions naturelles
- ‚úÖ Score de confiance pour chaque d√©tection
- ‚úÖ Fallback automatique vers mots-cl√©s si NLP indisponible

### Configuration

Le NLP est activ√© automatiquement si `sentence-transformers` est install√© :

```bash
pip install sentence-transformers

```

**Mod√®le utilis√©** : `sentence-transformers/all-MiniLM-L6-v2` (gratuit, Hugging Face)

### Utilisation

La d√©tection NLP est int√©gr√©e automatiquement dans `BBIAHuggingFace.chat()` :

```python
from src.bbia_sim.bbia_huggingface import BBIAHuggingFace
from src.bbia_sim.bbia_tools import BBIATools

# Initialiser avec outils
tools = BBIATools(robot_api=robot_api)
hf = BBIAHuggingFace(tools=tools)

# Chat avec d√©tection NLP automatique
response = hf.chat("peux-tu orienter ta t√™te vers la droite", enable_tools=True)
# ‚Üí D√©tecte automatiquement "move_head" via NLP

```

### M√©thode manuelle

Pour d√©tecter manuellement un outil :

```python
result = hf._detect_tool_with_nlp("tourne la t√™te √† gauche")
if result:
    tool_name, confidence = result
    logging.info(f"Outil: {tool_name}, Confiance: {confidence:.2f}")

```

### Seuil de confiance

Le seuil par d√©faut est **0.6** (ajustable dans `_detect_tool_with_nlp()`).

---

## üìê Extraction Param√®tres NER

### Description

BBIA extrait automatiquement des param√®tres num√©riques depuis les phrases naturelles :

- **Angles** : "30 degr√©s", "pi/4 radians", "50%"
- **Intensit√©s** : "l√©g√®rement", "beaucoup", "√† 75%"

### Flux d'Extraction

```mermaid
flowchart LR
    MSG[Message Utilisateur] --> EXTRACT{Extraction}
    
    EXTRACT --> ANGLE{Angle?}
    EXTRACT --> INTENSITY{Intensit√©?}
    
    ANGLE -->|Oui| REG_ANGLE[Regex Angle]
    REG_ANGLE --> DEG[30 degr√©s<br/>‚Üí 30.0]
    REG_ANGLE --> RAD[pi/4 radians<br/>‚Üí 45.0¬∞]
    REG_ANGLE --> PCT[50%<br/>‚Üí 45.0¬∞]
    
    INTENSITY -->|Oui| KEYWORD[Keywords]
    KEYWORD --> LIGHT[l√©g√®rement<br/>‚Üí 0.2]
    KEYWORD --> MEDIUM[mod√©r√©ment<br/>‚Üí 0.5]
    KEYWORD --> STRONG[beaucoup<br/>‚Üí 0.8]
    KEYWORD --> MAX[compl√®tement<br/>‚Üí 1.0]
    
    DEG --> PARAMS[Param√®tres Finalis√©s]
    RAD --> PARAMS
    PCT --> PARAMS
    LIGHT --> PARAMS
    MEDIUM --> PARAMS
    STRONG --> PARAMS
    MAX --> PARAMS
    
    PARAMS --> EXEC[Ex√©cuter Action]
    
    style EXTRACT fill:#FFD700
    style PARAMS fill:#90EE90

```

### Exemples

#### Extraction angles

```python
# Automatique dans _execute_detected_tool()
hf.chat("tourne la t√™te de 45 degr√©s vers la droite", enable_tools=True)
# ‚Üí Extraie: angle=45¬∞, direction="right"

hf.chat("√† pi/2 radians vers le haut", enable_tools=True)
# ‚Üí Extraie: angle=90¬∞ (converti depuis radians)

hf.chat("√† 75% vers la gauche", enable_tools=True)
# ‚Üí Extraie: angle=67.5¬∞ (75% de 90¬∞)

```

#### Extraction intensit√©s

```python
hf.chat("l√©g√®rement vers la droite", enable_tools=True)
# ‚Üí Intensit√©: 0.2

hf.chat("beaucoup plus haut", enable_tools=True)
# ‚Üí Intensit√©: 0.8

hf.chat("√† 50% vers le bas", enable_tools=True)
# ‚Üí Intensit√©: 0.5

```

### Mots-cl√©s intensit√©

| Mots-cl√©s | Intensit√© |
|-----------|-----------|
| "l√©g√®rement", "un peu" | 0.2 |
| "mod√©r√©ment", "normalement" | 0.5 |
| "beaucoup", "fortement" | 0.8 |
| "compl√®tement", "totalement" | 1.0 |

---

## üñºÔ∏è Vision SmolVLM2

### Description

**SmolVLM2** et **Moondream2** sont des mod√®les de vision gratuits et l√©gers (alternatives √† gpt-realtime) pour descriptions d'images plus riches.

### Configuration

Ajout dans `model_configs` :

```python
model_configs["multimodal"] = {
    # ... autres mod√®les
    "smolvlm": {
        "repo_id": "HuggingFaceTB/SmolVLM",
        "model_name": "smolvlm-2b",
    },
    "moondream2": {
        "repo_id": "vikhyatk/moondream2",
        "revision": "2025-08-26",
    },
}

```

### Utilisation

Activer SmolVLM2 pour description d'image :

```python
from src.bbia_sim.bbia_huggingface import BBIAHuggingFace

hf = BBIAHuggingFace()

# Charger mod√®le SmolVLM2
hf.enable_multimodal_model("smolvlm")

# D√©crire image
description = hf.describe_image(image_path, model_name="smolvlm")
logging.info(description)

```

### Mod√®les disponibles

| Mod√®le | Repo | Taille | Description |
|--------|------|--------|-------------|
| **SmolVLM** | `HuggingFaceTB/SmolVLM` | ~2B | Vision + langage |
| **Moondream2** | `vikhyatk/moondream2` | ~1.6B | Vision descriptif |

---

## üé§ VAD (Voice Activity Detection)

### Description

**VAD** d√©tecte automatiquement quand l'utilisateur parle, permettant l'activation automatique de la conversation (sans bouton).

**Mod√®le utilis√©** : `silero/vad` (gratuit, Hugging Face)

### Utilisation

#### Activation VAD

```python
from src.bbia_sim.voice_whisper import WhisperSTT

# Initialiser avec VAD activ√© (par d√©faut)
whisper = WhisperSTT(enable_vad=True)

# D√©tecter parole depuis chunk audio
import numpy as np
audio_chunk = np.random.rand(16000).astype(np.float32)
is_speech = whisper.detect_speech_activity(audio_chunk)

```

#### Transcription avec VAD automatique

```python
# Enregistrer avec d√©tection automatique de fin (silence)
text = whisper.transcribe_microphone_with_vad(
    duration=10.0,  # Dur√©e max
    silence_threshold=0.3  # Arr√™t apr√®s 0.3s de silence
)

```

### D√©sactivation VAD

```python
whisper = WhisperSTT(enable_vad=False)
# ‚Üí Toujours consid√®re comme parole (pas de d√©tection)

```

---

## üîÑ Whisper Streaming

### Description

**Whisper streaming** permet une transcription en continu avec latence r√©duite (500ms vs 1-2s).

### Utilisation

#### Streaming basique

```python
from src.bbia_sim.voice_whisper import WhisperSTT

whisper = WhisperSTT()
text = whisper.transcribe_streaming(
    chunk_duration=0.5,  # Chunks de 500ms
    max_duration=30.0  # Dur√©e max
)

```

#### Streaming avec callback

```python
def on_chunk_transcribed(text: str, duration: float):
    logging.info(f"[{duration:.1f}s] {text}")

text = whisper.transcribe_streaming(
    callback=on_chunk_transcribed,
    chunk_duration=0.5
)

```

### Optimisations

- **Buffer contexte** : Garde 3 chunks pour meilleure pr√©cision
- **Latence r√©duite** : Chunks de 500ms au lieu de 3s
- **VAD int√©gr√©** : D√©tection silence optionnelle

---

## üìù Exemples d'Utilisation

### Exemple 1 : Conversation avec NLP automatique

```python
from src.bbia_sim.bbia_huggingface import BBIAHuggingFace
from src.bbia_sim.bbia_tools import BBIATools

tools = BBIATools(robot_api=robot_api)
hf = BBIAHuggingFace(tools=tools)

# Phrases vari√©es - toutes d√©tect√©es via NLP
messages = [
    "peux-tu orienter ta t√™te vers la droite",
    "tourne-toi vers la gauche",
    "fais une danse",
    "capture une image de l'environnement",
]

for msg in messages:
    response = hf.chat(msg, enable_tools=True)
    logging.info(f"Utilisateur: {msg}")
    logging.info(f"BBIA: {response}\n")

```

### Exemple 2 : Extraction param√®tres avanc√©e

```python
# Extraction angle + direction
hf.chat("tourne la t√™te de 30 degr√©s vers la droite", enable_tools=True)
# ‚Üí Param√®tres extraits: angle=30¬∞, direction="right", intensity=0.33

# Extraction intensit√©
hf.chat("l√©g√®rement vers la gauche", enable_tools=True)
# ‚Üí Param√®tres: direction="left", intensity=0.2

```

### Exemple 3 : VAD + Streaming

```python
from src.bbia_sim.voice_whisper import WhisperSTT

whisper = WhisperSTT(enable_vad=True)

# Streaming avec VAD automatique
def on_transcription(text: str, duration: float):
    if text:
        logging.info(f"Transcription: {text}")

text = whisper.transcribe_streaming(
    callback=on_transcription,
    chunk_duration=0.5,
    max_duration=10.0
)

```

### Exemple 4 : SmolVLM2 pour description images

```python
from src.bbia_sim.bbia_huggingface import BBIAHuggingFace

hf = BBIAHuggingFace()

# Activer SmolVLM2
hf.enable_multimodal_model("smolvlm")

# D√©crire image
description = hf.describe_image("image.jpg", model_name="smolvlm")
logging.info(f"Description: {description}")

```

---

## ‚öôÔ∏è Configuration

### Variables d'environnement

```bash
# D√©sactiver audio (CI/headless)
export BBIA_DISABLE_AUDIO=1

# Mod√®le Whisper
export WHISPER_MODEL_SIZE=tiny  # tiny, base, small, medium, large

```

### Dependencies

```toml
# pyproject.toml
dependencies = [
    "sentence-transformers>=2.2.0",  # NLP
    "transformers>=4.30.0",  # VAD, SmolVLM2
    "openai-whisper>=20231117",  # Streaming
]

```

---

## üêõ D√©pannage

### NLP non disponible

**Sympt√¥me** : Fallback vers mots-cl√©s seulement

**Solution** :

```bash
pip install sentence-transformers scikit-learn

```

### VAD erreur chargement

**Sympt√¥me** : `‚ö†Ô∏è Impossible de charger VAD`

**Solution** :

```bash
pip install transformers torch

```

### SmolVLM2 lent

**Sympt√¥me** : Chargement tr√®s lent

**Solution** : Utiliser `moondream2` (plus l√©ger) ou d√©sactiver si non n√©cessaire

---

## üìö R√©f√©rences

- **sentence-transformers** : https://www.sbert.net/
- **SmolVLM** : https://huggingface.co/HuggingFaceTB/SmolVLM
- **Moondream2** : https://huggingface.co/vikhyatk/moondream2
- **Silero VAD** : https://huggingface.co/silero/vad

---

**Derni√®re mise √† jour** : 8 D√©cembre 2025

---

## üéØ Navigation

**Retour √†** : [README Documentation](../README.md)  
**Voir aussi** : [Guide Chat BBIA](GUIDE_CHAT_BBIA.md) ‚Ä¢ [Guide de D√©marrage](GUIDE_DEMARRAGE.md) ‚Ä¢ [Index Th√©matique](../reference/INDEX_THEMATIQUE.md)
