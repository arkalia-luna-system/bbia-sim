# Intelligence conversationnelle LLM - guide complet

**Date :** 22 novembre 2025  
**Derni√®re mise √† jour :** 22 novembre 2025  
**Objectif :** LLM conversationnel l√©ger (Phi-2/TinyLlama) pour conversations intelligentes

---

## Objectif

Remplacer les r√©ponses bas√©es sur r√®gles par un LLM conversationnel qui comprend le contexte et g√©n√®re des r√©ponses naturelles.

**Avant :** R√®gles + sentiment analysis (limit√©)  
**Apr√®s :** BBIAChat avec Phi-2/TinyLlama (conversations naturelles avec contexte, personnalit√©s, √©motions)

---

## Installation

### Pr√©requis

```bash
# Activer venv
source venv/bin/activate

# Installer d√©pendances LLM (d√©j√† dans pyproject.toml)
pip install transformers accelerate sentencepiece torch

# Optionnel : optimisations Apple Silicon
# (acc√©l√©ration automatique via MPS si disponible)

```

---

## Utilisation

### BBIAChat (Recommand√© - LLM l√©ger)

**BBIAChat** est maintenant int√©gr√© automatiquement dans `BBIAHuggingFace` :

```python
from bbia_sim.bbia_huggingface import BBIAHuggingFace

# Initialiser BBIA (BBIAChat charg√© automatiquement)
bbia = BBIAHuggingFace()

# Conversation intelligente (utilise Phi-2 ou TinyLlama automatiquement)
response = bbia.chat("Bonjour, comment √ßa va ?")
# ‚Üí Utilise BBIAChat avec LLM l√©ger si disponible

```

**Avantages BBIAChat :**
- ‚úÖ LLM l√©ger (Phi-2 ~5GB, TinyLlama ~2GB) - Compatible RPi 5
- ‚úÖ 5 personnalit√©s (friendly, professional, playful, calm, enthusiastic)
- ‚úÖ D√©tection et ex√©cution d'actions robot
- ‚úÖ Int√©gration √©motions BBIA
- ‚úÖ Apprentissage pr√©f√©rences utilisateur
- ‚úÖ Historique conversationnel (10 messages)

### Utilisation automatique

Le LLM s'active automatiquement dans `ConversationBehavior` si disponible :

```python
from bbia_sim.bbia_behavior import ConversationBehavior

# Le comportement utilise automatiquement le LLM si activ√©
behavior = ConversationBehavior()
# Les conversations utiliseront Mistral 7B si disponible

```

### Conversation avec LLM

```python
from bbia_sim.bbia_huggingface import BBIAHuggingFace

bbia = BBIAHuggingFace()

# Activer LLM
bbia.enable_llm_chat("mistralai/Mistral-7B-Instruct-v0.2")

# Conversation intelligente
response = bbia.chat("Bonjour, comment √ßa va ?")
logging.info(response)  # R√©ponse naturelle g√©n√©r√©e par Mistral 7B

# Avec contexte (utilise historique)
response2 = bbia.chat("Tu te rappelles ce que je viens de dire ?", use_context=True)
logging.info(response2)  # R√©ponse qui r√©f√©rence la conversation pr√©c√©dente

```

### D√©sactiver LLM (Lib√©rer M√©moire)

```python
# D√©sactiver pour lib√©rer ~14 GB RAM
bbia.disable_llm_chat()

```

---

## Personnalit√©s BBIA

BBIAChat supporte 5 personnalit√©s distinctes :

```python
from bbia_sim.bbia_chat import BBIAChat

chat = BBIAChat()

# Personnalit√© amicale (d√©faut)
chat.set_personality("friendly")
response = chat.chat("Salut !")
# ‚Üí R√©ponse chaleureuse et empathique

# Personnalit√© professionnelle
chat.set_personality("professional")
response = chat.chat("Salut !")
# ‚Üí R√©ponse formelle et pr√©cise

# Personnalit√© joueuse
chat.set_personality("playful")
response = chat.chat("Salut !")
# ‚Üí R√©ponse d√©contract√©e et humoristique

# Personnalit√© calme
chat.set_personality("calm")
response = chat.chat("Salut !")
# ‚Üí R√©ponse sereine et apaisante

# Personnalit√© enthousiaste
chat.set_personality("enthusiastic")
response = chat.chat("Salut !")
# ‚Üí R√©ponse √©nergique et positive

```

---

## Comparaison avant/apr√®s

### Avant (r√®gles + sentiment)

```python
# R√©ponses bas√©es sur r√®gles
User: "Bonjour"
BBIA: "Bonjour ! Ravi de vous revoir ! Comment allez-vous aujourd'hui ?"
# ‚Üí Variantes limit√©es (6-8 r√©ponses possibles)

```

### Apr√®s (LLM Mistral 7B)

```python
# R√©ponses g√©n√©r√©es intelligemment
User: "Bonjour"
BBIA: "Bonjour ! Content de te revoir. Comment s'est pass√©e ta journ√©e ?"
# ‚Üí R√©ponses naturelles, vari√©es, contextuelles

```

Avantages LLM :

- r√©ponses naturelles et vari√©es
- compr√©hension du contexte
- r√©f√©rence √† l'historique conversationnel
- g√©n√©ration cr√©ative selon personnalit√©
- adaptabilit√© √† n'importe quelle question

---

## Configuration

### Mod√®les disponibles dans BBIAChat

1. **Phi-2 2.7B** (recommand√© - BBIAChat)
   - Qualit√© : excellente
   - Fran√ßais : tr√®s bon
   - Taille : ~5GB RAM (compatible RPi 5)
   - Support MPS/CUDA : oui
   - Vitesse : ~1-2 secondes/r√©ponse

2. **TinyLlama 1.1B** (fallback - BBIAChat)
   - Qualit√© : bonne
   - Fran√ßais : bon
   - Taille : ~2GB RAM (ultra-l√©ger)
   - Support MPS/CUDA : oui
   - Vitesse : <1 seconde/r√©ponse

3. **Mistral 7B Instruct** (optionnel - BBIAHuggingFace)
   - Qualit√© : excellente
   - Fran√ßais : tr√®s bon
   - Taille : ~14GB RAM
   - Support MPS : oui
   - Vitesse : ~1-3 secondes/r√©ponse

### Configuration personnalis√©e

```python
# Charger mod√®le personnalis√©
bbia.enable_llm_chat("meta-llama/Llama-3-8B-Instruct")

# Ou utiliser mod√®le local
bbia.enable_llm_chat("./models/mistral-7b-instruct")

```

---

## Param√®tres g√©n√©ration

### Personnalisation r√©ponses

Le LLM utilise ces param√®tres par d√©faut :

- `max_new_tokens=150` : Limite longueur r√©ponse
- `temperature=0.7` : Cr√©ativit√© mod√©r√©e
- `top_p=0.9` : Nucleus sampling
- `do_sample=True` : G√©n√©ration vari√©e

Pour modifier (dans `bbia_huggingface.py`) :

```python
outputs = self.chat_model.generate(
    **inputs,
    max_new_tokens=200,  # R√©ponses plus longues
    temperature=0.9,     # Plus cr√©atif
    top_p=0.95,
    do_sample=True,
)

```

---

## Performance

### Ressources n√©cessaires

**Mistral 7B Instruct :**

- RAM : ~14 GB
- Premier chargement : 1-2 minutes
- G√©n√©ration : ~1-3 secondes/r√©ponse
- Disk : ~14GB (cache mod√®le)

**Optimisations :**

- Apple Silicon (M1/M2/M3) : Acc√©l√©ration MPS automatique
- CUDA : Acc√©l√©ration GPU si disponible
- Quantization : R√©duire RAM √† ~8GB (qualit√© l√©g√®rement r√©duite)

### Gestion m√©moire

```python
# Si m√©moire limit√©e, charger seulement quand n√©cessaire
if need_intelligent_chat:
    bbia.enable_llm_chat()
    response = bbia.chat(user_message)
    bbia.disable_llm_chat()  # Lib√©rer m√©moire apr√®s

```

---

## üß™ Tests

```bash
# Tester LLM conversationnel
cd /Volumes/T7/bbia-reachy-sim
source venv/bin/activate
python -c "
from bbia_sim.bbia_huggingface import BBIAHuggingFace

bbia = BBIAHuggingFace()
    logging.info('Chargement LLM...')
if bbia.enable_llm_chat():
    logging.info('LLM charg√©')
    response = bbia.chat('Bonjour, qui es-tu ?')
    logging.info(f'R√©ponse: {response}')
else:
    logging.warning('LLM non charg√©')
"

```

---

## Limitations

1. **M√©moire :**
   - Requiert ~14GB RAM (Mistral 7B)
   - Peut √™tre lourd sur machines limit√©es

2. **Vitesse :**
   - Premier chargement : 1-2 minutes
   - G√©n√©ration : 1-3 secondes/r√©ponse (CPU) ou <1s (MPS/CUDA)

3. **Qualit√© :**
   - Excellent fran√ßais mais parfois g√©n√®re en anglais
   - R√©ponses parfois trop longues (limit√© √† 150 tokens)

4. **Fallback :**
   - Si LLM √©choue, fallback automatique vers r√©ponses enrichies (r√®gles)

---

## Migration automatique

Le code existant **fonctionne sans modification** :

```python
# Code existant continue de fonctionner
from bbia_sim.bbia_huggingface import BBIAHuggingFace

bbia = BBIAHuggingFace()
response = bbia.chat("Bonjour")
# ‚Üí Utilise r√©ponses enrichies (r√®gles) par d√©faut

# Activer LLM si souhait√©
bbia.enable_llm_chat()
response = bbia.chat("Bonjour")
# ‚Üí Utilise Mistral 7B pour r√©ponse intelligente

```

---

## R√©f√©rences

- **Mistral 7B :** https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
- **Llama 3 :** https://huggingface.co/meta-llama/Llama-3-8B-Instruct
- **Transformers :** https://huggingface.co/docs/transformers

---

## Prochaines √©tapes

1. OK LLM int√©gr√© dans `BBIAHuggingFace.chat()`
2. OK Activation optionnelle via `enable_llm_chat()`
3. ‚è≥ Tests unitaires
4. ‚è≥ Optimisation m√©moire (quantization optionnelle)
5. ‚è≥ Support streaming (r√©ponses au fil de l'eau)

---

**Status :** ‚úÖ **TERMIN√â** (19 novembre 2025) - BBIAChat avec Phi-2/TinyLlama, 5 personnalit√©s, √©motions, pr√©f√©rences

---

## üéØ Navigation

**Retour √†** : [README Documentation](../README.md)  
**Voir aussi** : [Modules IA](modules.md) ‚Ä¢ [Guide Chat BBIA](../guides/GUIDE_CHAT_BBIA.md) ‚Ä¢ [Index Th√©matique](../reference/INDEX_THEMATIQUE.md)
