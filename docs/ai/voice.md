# üé§ Analyse Compl√®te : Voix & Intelligence BBIA

**Derni√®re mise √† jour** : 26 Janvier 2026  
**Auteur :** Analyse technique  
**Objectif :** Identifier blocages macOS, solutions voix alternatives, et √©tat intelligence BBIA

---

## üìã Table des Mati√®res

1. [√âtat Actuel : Voix BBIA](#-√©tat-actuel--voix-bbia)
2. [Solutions Alternatives : G√©n√©rateurs de Voix](#-solutions-alternatives--g√©n√©rateurs-de-voix)
3. [√âtat Actuel : Intelligence BBIA](#-√©tat-actuel--intelligence-bbia)
4. [Recommandations Propos√©es](#-recommandations-propos√©es)
5. [Plan d'Impl√©mentation](#-plan-dimpl√©mentation)
6. [R√©sum√© des Blocages](#-r√©sum√©-des-blocages)
7. [Navigation](#-navigation)

---

## üîÑ Flux Voix BBIA (TTS/STT)

```mermaid
flowchart TB
    subgraph "Speech-to-Text (STT)"
        Audio[Audio Input<br/>Microphone]
        Audio --> Whisper{Whisper disponible?}
        Whisper -->|Oui| WhisperSTT[Whisper STT<br/>Haute qualit√©]
        Whisper -->|Non| GoogleSTT[Google API<br/>Fallback]
        WhisperSTT --> Text[Texte transcrit]
        GoogleSTT --> Text
    end
    
    subgraph "Text-to-Speech (TTS)"
        TextInput[Texte √† synth√©tiser]
        TextInput --> Backend{Backend TTS}
        Backend -->|Coqui| CoquiTTS[Coqui TTS<br/>Pitch + √âmotion]
        Backend -->|Piper| PiperTTS[Piper TTS<br/>L√©ger]
        Backend -->|Fallback| Pyttsx3[pyttsx3<br/>Syst√®me macOS]
        CoquiTTS --> AudioOut[Audio g√©n√©r√©]
        PiperTTS --> AudioOut
        Pyttsx3 --> AudioOut
    end
    
    subgraph "Int√©gration Robot"
        AudioOut --> Robot[Reachy Mini<br/>media.speaker]
        Robot --> Playback[Lecture audio]
    end
    
    Text -.-> TextInput
```

---

## üìä √âtat actuel : Voix BBIA

### Impl√©mentation actuelle (`bbia_voice.py`)

**Technologie** : `pyttsx3` (wrapper autour des voix syst√®me macOS)

**Blocages macOS identifi√©s (confirm√©s par test) :**

‚ùå **Pitch non contr√¥lable**

- **Message d'erreur** : `"Pitch adjustment not supported when using NSSS"`
- **Impact** : Impossible de modifier la tonalit√©/hauteur de voix

‚ùå **Contr√¥le √©motionnel inexistant**

- `pyttsx3` ne permet pas d'ajuster l'√©motion (joyeux, triste, excit√©, etc.)
- Seule la vitesse et le volume sont contr√¥lables

‚ùå **Voix forc√©e √† "Am√©lie"**

- Actuellement, le code force l'utilisation d'Am√©lie
- 197 voix disponibles sur macOS mais non exploitables avec pyttsx3

‚ùå **Vitesse limit√©e**

- Rate = 170 (fixe)
- Pas de variation dynamique selon le contexte

---

## üîç Solutions alternatives : G√©n√©rateurs de voix avanc√©s

### Option 1 : ‚≠ê Coqui TTS (Recommand√©)

**Repository** : `https://github.com/coqui-ai/TTS`

**Avantages :**

- ‚úÖ Contr√¥le pitch/tonalit√© complet
- ‚úÖ Contr√¥le √©motionnel (happy, sad, excited, etc.)
- ‚úÖ Multi-langues (fran√ßais inclus)
- ‚úÖ Voix pr√©-entra√Æn√©es disponibles
- ‚úÖ Installation simple : `pip install TTS`
- ‚úÖ Support Apple Silicon (MPS)

**Fonctionnalit√©s cl√©s :**

```python
# Exemple d'utilisation
from TTS.api import TTS

tts = TTS("tts_models/fr/css10/vits")
tts.tts_to_file(
    text="Bonjour, je suis BBIA",
    file_path="output.wav",
    speaker_wav="reference_voice.wav",  # Clonage voix optionnel
    emotion="happy",  # √âmotion contr√¥lable
    pitch=0.2  # Pitch contr√¥lable
)

```

**Mod√®les fran√ßais disponibles :**

- `tts_models/fr/css10/vits` - Voix fran√ßaise de qualit√©
- `tts_models/multilingual/multi-dataset/your_tts` - Multi-langues + clonage

**Installation :**

```bash
pip install TTS
# Optionnel : pour voix haute qualit√©
pip install TTS[all]

```

---

### Option 2 : Piper TTS (L√©ger et rapide)

**Repository** : `https://github.com/rhasspy/piper`

**Avantages :**

- ‚úÖ Tr√®s l√©ger (mod√®les ~10-20MB)
- ‚úÖ Rapide (temps r√©el garanti)
- ‚úÖ Contr√¥le pitch via SSML
- ‚úÖ Installation : `pip install piper-tts`

**Limitations :**

- ‚ö†Ô∏è Pas de contr√¥le √©motionnel direct
- ‚ö†Ô∏è Qualit√© vocale inf√©rieure √† Coqui TTS

**Fonctionnalit√©s :**

```python
from piper import PiperVoice

voice = PiperVoice.load("fr_FR-am√©lie-medium")
audio = voice.synthesize(
    "Bonjour",
    speaker_id=0,
    length_scale=1.0,  # Vitesse
    noise_scale=0.667,  # Variation
    noise_w=0.8
)

```

---

### Option 3 : XTTS v2 (Clonage voix avanc√©)

**Repository** : `https://github.com/coqui-ai/TTS` (partie XTTS)

**Avantages :**

- ‚úÖ Clonage voix avec 3 secondes d'audio seulement
- ‚úÖ Contr√¥le √©motionnel complet
- ‚úÖ Multi-langues
- ‚úÖ Qualit√© professionnelle

**Limitations :**

- ‚ö†Ô∏è Plus lourd (mod√®le ~1.5GB)
- ‚ö†Ô∏è Plus lent (g√©n√©ration ~2-5 secondes)

**Utilisation :**

```python
from TTS.api import TTS

tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2")
tts.tts_to_file(
    text="Bonjour",
    file_path="output.wav",
    speaker_wav="reference_voice.wav",  # Clone voix
    language="fr",
    emotion="happy"
)

```

---

### Option 4 : **Bark** (Voix naturelles avec bruitages)

**Repository :** `https://github.com/suno-ai/bark`

**Avantages :**

- ‚úÖ Voix tr√®s naturelles
- ‚úÖ Support bruitages (rire, soupir, etc.)
- ‚úÖ Contr√¥le style vocal

**Limitations :**

- ‚ö†Ô∏è Pas de fran√ßais natif (anglais principalement)
- ‚ö†Ô∏è Tr√®s lent (g√©n√©ration ~10-30 secondes)
- ‚ö†Ô∏è Mod√®le tr√®s lourd (~1GB)

---

## üß† √âTAT ACTUEL : Intelligence BBIA

### Architecture Intelligence Actuelle

#### 1. **Intelligence Verbale (Conversation)**

**Module :** `bbia_huggingface.py` ‚Üí `BBIAHuggingFace.chat()`

**Probl√®me identifi√© :**

- ‚úÖ Analyse sentiment : `cardiffnlp/twitter-roberta-base-sentiment-latest`
- ‚úÖ Analyse √©motion : `j-hartmann/emotion-english-distilroberta-base`
- ‚ùå **PAS de vrai LLM conversationnel** - Utilise seulement des r√®gles et sentiment analysis

**Fonctionnement actuel :**

```python
# Actuellement dans bbia_huggingface.py :
def chat(self, user_message: str) -> str:
    # 1. Analyse sentiment (‚úÖ)
    sentiment = self.analyze_sentiment(user_message)

    # 2. G√©n√©ration r√©ponse (‚ùå R√®gles basiques)
    response = self._generate_response_from_sentiment(sentiment)
    # ‚Üí Pas de vrai mod√®le de langage !

```

**Ce qui manque :**

- ‚ùå LLM pr√©-entra√Æn√© pour conversation naturelle
- ‚ùå Compr√©hension contextuelle avanc√©e
- ‚ùå G√©n√©ration de texte intelligente

---

#### 2. Intelligence √©motionnelle

**Module** : `bbia_emotions.py` + `bbia_integration.py`

**√âtat actuel :**

- ‚úÖ 12 √©motions BBIA d√©finies
- ‚úÖ Mapping vers 6 √©motions SDK Reachy Mini
- ‚úÖ Application √©motions au robot via `set_emotion()` ou `goto_target()`
- ‚úÖ Transitions fluides avec interpolation minjerk
- ‚úÖ Dur√©e adaptative selon intensit√©

**Fonctionnalit√©s :**

```python
# Dans bbia_integration.py
def apply_emotion_to_robot(self, emotion: str, intensity: float):
    # 1. Mapping 12 BBIA ‚Üí 6 SDK
    sdk_emotion = self._map_bbia_to_sdk_emotion(emotion)

    # 2. Application avec goto_target (optimis√©)
    self.robot_api.goto_target(
        head=pose, body_yaw=yaw,
        duration=adaptive_duration(intensity),
        method="minjerk"
    )

```

**‚úÖ √âtat : Fonctionnel et optimis√©**

---

#### 3. Intelligence audio (STT/Reconnaissance)

**Module** : `bbia_voice.py` + `voice_whisper.py`

**√âtat actuel :**

- ‚úÖ **Basique** : `speech_recognition` + Google API (gratuit, limit√©)
- ‚úÖ **Avanc√©** : Whisper OpenAI via `voice_whisper.py` (si disponible)

**Fonctionnalit√©s :**

```python
# Dans bbia_voice.py
def reconnaitre_parole(duree=3):
    # Utilise speech_recognition + Google API
    texte = r.recognize_google(audio, language="fr-FR")
    return texte

# Dans voice_whisper.py (optionnel)
whisper_stt = WhisperSTT(model_size="tiny")
texte = whisper_stt.transcribe(audio)

```

**‚úÖ √âtat : Fonctionnel (Whisper = meilleure qualit√© si disponible)**

---

#### 4. Intelligence visuelle

**Module** : `bbia_vision.py` + `bbia_huggingface.py`

**√âtat actuel :**

- ‚úÖ D√©tection objets : YOLOv8n
- ‚úÖ Reconnaissance visages : MediaPipe
- ‚úÖ Description images : BLIP via `bbia_huggingface.py`
- ‚úÖ Vision avanc√©e : CLIP pour classification

**‚úÖ √âtat** : Tr√®s fonctionnel

---

## üí° Recommandations propos√©es

### 1. Remplacer pyttsx3 par Coqui TTS ‚≠ê

**Pourquoi :**

- Contr√¥le pitch/tonalit√© complet (r√©sout blocage macOS)
- Contr√¥le √©motionnel (happy, sad, excited, etc.)
- Qualit√© vocale sup√©rieure
- Support fran√ßais natif

**Migration :**

```python
# Nouveau bbia_voice_advanced.py
from TTS.api import TTS

class BBIAVoiceAdvanced:
    def __init__(self):
        self.tts = TTS("tts_models/fr/css10/vits")
        self.current_emotion = "neutral"

    def say(self, text: str, emotion: str = None, pitch: float = 0.0):
        emotion = emotion or self.current_emotion
        self.tts.tts_to_file(
            text=text,
            file_path="temp_audio.wav",
            emotion=emotion,
            pitch=pitch
        )
        # Jouer audio
        playsound("temp_audio.wav")

```

---

### 2. Ajouter LLM pr√©-entra√Æn√© pour conversation ‚≠ê‚≠ê

**Options :**

#### A. Mistral 7B Instruct (Recommand√©)

- ‚úÖ L√©ger (7B param√®tres)
- ‚úÖ Fran√ßais de qualit√©
- ‚úÖ Open-source
- ‚úÖ Support Apple Silicon (MPS)

**Installation :**

```bash
pip install transformers accelerate

```

**Int√©gration :**

```python
# Dans bbia_huggingface.py
from transformers import AutoModelForCausalLM, AutoTokenizer

class BBIAHuggingFace:
    def __init__(self):
        # ... code existant ...
        self.chat_model = None

    def load_chat_model(self):
        model_name = "mistralai/Mistral-7B-Instruct-v0.2"
        self.chat_tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.chat_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto"  # Auto-d√©tecte MPS/CPU
        )

    def chat(self, user_message: str) -> str:
        if not self.chat_model:
            self.load_chat_model()

        # Conversation avec contexte
        messages = [
            {"role": "system", "content": "Tu es BBIA, un robot Reachy Mini amical et curieux."},
            {"role": "user", "content": user_message}
        ]

        inputs = self.chat_tokenizer.apply_chat_template(
            messages, return_tensors="pt"
        ).to(self.device)

        outputs = self.chat_model.generate(
            inputs, max_new_tokens=100, temperature=0.7
        )

        response = self.chat_tokenizer.decode(
            outputs[0][inputs.shape[1]:], skip_special_tokens=True
        )

        # Analyser sentiment pour √©motion robot
        sentiment = self.analyze_sentiment(user_message)
        self._apply_sentiment_to_robot(sentiment)

        return response

```

#### B. Llama 3 8B (Alternative)

- ‚úÖ Open-source
- ‚úÖ Qualit√© √©lev√©e
- ‚ö†Ô∏è Plus lourd que Mistral 7B

#### C. API OpenAI (GPT-4o-mini) (Simple mais payant)

- ‚úÖ Facile √† int√©grer
- ‚úÖ Qualit√© maximale
- ‚ùå Co√ªt (~$0.15/M tokens)
- ‚ùå D√©pendance externe

---

### 3. Int√©gration voix + intelligence combin√©e

**Architecture propos√©e :**

```python
# bbia_voice_advanced.py
class BBIAVoiceAdvanced:
    def __init__(self):
        self.tts = TTS("tts_models/fr/css10/vits")
        self.emotion_map = {
            "happy": {"emotion": "happy", "pitch": 0.3},
            "sad": {"emotion": "sad", "pitch": -0.2},
            "excited": {"emotion": "excited", "pitch": 0.4},
            "neutral": {"emotion": "neutral", "pitch": 0.0}
        }

    def say_with_emotion(self, text: str, bbia_emotion: str):
        """Synth√©tise voix avec √©motion correspondante."""
        voice_config = self.emotion_map.get(bbia_emotion, self.emotion_map["neutral"])

        self.tts.tts_to_file(
            text=text,
            file_path="temp_audio.wav",
            emotion=voice_config["emotion"],
            pitch=voice_config["pitch"]
        )

        # Jouer + synchroniser avec mouvements robot
        playsound("temp_audio.wav")

```

---

## üìã Plan d'impl√©mentation

### Phase 1 : Migration voix (Priorit√© haute)

1. ‚úÖ Installer Coqui TTS : `pip install TTS`
2. ‚úÖ Cr√©er `bbia_voice_advanced.py` avec Coqui TTS
3. ‚úÖ Tester contr√¥le pitch/√©motion
4. ‚úÖ Int√©grer dans `bbia_behavior.py` et `bbia_integration.py`
5. ‚úÖ D√©pr√©cier `bbia_voice.py` (garder en fallback)

### Phase 2 : Ajouter LLM conversation (Priorit√© moyenne)

1. ‚úÖ Installer Mistral 7B : `pip install transformers accelerate`
2. ‚úÖ Modifier `BBIAHuggingFace.chat()` pour utiliser vrai LLM
3. ‚úÖ Tester conversations longues avec contexte
4. ‚úÖ Optimiser pour Apple Silicon (MPS)

### Phase 3 : Int√©gration compl√®te (Priorit√© basse)

1. ‚úÖ Synchroniser voix √©motionnelle avec mouvements robot
2. ‚úÖ Ajouter contr√¥le fin pitch selon contexte
3. ‚úÖ Optimiser latence (cache mod√®les)

---

## üéØ R√©sum√© des blocages

### Blocages macOS avec pyttsx3

- ‚ùå Pitch non contr√¥lable
- ‚ùå Contr√¥le √©motionnel inexistant
- ‚ùå Voix limit√©es (197 dispo mais non exploitables)
- ‚ùå Vitesse fixe

### Solutions propos√©es

- ‚úÖ **Coqui TTS** : R√©sout tous les blocages
- ‚úÖ **Piper TTS** : Alternative l√©g√®re (pas d'√©motion)
- ‚úÖ **XTTS v2** : Clonage voix (si besoin voix personnalis√©e)

### Intelligence conversationnelle

- ‚ùå **Actuellement** : R√®gles + sentiment analysis uniquement
- ‚úÖ **Solution** : Mistral 7B Instruct ou Llama 3 8B
- ‚úÖ **Alternative simple** : API OpenAI (payant)

---

## üìö RESSOURCES

- **Coqui TTS :** https://github.com/coqui-ai/TTS
- **Piper TTS :** https://github.com/rhasspy/piper
- **Mistral 7B :** https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
- **Llama 3 :** https://huggingface.co/meta-llama/Llama-3-8B-Instruct

---

**Prochaine √©tape :** Impl√©menter Phase 1 (Coqui TTS) pour r√©soudre blocages macOS imm√©diatement.

## üéß Int√©gration SDK Reachy Mini (media.speaker)

- **Nouveau**: `bbia_voice_advanced.py` supporte `robot_api.media.play_audio` quand disponible.
- **Priorit√© d'ex√©cution**:
  1. `robot.media.play_audio(bytes, volume)` (si expos√© par le backend Reachy Mini)
  2. `robot.media.speaker.play_file(path)` ou `robot.media.speaker.play(bytes)`
  3. Fallback local `playsound()`
- **B√©n√©fices**:
  - Latence plus faible et rendu audio mat√©riel direct du Reachy Mini
  - Volume g√©r√© c√¥t√© robot
  - Alignement avec la pile officielle (r√©f√©rence: SDK `reachy_mini`)

> Note: Aucun changement de date; cette section documente l'alignement avec le SDK officiel.

---

## üéØ Navigation

**Retour √†** : [README Documentation](../README.md)  
**Voir aussi** : [Modules IA](modules.md) ‚Ä¢ [Intelligence LLM](llm.md) ‚Ä¢ [Index Th√©matique](../reference/INDEX_THEMATIQUE.md)
