# üé§ Analyse Compl√®te : Voix & Intelligence BBIA

**Date :** Octobre 2025
**Auteur :** Analyse Expert
**Objectif :** Identifier blocages macOS, solutions voix alternatives, et √©tat intelligence BBIA

---

## üìä √âTAT ACTUEL : Voix BBIA

### Impl√©mentation Actuelle (`bbia_voice.py`)

**Technologie :** `pyttsx3` (wrapper autour des voix syst√®me macOS)

**Blocages macOS identifi√©s (confirm√©s par test) :**

‚ùå **Pitch non contr√¥lable**
- Message d'erreur : `"Pitch adjustment not supported when using NSSS"`
- Impact : Impossible de modifier la tonalit√©/hauteur de voix

‚ùå **Contr√¥le √©motionnel inexistant**
- `pyttsx3` ne permet pas d'ajuster l'√©motion (joyeux, triste, excit√©, etc.)
- Seule la vitesse et le volume sont contr√¥lables

‚ùå **Voix forc√©e √† "Am√©lie"**
- Actuellement, le code force l'utilisation d'Am√©lie
- 197 voix disponibles sur macOS mais non exploitables avec pyttsx3

‚ùå **Vitesse limit√©e**
- Rate = 170 (fixe)
- Pas de variation dynamique selon le contexte

---

## üîç SOLUTIONS ALTERNATIVES : G√©n√©rateurs de Voix Avanc√©s

### Option 1 : ‚≠ê **Coqui TTS** (Recommand√©)

**Repository :** `https://github.com/coqui-ai/TTS`

**Avantages :**
- ‚úÖ Contr√¥le pitch/tonalit√© complet
- ‚úÖ Contr√¥le √©motionnel (happy, sad, excited, etc.)
- ‚úÖ Multi-langues (fran√ßais inclus)
- ‚úÖ Voix pr√©-entra√Æn√©es disponibles
- ‚úÖ Installation simple : `pip install TTS`
- ‚úÖ Support Apple Silicon (MPS)

**Fonctionnalit√©s cl√©s :**
```python
# Exemple d'utilisation
from TTS.api import TTS

tts = TTS("tts_models/fr/css10/vits")
tts.tts_to_file(
    text="Bonjour, je suis BBIA",
    file_path="output.wav",
    speaker_wav="reference_voice.wav",  # Clonage voix optionnel
    emotion="happy",  # √âmotion contr√¥lable
    pitch=0.2  # Pitch contr√¥lable
)
```

**Mod√®les fran√ßais disponibles :**
- `tts_models/fr/css10/vits` - Voix fran√ßaise de qualit√©
- `tts_models/multilingual/multi-dataset/your_tts` - Multi-langues + clonage

**Installation :**
```bash
pip install TTS
# Optionnel : pour voix haute qualit√©
pip install TTS[all]
```

---

### Option 2 : **Piper TTS** (L√©ger et rapide)

**Repository :** `https://github.com/rhasspy/piper`

**Avantages :**
- ‚úÖ Tr√®s l√©ger (mod√®les ~10-20MB)
- ‚úÖ Tr√®s rapide (temps r√©el garanti)
- ‚úÖ Contr√¥le pitch via SSML
- ‚úÖ Installation : `pip install piper-tts`

**Limitations :**
- ‚ö†Ô∏è Pas de contr√¥le √©motionnel direct
- ‚ö†Ô∏è Qualit√© vocale inf√©rieure √† Coqui TTS

**Fonctionnalit√©s :**
```python
from piper import PiperVoice

voice = PiperVoice.load("fr_FR-am√©lie-medium")
audio = voice.synthesize(
    "Bonjour",
    speaker_id=0,
    length_scale=1.0,  # Vitesse
    noise_scale=0.667,  # Variation
    noise_w=0.8
)
```

---

### Option 3 : **XTTS v2** (Clonage voix avanc√©)

**Repository :** `https://github.com/coqui-ai/TTS` (partie XTTS)

**Avantages :**
- ‚úÖ Clonage voix avec 3 secondes d'audio seulement
- ‚úÖ Contr√¥le √©motionnel complet
- ‚úÖ Multi-langues
- ‚úÖ Qualit√© professionnelle

**Limitations :**
- ‚ö†Ô∏è Plus lourd (mod√®le ~1.5GB)
- ‚ö†Ô∏è Plus lent (g√©n√©ration ~2-5 secondes)

**Utilisation :**
```python
from TTS.api import TTS

tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2")
tts.tts_to_file(
    text="Bonjour",
    file_path="output.wav",
    speaker_wav="reference_voice.wav",  # Clone voix
    language="fr",
    emotion="happy"
)
```

---

### Option 4 : **Bark** (Voix naturelles avec bruitages)

**Repository :** `https://github.com/suno-ai/bark`

**Avantages :**
- ‚úÖ Voix tr√®s naturelles
- ‚úÖ Support bruitages (rire, soupir, etc.)
- ‚úÖ Contr√¥le style vocal

**Limitations :**
- ‚ö†Ô∏è Pas de fran√ßais natif (anglais principalement)
- ‚ö†Ô∏è Tr√®s lent (g√©n√©ration ~10-30 secondes)
- ‚ö†Ô∏è Mod√®le tr√®s lourd (~1GB)

---

## üß† √âTAT ACTUEL : Intelligence BBIA

### Architecture Intelligence Actuelle

#### 1. **Intelligence Verbale (Conversation)**

**Module :** `bbia_huggingface.py` ‚Üí `BBIAHuggingFace.chat()`

**Probl√®me identifi√© :**
- ‚úÖ Analyse sentiment : `cardiffnlp/twitter-roberta-base-sentiment-latest`
- ‚úÖ Analyse √©motion : `j-hartmann/emotion-english-distilroberta-base`
- ‚ùå **PAS de vrai LLM conversationnel** - Utilise seulement des r√®gles et sentiment analysis

**Fonctionnement actuel :**
```python
# Actuellement dans bbia_huggingface.py :
def chat(self, user_message: str) -> str:
    # 1. Analyse sentiment (‚úÖ)
    sentiment = self.analyze_sentiment(user_message)

    # 2. G√©n√©ration r√©ponse (‚ùå R√®gles basiques)
    response = self._generate_response_from_sentiment(sentiment)
    # ‚Üí Pas de vrai mod√®le de langage !
```

**Ce qui manque :**
- ‚ùå LLM pr√©-entra√Æn√© pour conversation naturelle
- ‚ùå Compr√©hension contextuelle avanc√©e
- ‚ùå G√©n√©ration de texte intelligente

---

#### 2. **Intelligence √âmotionnelle**

**Module :** `bbia_emotions.py` + `bbia_integration.py`

**√âtat actuel :**
- ‚úÖ 12 √©motions BBIA d√©finies
- ‚úÖ Mapping vers 6 √©motions SDK Reachy Mini
- ‚úÖ Application √©motions au robot via `set_emotion()` ou `goto_target()`
- ‚úÖ Transitions fluides avec interpolation minjerk
- ‚úÖ Dur√©e adaptative selon intensit√©

**Fonctionnalit√©s :**
```python
# Dans bbia_integration.py
def apply_emotion_to_robot(self, emotion: str, intensity: float):
    # 1. Mapping 12 BBIA ‚Üí 6 SDK
    sdk_emotion = self._map_bbia_to_sdk_emotion(emotion)

    # 2. Application avec goto_target (optimis√©)
    self.robot_api.goto_target(
        head=pose, body_yaw=yaw,
        duration=adaptive_duration(intensity),
        method="minjerk"
    )
```

**‚úÖ √âtat : Fonctionnel et optimis√©**

---

#### 3. **Intelligence Audio (STT/Reconnaissance)**

**Module :** `bbia_voice.py` + `voice_whisper.py`

**√âtat actuel :**
- ‚úÖ **Basique :** `speech_recognition` + Google API (gratuit, limit√©)
- ‚úÖ **Avanc√© :** Whisper OpenAI via `voice_whisper.py` (si disponible)

**Fonctionnalit√©s :**
```python
# Dans bbia_voice.py
def reconnaitre_parole(duree=3):
    # Utilise speech_recognition + Google API
    texte = r.recognize_google(audio, language="fr-FR")
    return texte

# Dans voice_whisper.py (optionnel)
whisper_stt = WhisperSTT(model_size="tiny")
texte = whisper_stt.transcribe(audio)
```

**‚úÖ √âtat : Fonctionnel (Whisper = meilleure qualit√© si disponible)**

---

#### 4. **Intelligence Visuelle**

**Module :** `bbia_vision.py` + `bbia_huggingface.py`

**√âtat actuel :**
- ‚úÖ D√©tection objets : YOLOv8n
- ‚úÖ Reconnaissance visages : MediaPipe
- ‚úÖ Description images : BLIP via `bbia_huggingface.py`
- ‚úÖ Vision avanc√©e : CLIP pour classification

**‚úÖ √âtat : Tr√®s fonctionnel**

---

## üí° RECOMMANDATIONS PROPOS√âES

### 1. **Remplacer pyttsx3 par Coqui TTS** ‚≠ê

**Pourquoi :**
- Contr√¥le pitch/tonalit√© complet (r√©sout blocage macOS)
- Contr√¥le √©motionnel (happy, sad, excited, etc.)
- Qualit√© vocale sup√©rieure
- Support fran√ßais natif

**Migration :**
```python
# Nouveau bbia_voice_advanced.py
from TTS.api import TTS

class BBIAVoiceAdvanced:
    def __init__(self):
        self.tts = TTS("tts_models/fr/css10/vits")
        self.current_emotion = "neutral"

    def say(self, text: str, emotion: str = None, pitch: float = 0.0):
        emotion = emotion or self.current_emotion
        self.tts.tts_to_file(
            text=text,
            file_path="temp_audio.wav",
            emotion=emotion,
            pitch=pitch
        )
        # Jouer audio
        playsound("temp_audio.wav")
```

---

### 2. **Ajouter LLM Pr√©-entra√Æn√© pour Conversation** ‚≠ê‚≠ê

**Options :**

#### A. **Mistral 7B Instruct** (Recommand√©)
- ‚úÖ L√©ger (7B param√®tres)
- ‚úÖ Fran√ßais excellent
- ‚úÖ Open-source
- ‚úÖ Support Apple Silicon (MPS)

**Installation :**
```bash
pip install transformers accelerate
```

**Int√©gration :**
```python
# Dans bbia_huggingface.py
from transformers import AutoModelForCausalLM, AutoTokenizer

class BBIAHuggingFace:
    def __init__(self):
        # ... code existant ...
        self.chat_model = None

    def load_chat_model(self):
        model_name = "mistralai/Mistral-7B-Instruct-v0.2"
        self.chat_tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.chat_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto"  # Auto-d√©tecte MPS/CPU
        )

    def chat(self, user_message: str) -> str:
        if not self.chat_model:
            self.load_chat_model()

        # Conversation avec contexte
        messages = [
            {"role": "system", "content": "Tu es BBIA, un robot Reachy Mini amical et curieux."},
            {"role": "user", "content": user_message}
        ]

        inputs = self.chat_tokenizer.apply_chat_template(
            messages, return_tensors="pt"
        ).to(self.device)

        outputs = self.chat_model.generate(
            inputs, max_new_tokens=100, temperature=0.7
        )

        response = self.chat_tokenizer.decode(
            outputs[0][inputs.shape[1]:], skip_special_tokens=True
        )

        # Analyser sentiment pour √©motion robot
        sentiment = self.analyze_sentiment(user_message)
        self._apply_sentiment_to_robot(sentiment)

        return response
```

#### B. **Llama 3 8B** (Alternative)
- ‚úÖ Open-source
- ‚úÖ Qualit√© excellente
- ‚ö†Ô∏è Plus lourd que Mistral 7B

#### C. **API OpenAI (GPT-4o-mini)** (Simple mais payant)
- ‚úÖ Tr√®s facile √† int√©grer
- ‚úÖ Qualit√© maximale
- ‚ùå Co√ªt (~$0.15/M tokens)
- ‚ùå D√©pendance externe

---

### 3. **Int√©gration Voix + Intelligence Combin√©e**

**Architecture propos√©e :**

```python
# bbia_voice_advanced.py
class BBIAVoiceAdvanced:
    def __init__(self):
        self.tts = TTS("tts_models/fr/css10/vits")
        self.emotion_map = {
            "happy": {"emotion": "happy", "pitch": 0.3},
            "sad": {"emotion": "sad", "pitch": -0.2},
            "excited": {"emotion": "excited", "pitch": 0.4},
            "neutral": {"emotion": "neutral", "pitch": 0.0}
        }

    def say_with_emotion(self, text: str, bbia_emotion: str):
        """Synth√©tise voix avec √©motion correspondante."""
        voice_config = self.emotion_map.get(bbia_emotion, self.emotion_map["neutral"])

        self.tts.tts_to_file(
            text=text,
            file_path="temp_audio.wav",
            emotion=voice_config["emotion"],
            pitch=voice_config["pitch"]
        )

        # Jouer + synchroniser avec mouvements robot
        playsound("temp_audio.wav")
```

---

## üìã PLAN D'IMPL√âMENTATION

### Phase 1 : Migration Voix (Priorit√© Haute)

1. ‚úÖ Installer Coqui TTS : `pip install TTS`
2. ‚úÖ Cr√©er `bbia_voice_advanced.py` avec Coqui TTS
3. ‚úÖ Tester contr√¥le pitch/√©motion
4. ‚úÖ Int√©grer dans `bbia_behavior.py` et `bbia_integration.py`
5. ‚úÖ D√©pr√©cier `bbia_voice.py` (garder en fallback)

### Phase 2 : Ajouter LLM Conversation (Priorit√© Moyenne)

1. ‚úÖ Installer Mistral 7B : `pip install transformers accelerate`
2. ‚úÖ Modifier `BBIAHuggingFace.chat()` pour utiliser vrai LLM
3. ‚úÖ Tester conversations longues avec contexte
4. ‚úÖ Optimiser pour Apple Silicon (MPS)

### Phase 3 : Int√©gration Compl√®te (Priorit√© Basse)

1. ‚úÖ Synchroniser voix √©motionnelle avec mouvements robot
2. ‚úÖ Ajouter contr√¥le fin pitch selon contexte
3. ‚úÖ Optimiser latence (cache mod√®les)

---

## üéØ R√âSUM√â DES BLOCAGES

### Blocages macOS avec pyttsx3 :
- ‚ùå Pitch non contr√¥lable
- ‚ùå Contr√¥le √©motionnel inexistant
- ‚ùå Voix limit√©es (197 dispo mais non exploitables)
- ‚ùå Vitesse fixe

### Solutions propos√©es :
- ‚úÖ **Coqui TTS** : R√©sout tous les blocages
- ‚úÖ **Piper TTS** : Alternative l√©g√®re (pas d'√©motion)
- ‚úÖ **XTTS v2** : Clonage voix (si besoin voix personnalis√©e)

### Intelligence conversationnelle :
- ‚ùå **Actuellement** : R√®gles + sentiment analysis uniquement
- ‚úÖ **Solution** : Mistral 7B Instruct ou Llama 3 8B
- ‚úÖ **Alternative simple** : API OpenAI (payant)

---

## üìö RESSOURCES

- **Coqui TTS :** https://github.com/coqui-ai/TTS
- **Piper TTS :** https://github.com/rhasspy/piper
- **Mistral 7B :** https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
- **Llama 3 :** https://huggingface.co/meta-llama/Llama-3-8B-Instruct

---

**Prochaine √©tape :** Impl√©menter Phase 1 (Coqui TTS) pour r√©soudre blocages macOS imm√©diatement.

## üéß Int√©gration SDK Reachy Mini (media.speaker)

- **Nouveau**: `bbia_voice_advanced.py` supporte `robot_api.media.play_audio` quand disponible.
- **Priorit√© d'ex√©cution**:
  1. `robot.media.play_audio(bytes, volume)` (si expos√© par le backend Reachy Mini)
  2. `robot.media.speaker.play_file(path)` ou `robot.media.speaker.play(bytes)`
  3. Fallback local `playsound()`
- **B√©n√©fices**:
  - Latence plus faible et rendu audio mat√©riel direct du Reachy Mini
  - Volume g√©r√© c√¥t√© robot
  - Alignement avec la pile officielle (r√©f√©rence: SDK `reachy_mini`)

> Note: Aucun changement de date; cette section documente l‚Äôalignement avec le SDK officiel.

