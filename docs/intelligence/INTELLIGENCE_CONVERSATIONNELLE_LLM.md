# Intelligence conversationnelle LLM - guide complet

**Date :** Octobre 2025  
**Objectif :** ajouter LLM pr√©-entra√Æn√© (Mistral 7B) pour conversations intelligentes

---

## Objectif

Remplacer les r√©ponses bas√©es sur r√®gles par un LLM conversationnel qui comprend le contexte et g√©n√®re des r√©ponses naturelles.

**Avant :** R√®gles + sentiment analysis (limit√©)  
**Apr√®s :** Mistral 7B Instruct (conversations naturelles avec contexte)

---

## Installation

### Pr√©requis

```bash
# Activer venv
source venv/bin/activate

# Installer d√©pendances LLM
pip install transformers accelerate torch

# Optionnel : optimisations Apple Silicon
# (acc√©l√©ration automatique via MPS si disponible)
```

---

## Utilisation

### Activation du LLM (optionnel)

Le LLM est **d√©sactiv√© par d√©faut** (pour √©viter consommation m√©moire inutile). Activer uniquement si n√©cessaire :

```python
from bbia_sim.bbia_huggingface import BBIAHuggingFace

# Initialiser BBIA
bbia = BBIAHuggingFace()

# Activer LLM conversationnel (peut prendre 1-2 minutes)
success = bbia.enable_llm_chat()
if success:
    print("LLM activ√© - conversations intelligentes disponibles")
else:
    print("LLM non charg√© - utilisation r√©ponses enrichies (r√®gles)")
```

### Utilisation automatique

Le LLM s'active automatiquement dans `ConversationBehavior` si disponible :

```python
from bbia_sim.bbia_behavior import ConversationBehavior

# Le comportement utilise automatiquement le LLM si activ√©
behavior = ConversationBehavior()
# Les conversations utiliseront Mistral 7B si disponible
```

### Conversation avec LLM

```python
from bbia_sim.bbia_huggingface import BBIAHuggingFace

bbia = BBIAHuggingFace()

# Activer LLM
bbia.enable_llm_chat("mistralai/Mistral-7B-Instruct-v0.2")

# Conversation intelligente
response = bbia.chat("Bonjour, comment √ßa va ?")
print(response)  # ü§ñ R√©ponse naturelle g√©n√©r√©e par Mistral 7B

# Avec contexte (utilise historique)
response2 = bbia.chat("Tu te rappelles ce que je viens de dire ?", use_context=True)
print(response2)  # ü§ñ R√©ponse qui r√©f√©rence la conversation pr√©c√©dente
```

### D√©sactiver LLM (Lib√©rer M√©moire)

```python
# D√©sactiver pour lib√©rer ~14GB RAM
bbia.disable_llm_chat()
```

---

## Personnalit√©s BBIA

Le LLM adapte ses r√©ponses selon la personnalit√© BBIA :

```python
# Personnalit√© amicale (d√©faut)
bbia.bbia_personality = "friendly_robot"
response = bbia.chat("Salut !")
# ‚Üí R√©ponse chaleureuse et professionnelle

# Personnalit√© curieuse
bbia.bbia_personality = "curious"
response = bbia.chat("Salut !")
# ‚Üí R√©ponse avec questions et curiosit√©

# Personnalit√© enthousiaste
bbia.bbia_personality = "enthusiastic"
response = bbia.chat("Salut !")
# ‚Üí R√©ponse √©nergique et positive

# Personnalit√© calme
bbia.bbia_personality = "calm"
response = bbia.chat("Salut !")
# ‚Üí R√©ponse sereine et apaisante
```

---

## Comparaison avant/apr√®s

### Avant (r√®gles + sentiment)

```python
# R√©ponses bas√©es sur r√®gles
User: "Bonjour"
BBIA: "Bonjour ! Ravi de vous revoir ! Comment allez-vous aujourd'hui ?"
# ‚Üí Variantes limit√©es (6-8 r√©ponses possibles)
```

### Apr√®s (LLM Mistral 7B)

```python
# R√©ponses g√©n√©r√©es intelligemment
User: "Bonjour"
BBIA: "Bonjour ! Content de te revoir. Comment s'est pass√©e ta journ√©e ?"
# ‚Üí R√©ponses naturelles, vari√©es, contextuelles
```

Avantages LLM :
- r√©ponses naturelles et vari√©es
- compr√©hension du contexte
- r√©f√©rence √† l'historique conversationnel
- g√©n√©ration cr√©ative selon personnalit√©
- adaptabilit√© √† n'importe quelle question

---

## Configuration

### Mod√®les disponibles

1. **Mistral 7B Instruct** (recommand√©)
   - Qualit√© : excellente
   - Fran√ßais : tr√®s bon
   - Taille : ~14GB RAM
   - Support MPS : oui (Apple Silicon)
   - Vitesse : ~1-3 secondes/r√©ponse

2. **Llama 3 8B Instruct** (alternative)
   - Qualit√© : excellente
   - Fran√ßais : bon
   - Taille : ~16GB RAM
   - Support MPS : oui
   - Vitesse : ~1-3 secondes/r√©ponse

### Configuration personnalis√©e

```python
# Charger mod√®le personnalis√©
bbia.enable_llm_chat("meta-llama/Llama-3-8B-Instruct")

# Ou utiliser mod√®le local
bbia.enable_llm_chat("./models/mistral-7b-instruct")
```

---

## Param√®tres g√©n√©ration

### Personnalisation r√©ponses

Le LLM utilise ces param√®tres par d√©faut :
- `max_new_tokens=150` : Limite longueur r√©ponse
- `temperature=0.7` : Cr√©ativit√© mod√©r√©e
- `top_p=0.9` : Nucleus sampling
- `do_sample=True` : G√©n√©ration vari√©e

Pour modifier (dans `bbia_huggingface.py`) :

```python
outputs = self.chat_model.generate(
    **inputs,
    max_new_tokens=200,  # R√©ponses plus longues
    temperature=0.9,     # Plus cr√©atif
    top_p=0.95,
    do_sample=True,
)
```

---

## Performance

### Ressources n√©cessaires

**Mistral 7B Instruct :**
- RAM : ~14GB
- Premier chargement : 1-2 minutes
- G√©n√©ration : ~1-3 secondes/r√©ponse
- Disk : ~14GB (cache mod√®le)

**Optimisations :**
- Apple Silicon (M1/M2/M3) : Acc√©l√©ration MPS automatique
- CUDA : Acc√©l√©ration GPU si disponible
- Quantization : R√©duire RAM √† ~8GB (qualit√© l√©g√®rement r√©duite)

### Gestion m√©moire

```python
# Si m√©moire limit√©e, charger seulement quand n√©cessaire
if need_intelligent_chat:
    bbia.enable_llm_chat()
    response = bbia.chat(user_message)
    bbia.disable_llm_chat()  # Lib√©rer m√©moire apr√®s
```

---

## üß™ Tests

```bash
# Tester LLM conversationnel
cd /Volumes/T7/bbia-reachy-sim
source venv/bin/activate
python -c "
from bbia_sim.bbia_huggingface import BBIAHuggingFace

bbia = BBIAHuggingFace()
print('üì• Chargement LLM...')
if bbia.enable_llm_chat():
    print('‚úÖ LLM charg√©')
    response = bbia.chat('Bonjour, qui es-tu ?')
    print(f'ü§ñ R√©ponse: {response}')
else:
    print('‚ùå LLM non charg√©')
"
```

---

## Limitations

1. **M√©moire :**
   - Requiert ~14GB RAM (Mistral 7B)
   - Peut √™tre lourd sur machines limit√©es

2. **Vitesse :**
   - Premier chargement : 1-2 minutes
   - G√©n√©ration : 1-3 secondes/r√©ponse (CPU) ou <1s (MPS/CUDA)

3. **Qualit√© :**
   - Excellent fran√ßais mais parfois g√©n√®re en anglais
   - R√©ponses parfois trop longues (limit√© √† 150 tokens)

4. **Fallback :**
   - Si LLM √©choue, fallback automatique vers r√©ponses enrichies (r√®gles)

---

## Migration automatique

Le code existant **fonctionne sans modification** :

```python
# Code existant continue de fonctionner
from bbia_sim.bbia_huggingface import BBIAHuggingFace

bbia = BBIAHuggingFace()
response = bbia.chat("Bonjour")
# ‚Üí Utilise r√©ponses enrichies (r√®gles) par d√©faut

# Activer LLM si souhait√©
bbia.enable_llm_chat()
response = bbia.chat("Bonjour")
# ‚Üí Utilise Mistral 7B pour r√©ponse intelligente
```

---

## R√©f√©rences

- **Mistral 7B :** https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
- **Llama 3 :** https://huggingface.co/meta-llama/Llama-3-8B-Instruct
- **Transformers :** https://huggingface.co/docs/transformers

---

## Prochaines √©tapes

1. OK LLM int√©gr√© dans `BBIAHuggingFace.chat()`
2. OK Activation optionnelle via `enable_llm_chat()`
3. ‚è≥ Tests unitaires
4. ‚è≥ Optimisation m√©moire (quantization optionnelle)
5. ‚è≥ Support streaming (r√©ponses au fil de l'eau)

---

Status : phase 2 compl√©t√©e - LLM conversationnel disponible

