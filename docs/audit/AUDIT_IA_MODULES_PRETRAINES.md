# Audit IA : Modules Pr√©-entra√Æn√©s dans BBIA

**Date** : 2025-01-30  
**Objectif** : V√©rifier quels mod√®les IA pr√©-entra√Æn√©s sont utilis√©s, o√π ils sont utilis√©s, et identifier ce qui manque.

---

## ‚úÖ 1. MOD√àLES PR√â-ENTRA√éN√âS UTILIS√âS

### üéØ Vision - D√âJ√Ä IMPL√âMENT√â ‚úÖ

**Mod√®les utilis√©s** :
- ‚úÖ **YOLOv8n** (Ultralytics) - D√©tection objets
- ‚úÖ **MediaPipe Face Detection** - D√©tection visages
- ‚úÖ **MediaPipe Face Mesh** - Landmarks faciaux
- ‚úÖ **CLIP** (OpenAI) - Classification images
- ‚úÖ **BLIP** (Salesforce) - Description images
- ‚úÖ **BLIP VQA** - Visual Question Answering

**O√π c'est utilis√©** :
- `src/bbia_sim/vision_yolo.py` : `YOLODetector` classe ‚Üí charge `yolov8n.pt`
- `src/bbia_sim/bbia_vision.py` : `BBIAVision` ‚Üí utilise YOLO + MediaPipe
- `src/bbia_sim/bbia_huggingface.py` : `BBIAHuggingFace._load_vision_model()` ‚Üí CLIP/BLIP

**√âtat** : ‚úÖ **FONCTIONNEL** - Mod√®les pr√©-entra√Æn√©s charg√©s automatiquement depuis Hugging Face/Ultralytics

---

### üí¨ Langage (LLM) - D√âJ√Ä IMPL√âMENT√â ‚úÖ

**Mod√®les utilis√©s** :
- ‚úÖ **Mistral 7B Instruct** (`mistralai/Mistral-7B-Instruct-v0.2`)
- ‚úÖ **Llama 3 8B Instruct** (`meta-llama/Llama-3-8B-Instruct`)
- ‚úÖ **llama.cpp** (GGUF, local) - Fallback l√©ger
- ‚úÖ **Twitter RoBERTa Sentiment** (`cardiffnlp/twitter-roberta-base-sentiment-latest`)
- ‚úÖ **Emotion DistilRoBERTa** (`j-hartmann/emotion-english-distilroberta-base`)

**O√π c'est utilis√©** :
- `src/bbia_sim/bbia_huggingface.py` : `BBIAHuggingFace.enable_llm_chat()` ‚Üí charge Mistral/Llama
- `src/bbia_sim/bbia_emotion_recognition.py` : `BBIAEmotionRecognition._load_emotion_models()` ‚Üí sentiment/√©motion
- `src/bbia_sim/ai_backends.py` : `LlamaCppLLM` ‚Üí support llama.cpp

**√âtat** : ‚úÖ **FONCTIONNEL** - LLM conversationnel + analyse sentiment/√©motion

---

### üé§ Audio/Parole - D√âJ√Ä IMPL√âMENT√â ‚úÖ

**Mod√®les utilis√©s** :
- ‚úÖ **OpenAI Whisper** - STT (Speech-to-Text)
- ‚úÖ **Coqui TTS** - TTS avec clonage voix (XTTS v2)
- ‚úÖ **SpeechRecognition** (Google API) - STT fallback
- ‚úÖ **pyttsx3** - TTS syst√®me macOS (fallback)

**O√π c'est utilis√©** :
- `src/bbia_sim/voice_whisper.py` : `WhisperSTT` ‚Üí charge `whisper-{size}.pt`
- `src/bbia_sim/ai_backends.py` : `CoquiTTSTTS` ‚Üí utilise `TTS.api`
- `scripts/voice_clone/generate_voice.py` : Clonage voix avec XTTS v2
- `src/bbia_sim/bbia_voice.py` : Int√©gration TTS/STT

**√âtat** : ‚úÖ **FONCTIONNEL** - STT avanc√© (Whisper) + TTS personnalisable (Coqui)

---

## ‚ö†Ô∏è 2. CE QUI MANQUE (recommandations open-source)

### ‚ùå Reconnaissance Visage Personnalis√©e (DeepFace)

**Manque** : Reconna√Ætre des personnes sp√©cifiques (famille, amis)

**Solution open-source recommand√©e** :
- ‚úÖ **DeepFace** (`serengil/deepface`) - Gratuit, open-source
- ‚úÖ **FaceNet** (via Hugging Face) - Alternative
- ‚úÖ **InsightFace** - Plus pr√©cis, mais plus complexe

**Pourquoi c'est important** :
- BBIA peut dire "Bonjour [Pr√©nom]" quand elle reconna√Æt quelqu'un
- Comportements personnalis√©s selon la personne

**O√π l'ajouter** :
- Cr√©er `src/bbia_sim/face_recognition.py`
- Int√©grer dans `BBIAVision.detect_faces()` ‚Üí retourner `{"name": "Alice", "confidence": 0.95}`

**Exemple code** :
```python
from deepface import DeepFace

# Enregistrer une personne
DeepFace.represent("photo_alice.jpg", model_name="VGG-Face")

# Reconna√Ætre une personne
result = DeepFace.find(img_path="frame.jpg", db_path="faces_db")
```

---

### ‚ùå D√©tection Postures/Corps (MediaPipe Pose)

**Manque** : D√©tecter poses (debout, assis, gestes)

**Solution open-source** :
- ‚úÖ **MediaPipe Pose** - D√©j√† disponible (pas utilis√© pour poses)
- ‚úÖ **YOLO-Pose** - D√©tection pose + objets simultan√©e

**Pourquoi c'est important** :
- BBIA peut r√©agir aux gestes (salut, applaudissements)
- D√©tecter si quelqu'un est debout/assis/bless√©

**O√π l'ajouter** :
- `src/bbia_sim/vision_yolo.py` ‚Üí ajouter `YOLOPoseDetector`
- Utiliser `mp.solutions.pose` dans `bbia_vision.py`

---

### ‚ùå D√©tection √âmotions Visuelles (DeepFace)

**Manque** : Analyser √©motions sur visages d√©tect√©s

**Solution open-source** :
- ‚úÖ **DeepFace** (`analyze()` ‚Üí `emotion`) - Gratuit
- ‚úÖ **FER2013** (via Hugging Face) - Alternative l√©g√®re

**Pourquoi c'est important** :
- BBIA peut adapter sa r√©ponse selon l'√©motion de l'utilisateur
- Exemple : "Tu as l'air triste, veux-tu parler ?"

**O√π l'ajouter** :
- `src/bbia_sim/bbia_emotion_recognition.py` ‚Üí m√©thode `detect_emotion_from_face()`

---

### ‚ö†Ô∏è LLM Local L√©ger (optionnel)

**√âtat actuel** : Mistral 7B = 14GB RAM (pas toujours pratique)

**Solutions open-source plus l√©g√®res** :
- ‚úÖ **Phi-2** (2.7B) - Microsoft, ~5GB RAM
- ‚úÖ **TinyLlama** (1.1B) - Ultra-l√©ger, ~2GB RAM
- ‚úÖ **Qwen 1.5B** - Alibaba, fran√ßais OK

**Pourquoi c'est utile** :
- Fonctionne sur machines moins puissantes
- Plus rapide pour r√©ponses simples

**O√π l'ajouter** :
- `src/bbia_sim/bbia_huggingface.py` ‚Üí ajouter config "chat_light"

---

## ‚úÖ 3. ARCHITECTURE MODULAIRE - D√âJ√Ä FAIT ‚úÖ

**√âtat** : ‚úÖ **EXCELLENT**

**Preuve** :
- Modules s√©par√©s : `bbia_vision.py`, `bbia_voice.py`, `bbia_huggingface.py`
- Backends s√©lectionnables via variables d'env (`BBIA_TTS_BACKEND`, `BBIA_LLM_BACKEND`)
- Plug-and-play : chaque module fonctionne ind√©pendamment
- Fallbacks : si un mod√®le √©choue, fallback automatique

**Exemple** :
```python
# Dans ai_backends.py
def get_tts_backend():
    name = os.environ.get("BBIA_TTS_BACKEND", "kitten")  # NOTE: d√©faut r√©el = kitten
    if name == "coqui":
        return CoquiTTSTTS()
    elif name == "openvoice":
        return OpenVoiceTTSTTS()
    return Pyttsx3TTS()  # Fallback
```

**‚úÖ Voix corrig√©e et v√©rifi√©e** :
- ‚úÖ `Pyttsx3TTS` dans `ai_backends.py` **UTILISE** `get_bbia_voice()` (ligne 56-58)
- ‚úÖ `get_bbia_voice()` s√©lectionne automatiquement : Aurelie Enhanced > Amelie Enhanced > Aurelie > Amelie
- ‚úÖ Si `BBIA_TTS_BACKEND` = "kitten" (d√©faut), fallback sur `Pyttsx3TTS` qui utilise `get_bbia_voice()` ‚Üí Aurelie Enhanced s√©lectionn√©e

**Recommandation** :
- ‚úÖ D√©faut actuel fonctionne bien (kitten ‚Üí Pyttsx3TTS ‚Üí Aurelie Enhanced)
- ‚úÖ Option avanc√©e : `BBIA_TTS_BACKEND=coqui` pour contr√¥le pitch/√©motion (Coqui TTS)

**Conclusion** : Architecture modulaire d√©j√† tr√®s bien con√ßue ‚úÖ

---

## ‚ö†Ô∏è 4. FINETUNED MODELS (Reconnaissance Personnalis√©e)

### √âtat Actuel

**D√©j√† fait** :
- ‚úÖ Clonage voix (Coqui XTTS v2) - Tu enregistres ta voix, BBIA l'utilise
- ‚úÖ Comportements personnalis√©s (`bbia_behavior.py`) - Tu peux cr√©er tes propres comportements

**Manque** :
- ‚ùå Reconnaissance visage personnalis√©e (DeepFace)
- ‚ùå Entra√Ænement objets custom (ex: reconna√Ætre tes objets sp√©cifiques)

### Solutions Open-Source

**1. DeepFace - Reconnaissance Visage** :
```python
# Enregistrer ta famille
DeepFace.build_model("VGG-Face")
DeepFace.represent("photo_alice.jpg", model_name="VGG-Face")

# BBIA reconna√Æt Alice
result = DeepFace.find("frame.jpg", db_path="./faces_db")
# Retourne : {"identity": "faces_db/alice.jpg", "distance": 0.2}
```

**2. YOLO Custom Training** :
- Utiliser **YOLOv8** avec ton dataset
- Exemple : entra√Æner pour reconna√Ætre tes objets (ex: "T√©l√©commande BBIA", "Lampe pr√©f√©r√©e")
- Guide : https://docs.ultralytics.com/modes/train/

**O√π l'ajouter** :
- `scripts/train_custom_yolo.py` - Script d'entra√Ænement personnalis√©
- `src/bbia_sim/face_recognition.py` - Module DeepFace

---

## ‚ö†Ô∏è 5. NO-CODE/LOW-CODE - PARTIELLEMENT FAIT

### √âtat Actuel

**D√©j√† fait** :
- ‚úÖ Dashboard web (`dashboard_advanced.py`) - Interface graphique pour contr√¥ler BBIA
- ‚úÖ Chat en temps r√©el (WebSocket) - Tu parles, BBIA r√©pond
- ‚úÖ Variables d'environnement - Configuration sans code

**Manque** :
- ‚ùå Interface drag-and-drop pour cr√©er comportements
- ‚ùå Dashboard pour entra√Æner mod√®les custom (ex: upload photos pour DeepFace)

### Recommandations Open-Source

**Option 1 : Streamlit Dashboard** (Simple)
```python
# Cr√©er scripts/dashboard_streamlit.py
import streamlit as st
from bbia_sim.bbia_vision import BBIAVision

st.title("BBIA Vision Control")
vision = BBIAVision()

if st.button("Scan Environment"):
    result = vision.scan_environment()
    st.json(result)
```

**Option 2 : Gradio** (Hugging Face)
- Interface simple pour tester mod√®les
- Upload images ‚Üí voir d√©tections
- Pas besoin de coder HTML/JS

**O√π l'ajouter** :
- `scripts/dashboard_gradio.py` - Interface simple pour vision/chat

---

## ‚úÖ 6. ENTR√ÇINEMENT PROGRESSIF - D√âJ√Ä SUPPORT√â ‚úÖ

**Preuve** :
- Conversation history (`BBIAHuggingFace.conversation_history`) - BBIA se souvient du contexte
- Comportements adaptatifs (`bbia_adaptive_behavior.py`) - Apprend des patterns
- Fallbacks intelligents - Si un mod√®le √©choue, BBIA essaie autre chose

**Am√©lioration possible** :
- Sauvegarder apprentissages dans fichier JSON/database
- Exemple : "Quand je dis 'salut', BBIA me reconna√Æt" ‚Üí sauvegarde pour prochaine fois

**O√π l'ajouter** :
- `src/bbia_sim/bbia_memory.py` - Module m√©moire persistante

---

## üìä R√âSUM√â - CE QUI EXISTE vs MANQUE

| Cat√©gorie | √âtat | O√π c'est | Recommandation |
|-----------|------|----------|----------------|
| **Vision (objets/visages)** | ‚úÖ FAIT | `vision_yolo.py`, `bbia_vision.py` | - |
| **LLM Conversationnel** | ‚úÖ FAIT | `bbia_huggingface.py` | Ajouter Phi-2 (l√©ger) |
| **TTS/STT** | ‚úÖ FAIT | `voice_whisper.py`, `ai_backends.py` | - |
| **Reconnaissance visage personnalis√©e** | ‚ùå MANQUE | - | **Ajouter DeepFace** ‚≠ê |
| **D√©tection √©motions visuelles** | ‚ùå MANQUE | - | **Ajouter DeepFace analyze()** ‚≠ê |
| **D√©tection postures** | ‚ö†Ô∏è PARTIEL | MediaPipe disponible mais pas utilis√© | Ajouter `mp.solutions.pose` |
| **Architecture modulaire** | ‚úÖ EXCELLENT | Tout le projet | - |
| **Finetuning personnalis√©** | ‚ö†Ô∏è PARTIEL | Voix OK, visage non | **Ajouter DeepFace** ‚≠ê |
| **Dashboard No-Code** | ‚ö†Ô∏è PARTIEL | Dashboard web existe | Ajouter Gradio/Streamlit |
| **Entra√Ænement progressif** | ‚úÖ FAIT | `bbia_huggingface.py` | Am√©liorer m√©moire persistante |

---

## üéØ PRIORIT√âS RECOMMAND√âES (Open-Source & Gratuit)

### Priorit√© 1 : DeepFace ‚≠ê‚≠ê‚≠ê

**Pourquoi** :
- Gratuit, open-source
- Reconnaissance visage + √©motions en 1 outil
- Facile √† int√©grer (1 module)

**Code √† ajouter** :
```python
# src/bbia_sim/face_recognition.py
from deepface import DeepFace

class BBIAPersonRecognition:
    def recognize_person(self, image_path: str, db_path: str = "./faces_db"):
        """Reconna√Æt une personne dans une image."""
        result = DeepFace.find(img_path=image_path, db_path=db_path, enforce_detection=False)
        if result:
            return {"name": result[0]["identity"], "confidence": 1 - result[0]["distance"]}
        return None
    
    def detect_emotion(self, image_path: str):
        """D√©tecte l'√©motion sur un visage."""
        result = DeepFace.analyze(img_path=image_path, actions=["emotion"])
        return result["dominant_emotion"]
```

**Installation** :
```bash
pip install deepface
```

---

### Priorit√© 2 : MediaPipe Pose ‚≠ê‚≠ê

**Pourquoi** :
- D√©j√† install√© (pas besoin d'ajouter de d√©pendance)
- D√©tection postures facile
- R√©actions aux gestes

**Code √† ajouter** :
```python
# Dans bbia_vision.py
import mediapipe as mp

self.pose_detector = mp.solutions.pose.Pose()

def detect_poses(self, image):
    results = self.pose_detector.process(image)
    # Retourner positions landmarks (√©paules, coudes, etc.)
```

---

### Priorit√© 3 : LLM L√©ger (Phi-2) ‚≠ê

**Pourquoi** :
- Moins de RAM que Mistral 7B
- Fonctionne sur machines moins puissantes

**Code √† ajouter** :
```python
# Dans bbia_huggingface.py model_configs
"chat_light": {
    "phi2": "microsoft/phi-2",
    "tinyllama": "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
}
```

---

## üìù CONCLUSION

**Ton projet est D√âJ√Ä tr√®s bien** ! ‚úÖ

- ‚úÖ Mod√®les pr√©-entra√Æn√©s utilis√©s partout
- ‚úÖ Architecture modulaire excellente
- ‚úÖ Support TTS/STT/LLM/Vision complet

**Ce qui manque (mais facile √† ajouter)** :
1. ‚úÖ **DeepFace** pour reconnaissance visage + √©motions - **AJOUT√â !** (voir `src/bbia_sim/face_recognition.py`)
2. ‚úÖ **MediaPipe Pose** pour d√©tection postures - **ACTIV√â !** (voir `src/bbia_sim/pose_detection.py`)
3. **LLM l√©ger** optionnel (facultatif) - Phi-2 pour RPi 5

**Tout est open-source et gratuit** comme tu veux ! üéâ

---

## üîí COMPATIBILIT√â REACHY MINI OFFICIEL

**‚ö†Ô∏è IMPORTANT** : Audit de compatibilit√© complet disponible dans `COMPATIBILITE_REACHY_MINI_OFFICIEL.md`

### R√©sum√© Compatibilit√©

**‚úÖ 100% COMPATIBLE** avec le SDK officiel Reachy Mini :

- ‚úÖ **Backend SDK** : Conforme (`ReachyMiniBackend` utilise `reachy_mini` officiel)
- ‚úÖ **D√©pendances SDK** : Toutes pr√©sentes dans `pyproject.toml`
- ‚úÖ **Modules IA isol√©s** : Pas de conflit avec SDK
- ‚úÖ **Imports conditionnels** : Fallbacks si SDK non disponible
- ‚úÖ **Hardware RPi 5** : YOLOv8n, MediaPipe, Whisper tiny fonctionnent bien

### Limitations Hardware (Raspberry Pi 5)

- ‚ö†Ô∏è **Mistral 7B** : Trop lourd (14GB RAM ‚Üí RPi 5 max 8GB)
- ‚úÖ **Solution** : Utiliser Phi-2 (2.7B, ~5GB) ou API Hugging Face gratuite
- ‚ö†Ô∏è **YOLOv8s/m** : Peut √™tre lent
- ‚úÖ **Solution** : Garder YOLOv8n (nano, optimis√©)

### Ajouts Recommand√©s (sans risque pour SDK)

1. **DeepFace** ‚úÖ Compatible, peut √™tre ajout√© dans `venv-vision-py310`
2. **MediaPipe Pose** ‚úÖ D√©j√† install√©, juste √† activer
3. **LLM l√©ger** ‚úÖ Phi-2 recommand√© pour RPi 5

**Conclusion** : Aucun risque de casser le SDK officiel. Tous les modules IA sont optionnels et isol√©s ‚úÖ

---

**Prochaine √©tape** : Veux-tu que j'ajoute DeepFace dans le projet ? (compatible SDK, gratuit, open-source)

