#!/usr/bin/env python3
"""
üß™ TESTS D'EXPERT - CONFORMIT√â BBIAHUGGINGFACE
V√©rifie que l'intelligence conversationnelle respecte les standards expert
et d√©tecte les probl√®mes subtils que seuls les experts en robotique IA
√©motionnelle verraient.
"""

import sys
from pathlib import Path

import pytest

# Ajouter le chemin src au PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))


class TestBBIAHuggingFaceExpertConformity:
    """Tests d'expert pour BBIAHuggingFace - D√©tection probl√®mes subtils."""

    def setup_method(self):
        """Configuration avant chaque test."""
        # Tenter d'importer BBIAHuggingFace (peut √©chouer si transformers non install√©)
        try:
            from bbia_sim.bbia_huggingface import BBIAHuggingFace

            # Cr√©er instance avec mock pour √©viter chargement r√©el mod√®les lourds
            # Note: torch n'est pas un attribut du module, c'est import√© localement
            # On skip simplement ces tests si transformers n'est pas disponible
            try:
                import torch  # noqa: F401

                self.hf_class = BBIAHuggingFace
                self.hf_available = True
            except ImportError:
                self.hf_class = None
                self.hf_available = False
        except ImportError:
            self.hf_class = None
            self.hf_available = False

    def test_01_greeting_variety_sufficient(self):
        """Test Expert 1: Les salutations doivent avoir suffisamment de vari√©t√© (min 8 variants)."""
        print("\nüß™ TEST EXPERT 1: Vari√©t√© salutations")
        print("=" * 60)

        if not self.hf_available:
            pytest.skip("BBIAHuggingFace non disponible")

        # Lire le fichier source pour v√©rifier vari√©t√©
        hf_file = (
            Path(__file__).parent.parent / "src" / "bbia_sim" / "bbia_huggingface.py"
        )
        if not hf_file.exists():
            pytest.skip("Fichier bbia_huggingface.py introuvable")

        content = hf_file.read_text(encoding="utf-8")

        # Compter les variants de salutations friendly_robot
        import re

        # Chercher les listes de greetings friendly_robot
        greeting_matches = re.findall(
            r'"friendly_robot":\s*\[(.*?)\]', content, re.DOTALL
        )
        if greeting_matches:
            # Compter les √©l√©ments dans la liste
            greeting_text = greeting_matches[0]
            variants = [
                line.strip()
                for line in greeting_text.split(",")
                if line.strip() and '"' in line
            ]

            assert len(variants) >= 8, (
                f"EXPERT: friendly_robot greetings doit avoir au moins 8 variants "
                f"(actuellement {len(variants)}). "
                f"Vari√©t√© insuffisante = r√©p√©titions perceptibles pour utilisateurs."
            )
            print(f"‚úÖ Vari√©t√© salutations suffisante: {len(variants)} variants")
        else:
            print("‚ö†Ô∏è  Impossible de d√©tecter les greetings dans le code")

    def test_02_question_responses_not_generic(self):
        """Test Expert 2: Les r√©ponses aux questions ne doivent pas √™tre trop g√©n√©riques."""
        print("\nüß™ TEST EXPERT 2: R√©ponses questions non g√©n√©riques")
        print("=" * 60)

        if not self.hf_available:
            pytest.skip("BBIAHuggingFace non disponible")

        hf_file = (
            Path(__file__).parent.parent / "src" / "bbia_sim" / "bbia_huggingface.py"
        )
        if not hf_file.exists():
            pytest.skip("Fichier bbia_huggingface.py introuvable")

        content = hf_file.read_text(encoding="utf-8")

        # V√©rifier que les r√©ponses questions incluent des √©l√©ments engageants
        # (pas juste "Je ne sais pas" ou "C'est int√©ressant" de mani√®re r√©p√©titive)
        question_responses = content.lower()

        # V√©rifier pr√©sence d'√©l√©ments engageants
        engaging_elements = [
            "en savoir plus",
            "donner plus de d√©tails",
            "ce qui vous intrigue",
            "ce qui vous am√®ne",
            "explorer",
            "r√©fl√©chir",
        ]

        found_elements = sum(
            1 for elem in engaging_elements if elem in question_responses
        )

        assert found_elements >= 3, (
            f"EXPERT: Les r√©ponses questions doivent inclure au moins 3 √©l√©ments engageants "
            f"(actuellement {found_elements}). R√©ponses trop g√©n√©riques = IA per√ßue comme peu intelligente."
        )
        print(f"‚úÖ R√©ponses questions engageantes: {found_elements} √©l√©ments d√©tect√©s")

    def test_03_context_usage_in_responses(self):
        """Test Expert 3: Les r√©ponses doivent utiliser le contexte (coh√©rence conversationnelle)."""
        print("\nüß™ TEST EXPERT 3: Utilisation contexte")
        print("=" * 60)

        if not self.hf_available:
            pytest.skip("BBIAHuggingFace non disponible")

        hf_file = (
            Path(__file__).parent.parent / "src" / "bbia_sim" / "bbia_huggingface.py"
        )
        if not hf_file.exists():
            pytest.skip("Fichier bbia_huggingface.py introuvable")

        content = hf_file.read_text(encoding="utf-8")

        # V√©rifier que le code utilise le contexte r√©cent
        has_context_check = (
            "recent_context" in content or "_get_recent_context" in content
        )
        has_reference_words = (
            "reference_words" in content
            or "√ßa"
            in content  # Les r√©f√©rences contextuelles utilisent "√ßa", "ce", etc.
        )

        assert has_context_check or has_reference_words, (
            "EXPERT: Le code doit utiliser le contexte r√©cent pour coh√©rence conversationnelle. "
            "Sans contexte = r√©ponses d√©connect√©es = IA per√ßue comme peu intelligente."
        )
        print("‚úÖ Utilisation contexte d√©tect√©e dans le code")

    def test_04_personality_distinct_responses(self):
        """Test Expert 4: Les personnalit√©s doivent avoir des r√©ponses distinctes (pas juste emoji diff√©rent)."""
        print("\nüß™ TEST EXPERT 4: Distinction personnalit√©s")
        print("=" * 60)

        if not self.hf_available:
            pytest.skip("BBIAHuggingFace non disponible")

        hf_file = (
            Path(__file__).parent.parent / "src" / "bbia_sim" / "bbia_huggingface.py"
        )
        if not hf_file.exists():
            pytest.skip("Fichier bbia_huggingface.py introuvable")

        content = hf_file.read_text(encoding="utf-8")

        # V√©rifier que chaque personnalit√© a ses propres variantes (pas juste emoji)
        personalities = ["friendly_robot", "curious", "enthusiastic", "calm"]

        for personality in personalities:
            # Chercher les r√©ponses sp√©cifiques √† cette personnalit√©
            pattern = rf'"{personality}":\s*\[(.*?)\]'
            import re

            matches = re.findall(pattern, content, re.DOTALL)
            assert len(matches) > 0, (
                f"EXPERT: La personnalit√© '{personality}' doit avoir ses propres r√©ponses. "
                f"Sans distinction = personnalit√©s per√ßues comme identiques."
            )

        print("‚úÖ Chaque personnalit√© a ses propres r√©ponses distinctes")

    def test_05_sentiment_affects_response_quality(self):
        """Test Expert 5: Le sentiment doit influencer la qualit√©/ton des r√©ponses."""
        print("\nüß™ TEST EXPERT 5: Influence sentiment")
        print("=" * 60)

        if not self.hf_available:
            pytest.skip("BBIAHuggingFace non disponible")

        hf_file = (
            Path(__file__).parent.parent / "src" / "bbia_sim" / "bbia_huggingface.py"
        )
        if not hf_file.exists():
            pytest.skip("Fichier bbia_huggingface.py introuvable")

        content = hf_file.read_text(encoding="utf-8")

        # V√©rifier que le code traite diff√©remment POSITIVE et NEGATIVE
        has_positive_handling = (
            'sentiment_type == "POSITIVE"' in content
            or 'sentiment.get("sentiment") == "POSITIVE"' in content
        )
        has_negative_handling = (
            'sentiment_type == "NEGATIVE"' in content
            or 'sentiment.get("sentiment") == "NEGATIVE"' in content
        )

        assert has_positive_handling and has_negative_handling, (
            "EXPERT: Le code doit traiter diff√©remment les sentiments POSITIVE et NEGATIVE. "
            "Ignorer sentiment = r√©ponses inadapt√©es √©motionnellement = IA per√ßue comme insensible."
        )
        print("‚úÖ Traitement diff√©renci√© des sentiments d√©tect√©")

    def test_06_response_length_appropriate(self):
        """Test Expert 6: Les r√©ponses doivent avoir une longueur appropri√©e (pas trop courtes ni trop longues)."""
        print("\nüß™ TEST EXPERT 6: Longueur r√©ponses appropri√©e")
        print("=" * 60)

        if not self.hf_available:
            pytest.skip("BBIAHuggingFace non disponible")

        hf_file = (
            Path(__file__).parent.parent / "src" / "bbia_sim" / "bbia_huggingface.py"
        )
        if not hf_file.exists():
            pytest.skip("Fichier bbia_huggingface.py introuvable")

        content = hf_file.read_text(encoding="utf-8")

        # Extraction robuste des r√©ponses candidates via AST
        import ast

        tree = ast.parse(content)

        candidates: list[str] = []

        def collect_from_list(node: ast.AST) -> list[str]:
            values: list[str] = []
            if isinstance(node, ast.List):
                for elt in node.elts:
                    if isinstance(elt, ast.Constant) and isinstance(elt.value, str):
                        values.append(elt.value)
            return values

        # Chercher variables/pools connus et blocs de personnalit√©s
        for n in ast.walk(tree):
            # SUFFIX_POOL, _expert_quality_padding
            if isinstance(n, ast.Assign):
                for t in n.targets:
                    if isinstance(t, ast.Name) and t.id in {"SUFFIX_POOL", "_expert_quality_padding"}:
                        candidates.extend(collect_from_list(n.value))
            # dictionaries de r√©ponses: greetings, goodbyes, personality_descriptions
            if isinstance(n, ast.Dict):
                # Heuristique: liste de cha√Ænes comme valeurs des cl√©s string
                for v in n.values:
                    vals = collect_from_list(v)
                    if vals:
                        candidates.extend(vals)

        # Filtres: longueur, contenu textuel, pas de motifs techniques
        def is_human_reply(s: str) -> bool:
            s2 = s.strip()
            if not (20 <= len(s2) <= 200):
                return False
            if sum(ch.isalpha() for ch in s2) < 10:
                return False
            tech_markers = ["{", "}", "[", "]", "^", "$", "\\", "http", "xml", "model", "pipeline", "processor"]
            if any(m in s2.lower() for m in tech_markers):
                return False
            return True

        responses = [s for s in candidates if is_human_reply(s)]

        if responses:
            # V√©rifier que la plupart des r√©ponses ont une longueur appropri√©e
            appropriate_length = [r for r in responses if 30 <= len(r.strip()) <= 150]

            percentage_appropriate = len(appropriate_length) / len(responses) * 100

            assert percentage_appropriate >= 70, (
                f"EXPERT: Au moins 70% des r√©ponses doivent avoir une longueur appropri√©e (30-150 caract√®res). "
                f"Actuellement {percentage_appropriate:.1f}%. "
                f"R√©ponses trop courtes = per√ßues comme peu engageantes, "
                f"trop longues = per√ßues comme verbeuses et peu naturelles."
            )
            print(
                f"‚úÖ {percentage_appropriate:.1f}% des r√©ponses ont une longueur appropri√©e"
            )
        else:
            print("‚ö†Ô∏è  Impossible d'extraire les r√©ponses pour analyse")

    def test_07_fallback_graceful_degradation(self):
        """Test Expert 7: Le fallback LLM ‚Üí simple_response doit √™tre gracieux (pas de r√©gression)."""
        print("\nüß™ TEST EXPERT 7: D√©gradation gracieuse fallback")
        print("=" * 60)

        if not self.hf_available:
            pytest.skip("BBIAHuggingFace non disponible")

        hf_file = (
            Path(__file__).parent.parent / "src" / "bbia_sim" / "bbia_huggingface.py"
        )
        if not hf_file.exists():
            pytest.skip("Fichier bbia_huggingface.py introuvable")

        content = hf_file.read_text(encoding="utf-8")

        # V√©rifier que le code a des fallbacks gracieux
        has_llm_fallback = (
            "fallback vers r√©ponses enrichies" in content.lower()
            or "fallback enrichi" in content.lower()
        )
        has_sentiment_fallback = (
            "fallback si sentiment" in content.lower()
            or 'sentiment.get("sentiment", "NEUTRAL")' in content
        )

        assert has_llm_fallback or has_sentiment_fallback, (
            "EXPERT: Le code doit avoir des fallbacks gracieux pour √©viter r√©gression. "
            "Sans fallback = erreurs bruyantes = exp√©rience utilisateur d√©grad√©e."
        )
        print("‚úÖ Fallbacks gracieux d√©tect√©s dans le code")

    def test_08_no_duplicate_responses(self):
        """Test Expert 8: Aucune r√©ponse ne doit √™tre dupliqu√©e (vari√©t√© essentielle)."""
        print("\nüß™ TEST EXPERT 8: Absence de doublons")
        print("=" * 60)

        if not self.hf_available:
            pytest.skip("BBIAHuggingface non disponible")

        hf_file = (
            Path(__file__).parent.parent / "src" / "bbia_sim" / "bbia_huggingface.py"
        )
        if not hf_file.exists():
            pytest.skip("Fichier bbia_huggingface.py introuvable")

        content = hf_file.read_text(encoding="utf-8")

        # Extraction robuste des r√©ponses candidates via AST
        import ast

        tree = ast.parse(content)
        candidates: list[str] = []

        def collect_from_list(node: ast.AST) -> list[str]:
            values: list[str] = []
            if isinstance(node, ast.List):
                for elt in node.elts:
                    if isinstance(elt, ast.Constant) and isinstance(elt.value, str):
                        values.append(elt.value)
            return values

        for n in ast.walk(tree):
            if isinstance(n, ast.Assign):
                for t in n.targets:
                    if isinstance(t, ast.Name) and t.id in {"SUFFIX_POOL", "_expert_quality_padding"}:
                        candidates.extend(collect_from_list(n.value))
            if isinstance(n, ast.Dict):
                for v in n.values:
                    vals = collect_from_list(v)
                    if vals:
                        candidates.extend(vals)

        # Filtres coh√©rents avec test 06
        def is_human_reply(s: str) -> bool:
            s2 = s.strip()
            if len(s2) < 10:
                return False
            if sum(ch.isalpha() for ch in s2) < 8:
                return False
            tech_markers = ["{", "}", "[", "]", "^", "$", "\\", "http", "xml", "model", "pipeline", "processor"]
            if any(m in s2.lower() for m in tech_markers):
                return False
            return True

        all_responses = [s for s in candidates if is_human_reply(s)]

        # Normaliser (enlever espaces, minuscules) pour d√©tecter doublons approximatifs
        normalized = [" ".join(r.lower().split()) for r in all_responses if len(r) > 10]

        # D√©tecter doublons
        from collections import Counter

        counts = Counter(normalized)
        duplicates = {r: c for r, c in counts.items() if c > 1}

        assert len(duplicates) == 0, (
            f"EXPERT: Aucune r√©ponse ne doit √™tre dupliqu√©e. "
            f"Trouv√© {len(duplicates)} doublons: {list(duplicates.keys())[:3]}. "
            f"Doublons = vari√©t√© insuffisante = r√©p√©titions perceptibles."
        )
        print("‚úÖ Aucun doublon d√©tect√© dans les r√©ponses")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
